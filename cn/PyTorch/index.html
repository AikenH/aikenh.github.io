<!doctype html>
<html lang="cn"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>PyTorch Handbook 00 （Archive） - AikenH Blogs</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Aiken Hong"><meta name="msapplication-TileImage" content="/img/pokemon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Aiken Hong"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Basic Part基础设定部分@AikenH 2020 + 2021 this part is about pytorch basic unit, help me to code deep learning better. Tensor张量计算两个tensor的数乘"><meta property="og:type" content="blog"><meta property="og:title" content="PyTorch Handbook 00 （Archive）"><meta property="og:url" content="http://aikenh.cn/cn/PyTorch/"><meta property="og:site_name" content="AikenH Blogs"><meta property="og:description" content="Basic Part基础设定部分@AikenH 2020 + 2021 this part is about pytorch basic unit, help me to code deep learning better. Tensor张量计算两个tensor的数乘"><meta property="og:locale" content="cn"><meta property="og:image" content="http://aikenh.cn/img/header_img/lml_bg31.jpg"><meta property="article:published_time" content="2021-12-15T00:00:57.000Z"><meta property="article:modified_time" content="2023-10-30T09:27:38.000Z"><meta property="article:author" content="AikenH"><meta property="article:tag" content="Python"><meta property="article:tag" content="Pytorch"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://aikenh.cn/img/header_img/lml_bg31.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://aikenh.cn/cn/PyTorch/"},"headline":"PyTorch Handbook 00 （Archive）","image":["http://aikenh.cn/img/header_img/lml_bg31.jpg"],"datePublished":"2021-12-15T00:00:57.000Z","dateModified":"2023-10-30T09:27:38.000Z","author":{"@type":"Person","name":"AikenH"},"publisher":{"@type":"Organization","name":"AikenH Blogs","logo":{"@type":"ImageObject","url":{"text":"Aiken's Blog"}}},"description":"Basic Part基础设定部分@AikenH 2020 + 2021 this part is about pytorch basic unit, help me to code deep learning better. Tensor张量计算两个tensor的数乘"}</script><link rel="canonical" href="http://aikenh.cn/cn/PyTorch/"><link rel="icon" href="/img/pokemon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-dark.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css" title="default"><link rel="alternate stylesheet" href="/css/cyberpunk.css" title="cyberpunk"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="/atom.xml" title="AikenH Blogs" type="application/atom+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Aiken&#039;s Blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/AikenH"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-lightbulb" id="night-icon"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/img/header_img/lml_bg31.jpg" alt="PyTorch Handbook 00 （Archive）"></span></div><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>PyTorch Handbook 00 （Archive）</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2021-12-15T00:00:57.000Z" title="2021-12-15T00:00:57.000Z">2021-12-15</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-10-30T09:27:38.000Z" title="2023/10/30 17:27:38">2023-10-30</time></span><span class="level-item"><a class="link-muted" href="/categories/Langs/">Langs</a><span> / </span><a class="link-muted" href="/categories/Langs/Pytorch/">Pytorch</a></span><span class="level-item">an hour read (About 9491 words)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><div class="content"><h1 id="Basic-Part基础设定部分"><a href="#Basic-Part基础设定部分" class="headerlink" title="Basic Part基础设定部分"></a>Basic Part基础设定部分</h1><p>@AikenH 2020 + 2021</p>
<p>this part is about pytorch basic unit, help me to code deep learning better.</p>
<h2 id="Tensor张量计算"><a href="#Tensor张量计算" class="headerlink" title="Tensor张量计算"></a>Tensor张量计算</h2><h3 id="两个tensor的数乘"><a href="#两个tensor的数乘" class="headerlink" title="两个tensor的数乘"></a>两个tensor的数乘</h3><span id="more"></span>
<p>计算两个tensor的矩阵乘法，注意其中的batch要相互对应，如果不考虑batch，就是另一个函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 简单的分析一下算法的逻辑</span><br><span class="hljs-comment"># 这是割裂出来batch的矩阵相乘形式</span><br>batch1 = torch.randn(<span class="hljs-number">10</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br>batch2 = torch.randn(<span class="hljs-number">10</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>)<br>out = torch.bmm(batch1, batch2)<br>out.size()<br><br><span class="hljs-string">&#x27;&#x27;&#x27;output ans is</span><br><span class="hljs-string">torch.size([10,3,5])&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment"># 按位相乘</span><br>res = torch.mul(batch1,batch2)<br></code></pre></td></tr></table></figure>
<p><strong>view和permute</strong>的使用实际上都是不改变原值，要用赋值的方式去做，主要是使用方式要对，一个是按照顺序去做。</p>
<h3 id="张量命名"><a href="#张量命名" class="headerlink" title="张量命名"></a>张量命名</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">NCHW = [‘N’, ‘C’, ‘H’, ‘W’]<br>images = torch.randn(<span class="hljs-number">32</span>, <span class="hljs-number">3</span>, <span class="hljs-number">56</span>, <span class="hljs-number">56</span>, names=NCHW)<br>images.<span class="hljs-built_in">sum</span>(<span class="hljs-string">&#x27;C&#x27;</span>)<br>images.select(<span class="hljs-string">&#x27;C&#x27;</span>, index=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>
<h3 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># tensor 与 nd.array进行互换</span><br>ndarray = tensor.cpu().numpy()<br>tensor = torch.from_numpy(ndarray).<span class="hljs-built_in">float</span>()<br><br><span class="hljs-comment"># tensor与PIL.IMAGE进行互换</span><br>image = torchvision.transforms.functional.to_pil_image(tensor)<br>path = <span class="hljs-string">r&#x27;./figure.jpg&#x27;</span><br>tensor = torchvision.transforms.functional.to_tensor(PIL.Image.<span class="hljs-built_in">open</span>(path))<br><br><span class="hljs-comment"># np.ndarray 与 PIL.Image的互换</span><br>image = PIL.Image.fromarray(nd.array.astype(np.uint8))<br>ndarray = np.asarray(PIL.Image.<span class="hljs-built_in">open</span>(path))<br></code></pre></td></tr></table></figure>
<h3 id="维度堆叠"><a href="#维度堆叠" class="headerlink" title="维度堆叠"></a>维度堆叠</h3><p>Stack，<strong>普通的维度堆叠的测试代码如下</strong></p>
<p>测试代码如下，实际上dim=0就是基本的堆起来，dim=1就是按照行来堆，dim=2就是按照列来堆</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.arange(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>).reshape(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)<br>b = torch.arange(<span class="hljs-number">10</span>,<span class="hljs-number">100</span>,<span class="hljs-number">10</span>).reshape(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)<br>c = torch.arange(<span class="hljs-number">100</span>,<span class="hljs-number">1000</span>,<span class="hljs-number">100</span>).reshape(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-----------------a----------------&#x27;</span>)<br><span class="hljs-built_in">print</span>(a)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-----------------b----------------&#x27;</span>)<br><span class="hljs-built_in">print</span>(b)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-----------------c----------------&#x27;</span>)<br><span class="hljs-built_in">print</span>(c)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-----------------dim =0----------------&#x27;</span>)<br>d = torch.stack((a,b,c),dim = <span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(d.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;the value of d:-    &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(d[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]))<br><span class="hljs-built_in">print</span>(d)<br><span class="hljs-comment"># 也就是说，把单个当成整体直接从上往下堆叠</span><br><span class="hljs-comment"># 以x[:][:]为构成单元</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-----------------dim =1----------------&#x27;</span>)<br>d = torch.stack((a,b,c),dim = <span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(d.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;the value of d:-    &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(d[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>]))<br><span class="hljs-built_in">print</span>(d)<br><span class="hljs-comment"># 将每个的第一个维度，按次序纳出来，同value的堆在一起</span><br><span class="hljs-comment"># for example：[a[i][:],b[i][:],c[i][:] ]组成新的单元块</span><br><span class="hljs-comment"># 不，另一种理解，以x[i][:] 为单元</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-----------------dim =2----------------&#x27;</span>)<br>d = torch.stack((a,b,c),dim = <span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(d.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;the value of d:-    &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(d[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]))<br><span class="hljs-built_in">print</span>(d)<br><span class="hljs-comment"># 相应的以x[i][j]为单元构成</span><br></code></pre></td></tr></table></figure>
<p><strong>list的情况下的维度堆叠测试代码如下</strong></p>
<p>相应的测试代码如下，实际上一般是按照dim=1来进行堆叠</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">A = torch.randn([<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>])<br>B = [A[:,i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(A.size(<span class="hljs-number">1</span>))]<br><span class="hljs-comment"># 这样生成的是一个list,按照我们index的排序</span><br><span class="hljs-built_in">print</span>(A)<br><span class="hljs-built_in">print</span>(B)<br>C = torch.stack(B,dim=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;---------------------result-----------------------&#x27;</span>)<br><span class="hljs-built_in">print</span>(C)<br></code></pre></td></tr></table></figure>
<p><strong>Cat</strong></p>
<p>实际上应该也是类似的堆叠思路</p>
<h2 id="基本的张量函数"><a href="#基本的张量函数" class="headerlink" title="基本的张量函数"></a>基本的张量函数</h2><p>torch.split() 划分tensor</p>
<p>torch.randperm进行list的乱序处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 和shuffle区分，这是另一种乱序的操作</span><br><span class="hljs-comment"># cat操作</span><br>a = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):<br>    a.append(torch.tensor([i,i]))<br>all_inputs = torch.cat(a)<br><span class="hljs-comment"># randperm的效果 test1</span><br>idx = torch.randperm(all_inputs.size(<span class="hljs-number">0</span>))<br><span class="hljs-built_in">print</span>(idx)<br>a1, b = all_inputs, all_inputs[idx]<br><span class="hljs-built_in">print</span>(a1,b)<br><span class="hljs-comment"># test2 ，</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-------------------------&#x27;</span>)<br><span class="hljs-comment"># randperm 进行list的shuffle</span><br>tensor_a = torch.randint(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>,[<span class="hljs-number">8</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;origin version &#x27;</span>, tensor_a)<br>idx = torch.randperm(tensor_a.size(<span class="hljs-number">0</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;shuffle idx &#x27;</span>, idx)<br>tensor_b = tensor_a[idx]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;after operation &#x27;</span>, tensor_b)<br></code></pre></td></tr></table></figure>
<p>.fill_()按照输入的值对张量进行填充</p>
<h3 id="选取划窗"><a href="#选取划窗" class="headerlink" title="选取划窗"></a>选取划窗</h3><p><code>nn.unfold</code>拆解卷积中的划窗步骤</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br>inputs = torch.randn(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)<br>unfold = torch.nn.Unfold(<span class="hljs-number">4</span>,stride=<span class="hljs-number">4</span>)<br>output = unfold(inputs)<br><span class="hljs-comment"># res output</span><br>output.size()<br>$ [<span class="hljs-number">1</span>,<span class="hljs-number">4</span>,<span class="hljs-number">3136</span>]<br><span class="hljs-comment"># 3136 = (224/4) * (224/4)</span><br></code></pre></td></tr></table></figure>
<h2 id="Torch环境设置"><a href="#Torch环境设置" class="headerlink" title="Torch环境设置"></a>Torch环境设置</h2><h3 id="pytorch中的随机种子初始化"><a href="#pytorch中的随机种子初始化" class="headerlink" title="pytorch中的随机种子初始化"></a>pytorch中的随机种子初始化</h3><p>yTorch 和 Python的随机数生成器就算随机种子一样也不会产生一样的结果。</p>
<p>我们可以这样来设置Pytorch的随机数种子：（通常和GPU一起使用）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.manual_seed(seed)<br><br></code></pre></td></tr></table></figure>
<h3 id="nn-parameter"><a href="#nn-parameter" class="headerlink" title="nn.parameter()"></a>nn.parameter()</h3><ol>
<li>Main idea：<strong>parameter的作用，主要是将参数和model绑定在一起</strong>，我们就知道这个模型中，可能需<strong>要训练的参数</strong>有哪些，可以需要进行训练的参数加进去，但是当我们想要freeze it的时候就使用detach或者直接修改require_grad来让参数不在接受训练就好了， require_grad是其中的一个属性。可以结合上面的代码分析。</li>
<li>tensor变量是不可训练的，只有修改成parameter才能进行训练。</li>
<li>自带的网络结构中的一些weight和bias应该都是parameter的变量</li>
</ol>
<h3 id="nn-Softmax中的dim"><a href="#nn-Softmax中的dim" class="headerlink" title="nn.Softmax中的dim"></a>nn.Softmax中的dim</h3><p>其实没那么复杂，就和数据的维度是一样的，我们需要把那一个维度的数据之后的数据全部加起来处理就用哪个维度去做。</p>
<p>IMAGE = N* DATA，dim=1 说明dim = 0 的Channel 是需要被排外的。也就是我们的softmax是基于data进行的。可以找寻源码进行进一步分析解释。</p>
<h2 id="测试、验证模块"><a href="#测试、验证模块" class="headerlink" title="测试、验证模块"></a>测试、验证模块</h2><h3 id="基本编写"><a href="#基本编写" class="headerlink" title="基本编写"></a>基本编写</h3><h3 id="model-eval-和model-train-的区别"><a href="#model-eval-和model-train-的区别" class="headerlink" title="model.eval()和model.train()的区别"></a>model.eval()和model.train()的区别</h3><p>通常在模型测试的时候会执行<code>model.eval()</code>切换模型的状态，而在训练的时候会执行<code>model.train()</code>，model在这两个状态下的区别主要有：</p>
<p>在<strong>train</strong>状态下会启用BN和Dropout，而在<strong>eval</strong>不启用这两个模块；</p>
<ul>
<li>启用BN指的是：用到每一个Batch数据的均值和方差；不启用则指的是使用整体的均值和方差（同时停止更新mean和var）</li>
<li>而对于Dropout来说：启用的时候指的是会随机进行dropout，而关闭的话就会用到全部的网络链接</li>
</ul>
<h3 id="with-torch-no-grad"><a href="#with-torch-no-grad" class="headerlink" title="with torch.no_grad()"></a>with torch.no_grad()</h3><p>上下文管理器，wrap起来的部分不会track grade</p>
<p>主要用于停止autograd模块的工作，被<code>with</code>包裹起来的部分会停止梯度的更新，得到进一步的加速把，因为我们实际上在验证的时候不会执行<code>step()</code>等操作，所以能够节省计算模型梯度的时间。</p>
<h3 id="模型的保存和读取专题"><a href="#模型的保存和读取专题" class="headerlink" title="模型的保存和读取专题"></a>模型的保存和读取专题</h3><p>@Aiken 2020</p>
<p>基于onenote笔记，我们知道关键在于如何自由的读取模型中的参数，并选择性的取出来。</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/LXX516/article/details/80124768">pytorch 模型部分参数的加载_LXX516的博客-CSDN博客_pytorch 加载部分参数</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-comment"># 至少基于这样的方式我们能把模型中参数的string取出来。</span><br>pretrained_dict=torch.load(model_weight)<br>model_dict=myNet.state_dict()<br><br><span class="hljs-comment"># 1. filter out unnecessary keys</span><br>pretrained_dict = &#123;k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> pretrained_dict.items() <span class="hljs-keyword">if</span> k <span class="hljs-keyword">in</span> model_dict&#125;<br><br><span class="hljs-comment"># 2. overwrite entries in the existing state dict</span><br>model_dict.update(pretrained_dict)<br>myNet.load_state_dict(model_dict)<br><br></code></pre></td></tr></table></figure>
<h1 id="GPU相关的设置"><a href="#GPU相关的设置" class="headerlink" title="GPU相关的设置"></a>GPU相关的设置</h1><p>@written by Aiken, 2020  this document is about Pytorch‘s CUDA, &amp; GPU setting.</p>
<h2 id="查看GPU状态"><a href="#查看GPU状态" class="headerlink" title="查看GPU状态"></a>查看GPU状态</h2><h3 id="设置默认GPU设备"><a href="#设置默认GPU设备" class="headerlink" title="设置默认GPU设备"></a>设置默认GPU设备</h3><p>一般使用GPU之前，我们需要知道系统中有多少GPU设备，因为默认的GPU设备是0，而且，大家一般都直接使用这张卡，所以我们如果只使用单卡的话，切换一下默认的GPU设备，能够避免一定的冲突。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 查看GPU使用状态</span><br>$ nvidia-smi<br><span class="hljs-comment"># or</span><br>$ gpustat [--watch]<br><br></code></pre></td></tr></table></figure>
<h3 id="设备基本信息"><a href="#设备基本信息" class="headerlink" title="设备基本信息"></a><strong>设备基本信息</strong></h3><ol>
<li><p>查看是否存在GPU，数量，类型</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-comment"># 查看是否存在GPU，数量，类型</span><br>torch.cuda.is_available()<br>torch.cuda.device_count()<br>torch.cuda.get_device_name(<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>
</li>
<li><p>查看指定的GPU的容量和名称</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.cuda.get_device_capability(device)<br>torch.cuda.get_device_name(device)<br></code></pre></td></tr></table></figure>
</li>
<li><p>设置当前系统的默认gpu_devices，推荐使用os来设置（实际上是命令行中的操作）实际上是系统设定针对当前进程的可见GPU，其他的GPU会对当前的程序隐藏，所以默认的0</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">os.environ[<span class="hljs-string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="hljs-string">&quot;id&quot;</span> <span class="hljs-comment">#推荐用法</span><br><span class="hljs-comment"># 可以在vscode的launch.json中设置env</span><br></code></pre></td></tr></table></figure>
<p> <strong>注意事项：该命令需要在所有调用了CUDA的代码、子程序之前，包括<code>import</code>，所以很多代码的import都是在main()中的。</strong></p>
</li>
</ol>
<h2 id="GPU使用率优化（注意事项）"><a href="#GPU使用率优化（注意事项）" class="headerlink" title="GPU使用率优化（注意事项）"></a>GPU使用率优化（注意事项）</h2><h3 id="缓存爆炸问题"><a href="#缓存爆炸问题" class="headerlink" title="缓存爆炸问题"></a>缓存爆炸问题</h3><p>GPU使用途中需要注意的地方，在每次iteration之后记得<strong>清除在GPU中占用</strong>的memory，cache等，不然有时候会导致缓存和内存的递增和爆炸。</p>
<p>具体操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.cuda.empty_cache()<br><span class="hljs-comment"># after every iteration</span><br><br></code></pre></td></tr></table></figure>
<h3 id="运行效率优化"><a href="#运行效率优化" class="headerlink" title="运行效率优化"></a>运行效率优化</h3><p><code>cudnn.benchmark</code>、<a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936">pytorch论坛</a> <a target="_blank" rel="noopener" href="https://www.pytorchtutorial.com/when-should-we-set-cudnn-benchmark-to-true/">pytorch中文网</a>、<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/73711222">zhihu究极分析文</a></p>
<p><strong>基本使用思路</strong>：</p>
<p>在程序的开始，让cudnn花费一点额外的时间，找到适用于当前配置的最佳算法，从而优化运行效率。</p>
<p><strong>注意事项：</strong></p>
<p>但是如果我们的input_size在每个iteration都存在变化的话，那么每一个iteration都要执行一次搜索，反而得不偿失。</p>
<p><strong>具体操作</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.backends.cudnn.benchmark = true<br></code></pre></td></tr></table></figure>
<h3 id="设置使用GPU的方式"><a href="#设置使用GPU的方式" class="headerlink" title="设置使用GPU的方式"></a>设置使用GPU的方式</h3><h3 id="设置相应的随机种子"><a href="#设置相应的随机种子" class="headerlink" title="设置相应的随机种子"></a>设置相应的随机种子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.cuda.empty_cache()<br><span class="hljs-comment"># part2 设置随机种子</span><br>torch.cuda.manual_seed(seed)<br>torch.cuda.manual_seed_all(seed)<br><br></code></pre></td></tr></table></figure>
<h3 id="CUDA转换"><a href="#CUDA转换" class="headerlink" title="CUDA转换"></a>CUDA转换</h3><p>使用<code>.cuda()</code>来对<code>模型</code>，<code>数据</code>，<code>Loss</code>进行赋值，或者使用<code>to_devices()</code>来设置到相应的GPU设备</p>
<p>将模型转化到cuda中要在优化器的建立之前执行，因为optimizer是对于模型建立的，对模型执行cuda后已经和原本的参数和模型都不是同一个了，所以一定<strong>要在建立优化器之前就对模型进行Cuda 的转化</strong>。</p>
<p>是否要对loss转换到CUDA，取决于一下的两种情况：</p>
<ul>
<li>损失函数是Functional：这样的话只要传入的参数是CUDA的就会再CUDA上计算</li>
<li>损失函数是Class with params：如果类内有参数的话，也要转换到CUDA才能一起在CUDA上计算</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>	<span class="hljs-keyword">try</span>:<br>		loss = loss.cuda()<br>	<span class="hljs-keyword">except</span> AttributeError:<br>		<span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;the loss is not cuda-able &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">type</span>(loss)))<br></code></pre></td></tr></table></figure>
<h3 id="多GPU并行"><a href="#多GPU并行" class="headerlink" title="多GPU并行"></a>多GPU并行</h3><p>主要使用的命令<code>nn.DataParallel()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">model = nn.DataParaller(model,device_ids=<span class="hljs-literal">None</span>)<br><span class="hljs-comment"># 如果不设定id的话，应该是自动指定全部可见的GPU的</span><br><br></code></pre></td></tr></table></figure>
<h1 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h1><p>偶然会由于<code>pin_memory</code> 的设置来致使CPU的不正常运行（满载等等），并非总是这样。</p>
<h2 id="核心和线程数设置"><a href="#核心和线程数设置" class="headerlink" title="核心和线程数设置"></a>核心和线程数设置</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lei_qi/article/details/115358703">限制或增加pytorch的线程个数！指定核数或者满核运行Pytorch！！！_lei_qi的博客-CSDN博客</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> multiprocessing <span class="hljs-keyword">import</span> cpu_count<br><span class="hljs-comment"># 设置环境变量来控制线程多发的情况</span><br>cpu_num = cpu_count()<br><span class="hljs-comment"># 核心代码</span><br>os.environ[<span class="hljs-string">&#x27;OMP_NUM_THREADS&#x27;</span>] = <span class="hljs-built_in">str</span>(cpu_num)<br><span class="hljs-comment"># 下面这些应该是不一定药有</span><br>os.environ [<span class="hljs-string">&#x27;OPENBLAS_NUM_THREADS&#x27;</span>] = <span class="hljs-built_in">str</span>(cpu_num)<br>os.environ [<span class="hljs-string">&#x27;MKL_NUM_THREADS&#x27;</span>] = <span class="hljs-built_in">str</span>(cpu_num)<br>os.environ [<span class="hljs-string">&#x27;VECLIB_MAXIMUM_THREADS&#x27;</span>] = <span class="hljs-built_in">str</span>(cpu_num)<br>os.environ [<span class="hljs-string">&#x27;NUMEXPR_NUM_THREADS&#x27;</span>] = <span class="hljs-built_in">str</span>(cpu_num)<br><br><span class="hljs-comment"># 从其他资料中可以感觉这条代码应该是和核心代码一样的功能，所以两个写一个应该就可以了</span><br>torch.set_num_threds(cpu_num)<br></code></pre></td></tr></table></figure>
<h1 id="网络定义模块"><a href="#网络定义模块" class="headerlink" title="网络定义模块"></a>网络定义模块</h1><h2 id="数据定义模块"><a href="#数据定义模块" class="headerlink" title="数据定义模块"></a>数据定义模块</h2><h3 id="利用TorchVision读取本地数据"><a href="#利用TorchVision读取本地数据" class="headerlink" title="利用TorchVision读取本地数据"></a>利用TorchVision读取本地数据</h3><p><code>torchvision.datasets.imagefolder()</code> 这个函数实际上能代替我们之前写的函数，但是由于自己写的有一部分统一规则可以使得我们的自定义程度很高，所以目前我们在绝大多数情况下不使用该方法来进行替代。</p>
<p>但是由于是一个重要的函数，我们在这里还是介绍一下该工具的使用方式：</p>
<h3 id="torch-自定义Dataset后的使用"><a href="#torch-自定义Dataset后的使用" class="headerlink" title="torch 自定义Dataset后的使用"></a>torch 自定义Dataset后的使用</h3><ol>
<li>自定义dataset的继承以及后续调用需要注意的是不能忘记将其转换成dataloaer，然后进行iter命令的执行。</li>
<li>也可以用enumerate函数来进行调用，就是记得调用的格式是什么就好</li>
<li>可以参考basicunit中的对shuffle的认知对该函数进行进一步的理解。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义dataset的部分</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RL_AET_Dataset</span>(torch.utils.data.Dataset):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(RL_AET_Dataset,self).__init__()<br>        <span class="hljs-keyword">pass</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">pass</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">pass</span><br><span class="hljs-comment"># 声明和构建部分 要记得使用dataloader</span><br>train_l_dataset = RL_AET_Dataset(x_l, y_l, args)<br>train_l_dataloader =torch.utils.data.DataLoader(train_l_dataset,batch_size=args[<span class="hljs-string">&#x27;b_s&#x27;</span>],shuffle=<span class="hljs-literal">True</span>,num_workers=args[<span class="hljs-string">&#x27;num_workers&#x27;</span>],drop_last=<span class="hljs-literal">True</span>,pin_memory=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment">#调用迭代部分</span><br>labeled_loader = <span class="hljs-built_in">iter</span>(train_l_dataloader)<br><span class="hljs-comment">#all_label_info =  [*next(labeled_loader)]</span><br><br></code></pre></td></tr></table></figure>
<h3 id="Dataloader中的transformer（）："><a href="#Dataloader中的transformer（）：" class="headerlink" title="Dataloader中的transformer（）："></a>Dataloader中的transformer（）：</h3><p><strong>疑惑解答  用compose集成的所有transform，都会应用，有个to_tensor，切to_tensor会自动转换PIL中的channel和数值范围。</strong></p>
<ol>
<li>compose中的变换组合的顺序关系<ul>
<li>PIL处理的图像变换（比如数据增强之类的方法）</li>
<li><code>to_tensor()</code></li>
<li>处理tensor的方法：<code>normalize</code></li>
</ul>
</li>
<li><p>示例代码</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">data_transforms = transforms.Compose([<br>                        transforms.RandomResizedCrop(<span class="hljs-number">224</span>),<br>                        transforms.RandomHorizontalFlip().<br>                        transforms.ToTensor(),<br>                        transforms.Normalize([a,b,c],[A,B,C])])<br><span class="hljs-comment"># 然后直接加入dataset中的参数，或者是我们自定义的部分</span><br><span class="hljs-comment"># 在dataset中的写法如下，我们可以在自己的dataset中进行定义</span><br><span class="hljs-keyword">if</span> self.transformer <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        img = self.transform(img)<br><span class="hljs-comment"># 具体的源码细节表现如下</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> self.transforms:<br>        img = t(img)<br><span class="hljs-keyword">return</span> img<br></code></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Dataloader中的参数"><a href="#Dataloader中的参数" class="headerlink" title="Dataloader中的参数"></a>Dataloader中的参数</h3><p>shuffle机制</p>
<p>主要解决问题：</p>
<ol>
<li>是否每次调用的时候都进行随机的操作，还是只有在初始化的时候才进行随机</li>
<li>两种不同使用Dataloader的方式是否会对shuffle的方式进行区分</li>
</ol>
<p>结论：</p>
<ol>
<li>每次对dataloader进行重新调用（重新放到enumerate），或者重新定义iter，都会重新进行shuffle。</li>
</ol>
<p>num_worker</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/hesse-summer/p/11343870.html">参考资料1</a>  参考资料2：pytorch中文文档👇</p>
<p><strong>num_workers</strong> (<em>int</em>, optional) – 用多少个子进程加载数据。0表示数据将在主进程中加载(默认: 0)<br>用num_worker个子进程加载数据，所以能够将数据在主进程展示还没有调用到该数据之前就将后续的数据存入RAM，所以在数据读取上会比较快，但是占用的RAM和CPU资源会比较大。</p>
<p>samples:</p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader">torch.utils.data - PyTorch 1.9.0 documentation</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/marsggbo/p/11308889.html">一文弄懂Pytorch的DataLoader, DataSet, Sampler之间的关系</a></p>
<p>官方的解释是：</p>
<p>sampler (Sampler or Iterable, optional) – defines the strategy to draw samples from the dataset. Can be any Iterable with <strong>len</strong> implemented. If specified, shuffle must not be specified.</p>
<p>定义从数据集（还是最开始的哪个数据集，不能是额外的数据集）中提取样本的策略：是否能通过该Method去实现Hard-Task或者像Meta-Task一样的采样过程呢？从Meta-Transfer-Learning中看来是可以的，可以学习一下它的写法。</p>
<h4 id="collate-fn"><a href="#collate-fn" class="headerlink" title="collate_fn()"></a>collate_fn()</h4><p>collate<em>fn的作用就是将一个batch的数据进行合并操作。默认的collate<em>fn是将img和label分别合并成imgs和labels，所以如果你的__getitem</em></em>方法只是返回 img, label,那么你可以使用默认的collate_fn方法,<br>但是如果你每次读取的数据有img, box, label等等，那么你就需要自定义collate_fn来将对应的数据合并成一个batch数据，这样方便后续的训练步骤。</p>
<ul>
<li>编写collate_fn可以参考qidong的文章主要是接受数据和标签列表，将其整合成一个矩阵的形式;</li>
<li>如果对传参有需求,可以参考<code>lambda</code>的形式或者是类定义的形式去传入</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">dataload = DataLoader(dataset, <span class="hljs-keyword">lambda</span> x: collate_fn(x, **params))<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">collater</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">**params</span>):<br>        self.params = ...<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call</span>(<span class="hljs-params">self,datas</span>):<br>        <span class="hljs-comment"># make it a batch in this function, then we will instance this class</span><br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_helpful_fn</span>(<span class="hljs-params">self</span>):<br>        ...<br></code></pre></td></tr></table></figure>
<p>using collate_fn, we can augment the dataset more flexible.</p>
<h2 id="编写模型"><a href="#编写模型" class="headerlink" title="编写模型"></a>编写模型</h2><h3 id="模型基本单元"><a href="#模型基本单元" class="headerlink" title="模型基本单元"></a>模型基本单元</h3><p>nn.conv2D：</p>
<ul>
<li>kernel_size[1]应该指的是卷积核的宽（不一定都是正方形的）</li>
</ul>
<h3 id="模型参数共享："><a href="#模型参数共享：" class="headerlink" title="模型参数共享："></a>模型参数共享：</h3><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wwzone/articles/12917333.html">pytorch：对比clone、detach以及copy_等张量复制操作</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 假设有modela和modelb，我们需要在进行下降的时候执行参数统一，</span><br><span class="hljs-keyword">for</span> a_para,b_para <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(modela.parameters(),modelb.parameters()):<br>        b_para.data.copy_(a_para.data)<br></code></pre></td></tr></table></figure>
<h3 id="网络定义的方式对比分析"><a href="#网络定义的方式对比分析" class="headerlink" title="网络定义的方式对比分析"></a>网络定义的方式对比分析</h3><p>@Aiken 2021 主要对比的是ModuleList和Sequtial</p>
<p><strong>结论：</strong>通常使用的话，这里我个人推荐使用的是<code>sequtial</code>结合<code>collection</code>中的<code>orderdict</code>来构建的方法，这个方法集成了内部的<code>forward</code>，同时通过<code>`orderdict</code>也能给print带来更好的可视化效果。</p>
<p>但是还是有一些特殊的使用场景我们会用到<code>ModuleList</code></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/75206669">详解PyTorch中的ModuleList和Sequential</a></p>
<p>主要区别：</p>
<ol>
<li>nn.Sequential内部实现了forward函数，因此可以不用写forward函数。而nn.ModuleList则没有实现内部forward函数。</li>
<li>nn.Sequential可以使用OrderedDict对每层进行命名，上面已经阐述过了；</li>
<li>nn.Sequential里面的模块按照顺序进行排列的，所以必须确保前一个模块的输出大小和下一个模块的输入大小是一致的。而nn.ModuleList 并没有定义一个网络，它只是将不同的模块储存在一起，这些模块之间并没有什么先后顺序可言。<strong>网络的执行顺序按照我们在forward中怎么编写来决定的</strong></li>
<li><p>有的时候网络中有很多相似或者重复的层，我们一般会考虑用 for 循环来创建它们，而不是一行一行地写，这种时候就使用ModuleList：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">net4</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(net4, self).__init__()<br>        layers = [nn.Linear(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>)]<br>        self.linears = nn.ModuleList(layers)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.linears:<br>            x = layer(x)<br>        <span class="hljs-keyword">return</span> x<br><br>net = net4()<br><span class="hljs-built_in">print</span>(net)<br><span class="hljs-comment"># net4(</span><br><span class="hljs-comment">#   (linears): ModuleList(</span><br><span class="hljs-comment">#     (0): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="hljs-comment">#     (1): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="hljs-comment">#     (2): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="hljs-comment">#   )</span><br><span class="hljs-comment"># )</span><br><br></code></pre></td></tr></table></figure>
</li>
</ol>
<p>基本使用：</p>
<ol>
<li><p>nn.sequential</p>
<p> 可以通过list和*以及add moudle来进行迭代的定义，同时这种定义方式，会方便我们的重复注册</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> OrderedDict<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">net_seq</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(net_seq, self).__init__()<br>        self.seq = nn.Sequential(OrderedDict([<br>                        (<span class="hljs-string">&#x27;conv1&#x27;</span>, nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>,<span class="hljs-number">5</span>)),<br>                         (<span class="hljs-string">&#x27;relu1&#x27;</span>, nn.ReLU()),<br>                          (<span class="hljs-string">&#x27;conv2&#x27;</span>, nn.Conv2d(<span class="hljs-number">20</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>)),<br>                       (<span class="hljs-string">&#x27;relu2&#x27;</span>, nn.ReLU())<br>                       ]))<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> self.seq(x)<br>net_seq = net_seq()<br><span class="hljs-built_in">print</span>(net_seq)<br><span class="hljs-comment">#net_seq(</span><br><span class="hljs-comment">#  (seq): Sequential(</span><br><span class="hljs-comment">#    (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="hljs-comment">#    (relu1): ReLU()</span><br><span class="hljs-comment">#    (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="hljs-comment">#    (relu2): ReLU()</span><br><span class="hljs-comment">#  )</span><br><span class="hljs-comment">#)</span><br><br></code></pre></td></tr></table></figure>
</li>
<li><p>nn.ModuleList:与python自带的List不同的地方在于他会自动将网络注册到Parameter中，成为网络，但是需要自己去编写forward过程</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">net_modlist</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(net_modlist, self).__init__()<br>        self.modlist = nn.ModuleList([<br>                       nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>),<br>                       nn.ReLU(),<br>                        nn.Conv2d(<span class="hljs-number">20</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>),<br>                        nn.ReLU()<br>                        ])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.modlist:<br>            x = m(x)<br>        <span class="hljs-keyword">return</span> x<br><br>net_modlist = net_modlist()<br><span class="hljs-built_in">print</span>(net_modlist)<br><span class="hljs-comment">#net_modlist(</span><br><span class="hljs-comment">#  (modlist): ModuleList(</span><br><span class="hljs-comment">#    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="hljs-comment">#    (1): ReLU()</span><br><span class="hljs-comment">#    (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="hljs-comment">#    (3): ReLU()</span><br><span class="hljs-comment">#  )</span><br><span class="hljs-comment">#)</span><br><br><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> net_modlist.parameters():<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(param.data), param.size())<br><span class="hljs-comment">#&lt;class &#x27;torch.Tensor&#x27;&gt; torch.Size([20, 1, 5, 5])</span><br><span class="hljs-comment">#&lt;class &#x27;torch.Tensor&#x27;&gt; torch.Size([20])</span><br><span class="hljs-comment">#&lt;class &#x27;torch.Tensor&#x27;&gt; torch.Size([64, 20, 5, 5])</span><br><span class="hljs-comment">#&lt;class &#x27;torch.Tensor&#x27;&gt; torch.Size([64])</span><br><br></code></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Detach-amp-detach"><a href="#Detach-amp-detach" class="headerlink" title="Detach &amp; detach_"></a>Detach &amp; detach_</h3><p>这个模块在后续进行pretrain或者transfer的时候应该会经常被用到，所以这种方法还是需要熟练掌握的</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wanghui-garcia/p/10677071.html">详细的分析介绍</a></p>
<p><code>detach</code>是产生一组不需要下降的“<code>Copy</code>”：如果要修改原值的话就要进行赋值操作。</p>
<p><code>detach_</code>则是修改本身参数的属性（<code>require_grad</code>etc.）执行函数就能将参数修改为不需要下降的情况，不需要执行赋值处理。</p>
<h3 id="模型调用的Tips"><a href="#模型调用的Tips" class="headerlink" title="模型调用的Tips"></a>模型调用的Tips</h3><p><strong>使用list进行多模型的混合调用</strong></p>
<p>由于python默认的是引用赋值，也就是浅拷贝的方式？<br>通过list来进行模型的批量构建，通过list来将模型整合起来，是<strong>不会</strong>使用<strong>额外的存储空间</strong>的，它们指向同一个地址。基于这样的假设，我们可以基于list来简化代码，通过LOOP来执行，相关的调用操作，比如生成器或者预测之类的，来<strong>简化</strong>代码结构。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">model1 = AET_model(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,**kwargs)<br>model2 = AET_model(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,**kwargs)<br>model_list = [model1,model2]<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">id</span>(model1)==<span class="hljs-built_in">id</span>(model2):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;the address of those model is same, so donot need extra space&#x27;</span>)<br><span class="hljs-comment"># 具体可以简化什么类型的操作：</span><br>optimizer_list = []<br><span class="hljs-keyword">for</span> _, models_t <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(model_list):<br>    optimizer_list.append(optim.SGD(<br>                            models_t.parameters(),<br>                            lr,mom，wd))<br>optimizer1 = _[<span class="hljs-number">0</span>]<br>optimizer2 = _[<span class="hljs-number">1</span>]<br><span class="hljs-comment"># like this</span><br><br></code></pre></td></tr></table></figure>
<h3 id="Warm-up-factor"><a href="#Warm-up-factor" class="headerlink" title="Warm-up factor"></a>Warm-up factor</h3><p>对于这一部分的概念我还是有些不了解，是否和冷启动和热启动的概念是相关的，如果不是的话，顺便就学习一下冷启动和热启动的概念。</p>
<p>具体解析：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/55933867/what-does-learning-rate-warm-up-mean">neural network - What does “learning rate warm-up” mean? - Stack Overflow</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36387683/article/details/97265084">关于warm<em>up学习率</em>云中寻雾的博客-CSDN博客</a></li>
</ol>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/xys430381_1/article/details/107468446">pytorch学习率调整方法（warm up） ，label smooth、apex混合精度训练、梯度累加_xys430381_1的专栏-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/338066667">神经网络中 warmup 策略为什么有效；有什么理论解释么？</a></p>
<h3 id="Weight-decay（L2）"><a href="#Weight-decay（L2）" class="headerlink" title="Weight decay（L2）"></a>Weight decay（L2）</h3><p>实际上就是对权重进行L2正则化，让权重衰减到更小的值，在一定程度上减少模型的过拟合问题，所以权重衰减实际上也叫L2正则化。</p>
<p>具体的数学推导后续将集成到<strong>GoodNote笔记</strong>上，将正则化单独作为一个模块去整理。</p>
<p><strong>权重衰减（L2正则化）的作用</strong></p>
<p><strong>作用：</strong> 权重衰减（L2正则化）可以避免模型过拟合问题。</p>
<p><strong>思考：</strong> L2正则化项有让w变小的效果，但是为什么w变小可以防止过拟合呢？</p>
<p><strong>原理：</strong> （1）从模型的复杂度上解释：更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合更好（这个法则也叫做奥卡姆剃刀），而在实际应用中，也验证了这一点，L2正则化的效果往往好于未经正则化的效果。（2）从数学方面的解释：过拟合的时候，拟合函数的系数往往非常大，为什么？如下图所示，过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。</p>
<p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20201205175236273.png" alt="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20201205175236273.png"></p>
<p>image-20201205175236273</p>
<p>内容来自： <a target="_blank" rel="noopener" href="https://blog.csdn.net/u012162613/article/details/44261657">正则化方法：L1和L2 regularization、数据集扩增、dropout</a></p>
<h3 id="Learning-Rate-Decay"><a href="#Learning-Rate-Decay" class="headerlink" title="Learning Rate Decay"></a>Learning Rate Decay</h3><p>当我们选择了一个合适的lr，但是损失训练到一定程度以后就不再下降了，就在一个区间中来回动荡，可能是出现了一下的问题：</p>
<p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20201205175605729.png" alt="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20201205175605729.png"></p>
<p>image-20201205175605729</p>
<p>对这种问题的解决就是通过学习率衰减来实现的：将学习率随着训练的进行来进行衰减，这个方法就比较直观了。具体的方法描述可以在 <code>../project_note/训练参数调整策略.md</code>中找到。</p>
<p>也可以参考如下连接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42662358/article/details/93732852">详细理解pytorch的六种学习率pytorch</a></p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>nn中自带的Loss Function比如说MSE之类的，计算出来的值本身就已经对batch取了平均值，同时我们进行交叉熵的计算的时候，我们不需要实现对他进行softmax，因为再CE中已经集成了softmax的操作。</p>
<h3 id="CrossEntropy交叉熵"><a href="#CrossEntropy交叉熵" class="headerlink" title="CrossEntropy交叉熵"></a>CrossEntropy交叉熵</h3><p>这里会介绍一下Pytorch中的CE损失的具体实现的方法，这里给出三种方式的对比。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-comment"># initial data and calculate method</span><br>input_x = torch.randn((<span class="hljs-number">4</span>,<span class="hljs-number">5</span>))<br>label = torch.tensor((<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>))<br>cri = torch.nn.CrossEntropyLoss()<br>nll_f = torch.nn.NLLLoss()<br><br><span class="hljs-comment"># output softmax and logsoftmax and pred</span><br>softamx_x = torch.softmax(input_x,dim=<span class="hljs-number">1</span>)<br>logsoftmax_x = torch.log(softamx_x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;softamx_x \n&quot;</span>, softamx_x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;pre_res \n&quot;</span>, softamx_x.argmax(axis=<span class="hljs-number">1</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;log_softamx_x \n&quot;</span>, logsoftmax_x)<br><br><span class="hljs-comment"># calculate official ce and NLL</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;torch ce \n&quot;</span>,cri(input_x,label))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;nll_cal \n&quot;</span>, nll_f(logsoftmax_x,label))<br><br><span class="hljs-comment"># calculate the manual ce loss we cal</span><br>res = [-logsoftmax_x[i][label[i]] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(label))]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;manual cal \n&quot;</span>,<span class="hljs-built_in">sum</span>(res)/<span class="hljs-built_in">len</span>(label))<br><br></code></pre></td></tr></table></figure>
<p>可以发现三种方式计算出来的损失是一样的，这就说明了我们在计算的时候要记住，ce中是自己集成了softmax的操作，同时在Nll中是存在了取negative的操作的。按照这个操作手册去实现自己相应的损失函数设计</p>
<h2 id="优化器设计"><a href="#优化器设计" class="headerlink" title="优化器设计"></a>优化器设计</h2><p>这一部分主要添加一些常见的优化器参数的设置包括SGD和Adam的对应设置，主要介绍一下设置Adam<br>实际上Adam的设置对于学习率来说没有那么敏感，但是我们还是要了解参数的意思才知道怎么去设置该优化器</p>
<h2 id="模型参数初始化和架构查看方法"><a href="#模型参数初始化和架构查看方法" class="headerlink" title="模型参数初始化和架构查看方法"></a>模型参数初始化和架构查看方法</h2><p>实际上对参数初始化也就是需要对整体的架构进行遍历，所以这两个会归为一个子课题</p>
<p>参数的初始化方法只要使用如下的方式，无论我们采取那种定义的方式，，都能遍历到其中所包含的所有网络层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 如果直接在网络定义的时候直接进行初始化</span><br><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.modules():<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m,nn.Conv2d):<br>        nn.init.kaiming_normal_(m.weight,mode=<span class="hljs-string">&#x27;fan_out&#x27;</span>)<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m,nn.BatchNorm2d):<br>        nn.init.constant_(m.weight,<span class="hljs-number">1</span>)<br>        nn.init.constant_(m.bias,<span class="hljs-number">1</span>)<br><span class="hljs-comment"># 如果是在模型定义的外部的话</span><br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> model.modules():<br>  <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(layer, torch.nn.Conv2d):<br>      torch.nn.init.kaiming_normal_(layer.weight,mode=<span class="hljs-string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br>  <span class="hljs-keyword">if</span> layer.bias isnotNone:<br>      torch.nn.init.constant_(layer.bias, val=<span class="hljs-number">0.0</span>)<br>  <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(layer, torch.nn.BatchNorm2d):<br>      torch.nn.init.constant_(layer.weight, val=<span class="hljs-number">1.0</span>) torch.nn.init.constant_(layer.bias, val=<span class="hljs-number">0.0</span>)<br>  <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(layer, torch.nn.Linear):<br>      torch.nn.init.xavier_normal_(layer.weight)<br>  <span class="hljs-keyword">if</span> layer.bias isnotNone:<br>      torch.nn.init.constant_(layer.bias, val=<span class="hljs-number">0.0</span>)<br>      layer.weight = torch.nn.Parameter(tensor)<br><span class="hljs-comment"># 也可以使用其他的方法比如parameters，children</span><br><br></code></pre></td></tr></table></figure>
<h3 id="children、modules、parameters："><a href="#children、modules、parameters：" class="headerlink" title="children、modules、parameters："></a>children、modules、parameters：</h3><p><code>model.modules</code>会遍历model中所有的子层，而<code>children</code>只会遍历当前层，也就是最外层的情况，所以如果要进行参数的初始化的话，最好是用类内或者类外的两种方法来实现初始化</p>
<p><code>parameter</code>返回的是模型的所有参数，所以初始化最好使用的是<code>`modules</code>，而parameter一般用来初始化参数</p>
<p><strong>用numel与parameters计算参数的个数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#可以简洁的写成下面的形式</span><br><span class="hljs-comment">#numel()函数本身的作用是返回数组中元素的个数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">count_parameters</span>(<span class="hljs-params">model</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(P.numel() <span class="hljs-keyword">for</span> P <span class="hljs-keyword">in</span> model.parameters() <span class="hljs-keyword">if</span> P.requires_grad)<br><br><span class="hljs-comment">#帮助理解的结构形式可以表达如下：</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">count_parameters</span>(<span class="hljs-params">model</span>):<br>    <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters():<br>        <span class="hljs-keyword">if</span> p.requires_grad:<br>            ans += p.numel()<br><br></code></pre></td></tr></table></figure>
<h3 id="初始化原则：（继续调研）"><a href="#初始化原则：（继续调研）" class="headerlink" title="初始化原则：（继续调研）"></a>初始化原则：（继续调研）</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/ys1305/article/details/94332007">pytorch中的参数初始化方法总结_ys1305的博客-CSDN博客_pytorch 参数初始化</a></p>
<p><strong>Batch-Normalization</strong>：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/shine-lee/p/11989612.html">Batch Normalization详解 - shine-lee - 博客园 (cnblogs.com)</a></p>
<ul>
<li>conv：<code>kaming_normal_</code></li>
<li>fc：<code>constan_,xvaier</code></li>
<li>bn：<code>normal_\constant|</code></li>
</ul>
<h3 id="典型的参数初始化方法"><a href="#典型的参数初始化方法" class="headerlink" title="典型的参数初始化方法"></a>典型的参数初始化方法</h3><p>EnAET中可以看到参考的源码如下，需要注意的是，BN中只有两个参数，所以不需要进行参数的初始化，或者直接置0、1即可.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.modules():<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m,nn.Conv2d):<br>        <span class="hljs-comment"># 计算参数</span><br>        n = m.kernel_size[<span class="hljs-number">0</span>] * m.kernel_size[<span class="hljs-number">1</span>] * m.out_channels<br>        m.weight.data.normal_(<span class="hljs-number">0</span>,math.sqrt(<span class="hljs-number">2.</span> / n))<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m,nn.BatchNorm2d):<br>        m.weight.data.fill_(<span class="hljs-number">1</span>)<br>        m.bias.data.zero_()<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, nn.Linear):<br>        nn.init.xavier_normal_(m.weight.data)    <span class="hljs-comment"># what&#x27;s this method</span><br>        m.bias.data.zero_()<br><br></code></pre></td></tr></table></figure>
<h2 id="数据类型和维度"><a href="#数据类型和维度" class="headerlink" title="数据类型和维度"></a>数据类型和维度</h2><p>在算法编写的过程中，数据的类型和维度的对齐和channel是很重要的，在这里也很容易出现很多的bug，在这里做一个信息的汇总</p>
<h3 id="输入数据的通道"><a href="#输入数据的通道" class="headerlink" title="输入数据的通道"></a>输入数据的通道</h3><p>结论：pytorch网络输入图片的shape要求通道是<strong>channel_first</strong>（通道在前）的，所以如果我们的图片不是这样的话，我们就需要执行相应的变化。</p>
<p>TODO：整理各种数据读取方式读入的channel first 或是 last : skimage,PIL,numpy</p>
<p>整理相应的各种数据类型进行transpose（numpy）的方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 也可以使用view函数，但是相应的，view需要计算出各个维度相应的数值</span><br><span class="hljs-comment"># view（）直接使用的时候不改变原值的大小，permute也不改变，使用的方法不同而已</span><br><span class="hljs-keyword">if</span> img.shape[-<span class="hljs-number">1</span>] == <span class="hljs-number">3</span>:<br>    img = img.permute(<span class="hljs-number">0</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)<br><br></code></pre></td></tr></table></figure>
<h3 id="标签的形式转换one-hot"><a href="#标签的形式转换one-hot" class="headerlink" title="标签的形式转换one-hot"></a>标签的形式转换one-hot</h3><p>进行训练之前要将数据转化为onehot的形式，才能输入训练，而且一般因为是batch_size的形式，所以我们需要转化为矩阵形式的onehot，不能用单个label的转化方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_onehot_single</span>(<span class="hljs-params">num,index</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;根据类别数量和index生成single，onehot&#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># BTW：scatter方法也能生成one-hot</span><br>    onehot = torch.zeros(num)<br>    onehot[index] = <span class="hljs-number">1.0</span><br><br>    <span class="hljs-keyword">return</span> onehot<br><br><span class="hljs-comment"># 主要是下面这种方法需要掌握，</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_onehot_array</span>(<span class="hljs-params">width,target</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;根据label生成onehot矩阵。</span><br><span class="hljs-string">    width：类别数 target：具体的labeldata&#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">try</span>:<br>        length = <span class="hljs-built_in">len</span>(target.view(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">except</span> ValueError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;the type of target is &#123;&#125; &#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">type</span>(target)))<br>        <span class="hljs-built_in">print</span>(target)<br>        <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">&#x27;break down&#x27;</span>)<br>    onehot = torch.zeros(length, width).scatter_(<span class="hljs-number">1</span>,target.view(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">return</span> onehot<br><br></code></pre></td></tr></table></figure>
<h1 id="Visualize-可视化部分"><a href="#Visualize-可视化部分" class="headerlink" title="Visualize 可视化部分"></a>Visualize 可视化部分</h1><h2 id="Tensorboard-in-Pytorch"><a href="#Tensorboard-in-Pytorch" class="headerlink" title="Tensorboard in Pytorch"></a>Tensorboard in Pytorch</h2><p>@Aiken H 2021 review  之前这一部分的projection和model都没有成功显示，这次在新框架中展示一下。</p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html">Visualizing Models, Data, and Training with TensorBoard - PyTorch Tutorials 1.8.1+cu102 documentation</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/bigbennyguo/article/details/87956434">详解PyTorch项目使用TensorboardX进行训练可视化_浅度寺-CSDN博客_tensorboardx</a></p>
<p><a target="_blank" rel="noopener" href="https://pytorch.apachecn.org/#/docs/1.7/17?id=使用-tensorboard-可视化模型，数据和训练">使用 TensorBoard 可视化模型，数据和训练 (apachecn.org)</a></p>
<p>在pytorch教程中的Projection可以结合后续输出的Feature使用来分析相应的聚类和分类可靠性</p>
<p>可以尝试使用，教程写的很简单易懂。</p>
<h3 id="Histogram-直方图参数统计"><a href="#Histogram-直方图参数统计" class="headerlink" title="Histogram 直方图参数统计"></a>Histogram 直方图参数统计</h3><p>一般来说用来统计模型中间的一些参数的分布情况，具体的使用在训练的epoch之间，和val是一个比较类似的机制，具体的代码样例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># visualize those parameter as historgram</span><br><span class="hljs-comment"># we can add other model here</span><br><span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>    <span class="hljs-keyword">for</span> name,param <span class="hljs-keyword">in</span> self.main_model.named_parameters():<br>        self.writer.add_histogram(<span class="hljs-string">&#x27;main_model&#x27;</span>+name,param.clone().cpu().data.numpy(),i)<br>    <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure>
<h3 id="Embedding-Projection"><a href="#Embedding-Projection" class="headerlink" title="Embedding Projection"></a>Embedding Projection</h3><p>@Aiken H 2021 这一部分可能才是神经网络的特征分布的可视化图。</p>
<p>下面这个是Google的Embedding Projection，需要上传.tsv保存的数据，但是实际上就是Tensorboard上也有集成的功能</p>
<p><a target="_blank" rel="noopener" href="http://projector.tensorflow.org/">Embedding projector - visualization of high-dimensional data</a></p>
<p><a target="_blank" rel="noopener" href="https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin">Visualizing Data using the Embedding Projector in TensorBoard</a></p>
<h3 id="PR-CURVE"><a href="#PR-CURVE" class="headerlink" title="PR_CURVE"></a>PR_CURVE</h3><p>这里会贴上pr_curve中需要的参数和我们这边编写的示例代码</p>
<h3 id="Add-TEXT"><a href="#Add-TEXT" class="headerlink" title="Add_TEXT"></a>Add_TEXT</h3><p>换行失效问题, 这是因为在Tensorboard中这一部分使用的是Markdown的格式, 所以在这里我们在换行符<code>\n</code>之前, 需要保留两个空格才能实现真正的换行</p>
<h3 id="ADD-Figure"><a href="#ADD-Figure" class="headerlink" title="ADD_Figure"></a>ADD_Figure</h3><p>有时候我们会发现我们编写的figure在step中没有全部现实出来, 这是我们可以通过启动命令来展示所有的图片</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">--samples_per_plugin images=<span class="hljs-number">9999</span><br><span class="hljs-comment"># 999 &gt; the num you want to displ</span><br></code></pre></td></tr></table></figure>
<h2 id="可视化神经网络热力图（CAM）"><a href="#可视化神经网络热力图（CAM）" class="headerlink" title="可视化神经网络热力图（CAM）"></a>可视化神经网络热力图（CAM）</h2><p>@Aiken2020 为了便于查看神经网络的<strong>输出</strong>，对于图像的哪一部分<strong>更加的侧重</strong>，也就是指导网络进行分类的主要是图像的哪些区域，（相应的也可以按照类似的方法查看Attention Network的效果把），就想着<strong>可视化一下CAM</strong>。看指导分类的高响应区域是否落在核心区域。</p>
<p>参考链接：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_37532065/article/details/103362517">CAM Pytorch</a></p>
<h3 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h3><p>其计算方法如下图所示。对于一个CNN模型，对其最后一个featuremap做全局平均池化（GAP）计算各通道均值，然后通过FC层等映射到class score，找出argmax，<strong>计算最大的那一类的输出相对于最后一个featuremap的梯度</strong>（实际上就是最后一个map中哪些对于分类的变化其更大的作用，也就是类似权重的机制），再把这个梯度可视化到原图上即可。直观来说，就是看一下<strong>网络抽取到的高层特征的哪部分对最终的classifier影响更大</strong>。</p>
<p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/20191203102807477.png" alt="ImgInGIthu"></p>
<ul>
<li><p>Quote: 找到了一篇基于Keras的CAM实现，感谢：</p>
<p>  <a target="_blank" rel="noopener" href="https://blog.csdn.net/Einstellung/article/details/82858974">https://blog.csdn.net/Einstellung/article/details/82858974</a> 但是我还是习惯用Pytorch一点，所以参考着改了一版Pytorch的实现。其中，有一个地方困扰了一下，因为Pytorch的自动求导机制，一般只会保存函数值对输入的导数值，而中间变量的导数值都没有保留，而此处我们需要计算输出层相对于最后一个feature map梯度，所以参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_27061325/article/details/84728539解决了该问题。">https://blog.csdn.net/qq_27061325/article/details/84728539解决了该问题。</a></p>
</li>
</ul>
<h3 id="代码实现："><a href="#代码实现：" class="headerlink" title="代码实现："></a>代码实现：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> cv2<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">draw_CAM</span>(<span class="hljs-params">model, img_path, save_path, transform=<span class="hljs-literal">None</span>, visual_heatmap=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    绘制 Class Activation Map</span><br><span class="hljs-string">    :param model: 加载好权重的Pytorch model</span><br><span class="hljs-string">    :param img_path: 测试图片路径</span><br><span class="hljs-string">    :param save_path: CAM结果保存路径</span><br><span class="hljs-string">    :param transform: 输入图像预处理方法</span><br><span class="hljs-string">    :param visual_heatmap: 是否可视化原始heatmap（调用matplotlib）</span><br><span class="hljs-string">    :return:</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># 图像加载&amp;预处理</span><br>    img = Image.<span class="hljs-built_in">open</span>(img_path).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>    <span class="hljs-keyword">if</span> transform:<br>        img = transform(img)<br>    img = img.unsqueeze(<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># 获取模型输出的feature/score</span><br>    model.<span class="hljs-built_in">eval</span>()<br>    features = model.features(img)<br>    output = model.classifier(features)<br><br>    <span class="hljs-comment"># 为了能读取到中间梯度定义的辅助函数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">extract</span>(<span class="hljs-params">g</span>):<br>        <span class="hljs-keyword">global</span> features_grad<br>        features_grad = g<br><br>    <span class="hljs-comment"># 预测得分最高的那一类对应的输出score</span><br>    pred = torch.argmax(output).item()<br>    pred_class = output[:, pred]<br><br>    features.register_hook(extract)<br>    pred_class.backward() <span class="hljs-comment"># 计算梯度</span><br><br>    grads = features_grad   <span class="hljs-comment"># 获取梯度</span><br><br>    pooled_grads = torch.nn.functional.adaptive_avg_pool2d(grads, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br><br>    <span class="hljs-comment"># 此处batch size默认为1，所以去掉了第0维（batch size维）</span><br>    pooled_grads = pooled_grads[<span class="hljs-number">0</span>]<br>    features = features[<span class="hljs-number">0</span>]<br>    <span class="hljs-comment"># 512是最后一层feature的通道数</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">512</span>):<br>        features[i, ...] *= pooled_grads[i, ...]<br><br>    <span class="hljs-comment"># 以下部分同Keras版实现</span><br>    heatmap = features.detach().numpy()<br>    heatmap = np.mean(heatmap, axis=<span class="hljs-number">0</span>)<br><br>    heatmap = np.maximum(heatmap, <span class="hljs-number">0</span>)<br>    heatmap /= np.<span class="hljs-built_in">max</span>(heatmap)<br><br>    <span class="hljs-comment"># 可视化原始热力图</span><br>    <span class="hljs-keyword">if</span> visual_heatmap:<br>        plt.matshow(heatmap)<br>        plt.show()<br><br>    img = cv2.imread(img_path)  <span class="hljs-comment"># 用cv2加载原始图像</span><br>    heatmap = cv2.resize(heatmap, (img.shape[<span class="hljs-number">1</span>], img.shape[<span class="hljs-number">0</span>]))  <span class="hljs-comment"># 将热力图的大小调整为与原始图像相同</span><br>    heatmap = np.uint8(<span class="hljs-number">255</span> * heatmap)  <span class="hljs-comment"># 将热力图转换为RGB格式</span><br>    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)  <span class="hljs-comment"># 将热力图应用于原始图像</span><br>    superimposed_img = heatmap * <span class="hljs-number">0.4</span> + img  <span class="hljs-comment"># 这里的0.4是热力图强度因子</span><br>    cv2.imwrite(save_path, superimposed_img)  <span class="hljs-comment"># 将图像保存到硬盘</span><br><br></code></pre></td></tr></table></figure>
<h2 id="BUGs"><a href="#BUGs" class="headerlink" title="BUGs"></a>BUGs</h2><p>如果想要展示出所有step的图片, 我们可以在命令行里执行tensoroard的时候执行下列命令.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tensorboard --logdir <span class="hljs-built_in">log</span>/cifar100_resnet18 --samples_per_plugin images=999999<br></code></pre></td></tr></table></figure>
<h1 id="DEBUG"><a href="#DEBUG" class="headerlink" title="DEBUG"></a>DEBUG</h1><h2 id="1-ImportError-cannot-import-name-‘PILLOW-VERSION’"><a href="#1-ImportError-cannot-import-name-‘PILLOW-VERSION’" class="headerlink" title="1.ImportError: cannot import name ‘PILLOW_VERSION’"></a>1.ImportError: cannot import name ‘PILLOW_VERSION’</h2><p>PIL版本过高，换低就可以，他不配是一个棘手的问题</p>
<p><code>pip install Pillow==6.2.2 --user</code></p>
<h2 id="2-模型参数-amp-计算量统计-and-Debug输出"><a href="#2-模型参数-amp-计算量统计-and-Debug输出" class="headerlink" title="2.模型参数&amp;计算量统计 and Debug输出"></a>2.模型参数&amp;计算量统计 and Debug输出</h2><ol>
<li>用来计算模型构建中网络的参数，空间大小，MAdd，FLOPs等指标，count_params很好写，然后剩下的计算我们交给两个第三方的库来实现：<code>torchstat</code>,<code>thop</code></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchstat <span class="hljs-keyword">import</span> stat<br>stat(model,(<span class="hljs-number">3</span>,<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)) <span class="hljs-comment">#that‘s all using it in the eval stage</span><br><br></code></pre></td></tr></table></figure>
<ol>
<li>也可以使用<code>torchsummary</code>来查看各层输出的数据的维度数目</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchsummary <span class="hljs-keyword">import</span> summary<br>summary(model.cuda(),input_size=(<span class="hljs-number">3</span>,<span class="hljs-number">224</span>,<span class="hljs-number">224</span>),batch_size=<span class="hljs-number">1</span>)<br><br></code></pre></td></tr></table></figure>
<ol>
<li>相应的Debug还可以使用<code>torchsnooper</code>进行：变量的类型和维度追踪这个模块通过<code>@xxxx</code>修饰器的方法调用在指定的method前面，能够在训练过程中输出一些<strong>参数值的类型</strong>和<strong>数值变化</strong>的较为详细的信息。个人理解的最佳使用环境是，用于调试或者监控<strong>类型之间的错误</strong>。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 这个package如果没记错的话好像是使用装饰器的方法去进行测试</span><br><span class="hljs-meta">@...</span><br>method()<br><br></code></pre></td></tr></table></figure>
<h2 id="3-PyTorch加载预训练模型"><a href="#3-PyTorch加载预训练模型" class="headerlink" title="3.PyTorch加载预训练模型"></a>3.PyTorch加载预训练模型</h2><p>具体错误：在于模型Dict中的Key和预训练model中的Key不对应，无法匹配。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs json">Unexpected key(s) in state_dict<span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;module.features. ...&quot;</span>.，Expected <span class="hljs-string">&quot;.features....&quot;</span><br><br></code></pre></td></tr></table></figure>
<p>问题分析：</p>
<p><strong>situation1</strong>：可以看到前面多了module这个str，这一般是由于其中一方使用了多GPU训练后直接保存的，也就是<code>DataParallel</code>模式下导致的不匹配问题。</p>
<p><strong>solution1</strong>： <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_32998593/article/details/89343507">参考资料</a></p>
<ol>
<li>load模型后去掉多余的参数在事先的时候发现这个方法还是存在问题的，并不是简单的dict封装的结构，所以没法这样简单的进行赋值处理:x:</li>
<li>用空白代替module，暂时还没尝试，但是我觉得会遇到和第一个一样的问题:x:</li>
<li><p>:zap:最简单的方法：加载模型后将模型进行DataParallel，再进行数据的转化，将数据进行并行化。具体的操作如下</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">model.cuda()<br><span class="hljs-comment"># 将ids设置成拥有的GPU即可，但是不知道单GPU的情况可不可以实现这种情况</span><br>model = nn.DataParallel(model, device_ids=<span class="hljs-literal">None</span>)<br><br></code></pre></td></tr></table></figure>
</li>
</ol>
<p><strong>Situation2：</strong> 保存模型格式为.pth.tar，无法载入训练好的模型</p>
<p><strong>Solution2</strong>：</p>
<p>原因是因为被保存的模型是在高版本的pytorch下实现的，但是再低版本中读取的模型是.pth格式的，就会出现版本冲突。<br>解决方法如下👇：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 在高版本的环境中load model，然后再重新保存，保存的时候添加参数，使得保存成旧版本即可</span><br>torch.save(checkpoint,save_path,_use_new_zipfile_serialization=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># DONE</span><br><br></code></pre></td></tr></table></figure>
<p><strong>xxx is a zip archive(did you mean to use torch.jit.load()?)</strong></p>
<p>使用低版本的Torch去Load高版本（&gt;1.6）保存的模型（.pth.tar）遇到的问题,</p>
<p>这种错误，主要是模型的版本冲突。</p>
<p><strong>解决办法</strong>：在高版本的环境中，重新load模型，然后直接save，在保存的时候添加参数</p>
<p><code>torch.save(model.state_dict(),model_path,_use_new_zipfile_serialization=False)</code></p>
<p>就可以保存成.pth的模型，也能在低版本的torch环境中使用了</p>
<h2 id="4-some-of-the-strides-of-a-given-numpy-array-are-negative"><a href="#4-some-of-the-strides-of-a-given-numpy-array-are-negative" class="headerlink" title="4.some of the strides of a given numpy array are negative."></a>4.some of the strides of a given numpy array are negative.</h2><p>ver：torch1.2 这个问题可能会在后续的版本中被优化。</p>
<p><strong>Situation</strong>：</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/devilmaycry812839668/p/13761613.html">https://www.cnblogs.com/devilmaycry812839668/p/13761613.html</a><br>问题出现在flat操作中，反向切片<code>[::-1]</code>会导致数据存储在内存上不连续，在旧版本中无法实现，对这样数据进行存储。<br><strong>Solution1</strong>:<br>所以在执倒排切片的时候执行，<code>img2 = np.ascontiguousarray(img)</code>  使得数据在内存空间上连续。</p>
<p><strong>Solution2</strong>:</p>
<p>或者执行倒排切片的时候，直接<code>return img.copy()</code></p>
<h2 id="5-读取loader的时候图像的大小不一"><a href="#5-读取loader的时候图像的大小不一" class="headerlink" title="5.读取loader的时候图像的大小不一"></a>5.读取loader的时候图像的大小不一</h2><p>使用Crop对图像进行处理的时候，不注意的话就是会出现这样的问题，图像的size随机改变，导致的输出不统一。也可能是Crop函数写的有问题。</p>
<p><strong>bug info</strong>如下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ RuntimeError: invalid argument 0: Sizes of tensors must match except <span class="hljs-keyword">in</span> dimension 0. Got 182 and 360 <span class="hljs-keyword">in</span> dimension 2<br><br></code></pre></td></tr></table></figure>
<p><strong>Solution：</strong></p>
<p>resize，spp，padding，<strong>adaptiveMaxPooling</strong>（自适应的pooling，pooling到指定的size（channel除外））</p>
<h2 id="6-bus-error-dataloader-num-worker"><a href="#6-bus-error-dataloader-num-worker" class="headerlink" title="6.bus error dataloader num_worker"></a>6.bus error dataloader num_worker</h2><p>原因暂时还不是太清楚，但是我们可以把num_worker设置为0 来解决这个问题.jpg</p>
<h2 id="7-bus-error：insufficient-shared-memory（shm）"><a href="#7-bus-error：insufficient-shared-memory（shm）" class="headerlink" title="7.bus error：insufficient shared memory（shm）"></a>7.bus error：insufficient shared memory（shm）</h2><p>这种原因通常会在docker环境中出现，由于未指定shm容量大小，比如<code>ipc=host</code>之类的命令，就会导致docker的shm只有64m，于是在运行的时候就会出问题。这种情况下<strong>只能重新run docker</strong>（目前只找到了这个方法）。</p>
<p>如果要妥协的话，就只能<strong>试着减小batch_size</strong>。但是随着模型的设计上，这其实不是一个可以逃避的问题，也会增加莫须有的其他成本，所以。</p>
<h2 id="8-训练过程中Cache和Memory的占用逐渐升高"><a href="#8-训练过程中Cache和Memory的占用逐渐升高" class="headerlink" title="8.训练过程中Cache和Memory的占用逐渐升高"></a>8.训练过程中Cache和Memory的占用逐渐升高</h2><p>主要的体现是：<strong>逐渐升高</strong>这一点，而不是稳定的情况；</p>
<p>有点玄学，但是在这种情况下，我们在每个iteration结束的时候使用清楚显存的函数，竟然就能进行控制了，虽然我不知道为啥清楚显存的函数会顺便连内存中的cache也一起清除了，但是就是，学。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.cuda.empty_cache()<br></code></pre></td></tr></table></figure>
<h2 id="9-梯度爆炸问题，算法没有学习效果"><a href="#9-梯度爆炸问题，算法没有学习效果" class="headerlink" title="9.梯度爆炸问题，算法没有学习效果"></a>9.梯度爆炸问题，算法没有学习效果</h2><p>梯度爆炸问题，分析可能出现存在的问题：</p>
<ul>
<li>某一部分的学习参数可能的lr过高，权重过高，导致误差快速传播。</li>
<li>问题的复杂度过高，算法overpower了把。</li>
</ul>
<p>针对于第一点的话，我们参考工程笔记中的学习率调整策略即可。</p>
<p>如果是问题的复杂度过高，那么可能是问题对于我们的模型来说已经overpower的，我们可能需要去加深网络的层数，或者对网络进行进一步的设计和对数据的分析问题。</p>
<h2 id="10-类型转换问题汇总"><a href="#10-类型转换问题汇总" class="headerlink" title="10.类型转换问题汇总"></a>10.类型转换问题汇总</h2><ol>
<li>比如<code>scatter_</code>需要将数据从int32的格式转换成int64，我们要掌握一下在Pytorch中进行数据类型转换的技巧。</li>
<li><strong>Expected object of scalar type Float but got scalar type Double for argument #2 ‘target’</strong> 数据类型不匹配，一个是np.float32,另一个是64<br>参考解决方案：<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/56741087/how-to-fix-runtimeerror-expected-object-of-scalar-type-float-but-got-scalar-typ">重要</a></li>
<li><p><strong>Expected object of scalar type Long but got scalar type Float for argument</strong><br>希望得到的是Long型标量数据，但是得到了Float型的数据（实际上可能是我们进行测试的时候使用了小数带来的，但是我们也能将其转化就是了）</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">Longtensor()<br><span class="hljs-built_in">type</span>(torch.longtensor)<br><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.DoubleTensor) should be the same</strong><br><strong>RuntimeError: Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same</strong>问题实际上都是和权重的数据类型不匹配，需要将字节型或者是FLoat型向Weight的数据类型转换，但是可能这里的问题实际上出现在就是我们导入的数据类型是不正确的。还是使用<code>type()</code>命令来进行数据类型的转换，但是关键还是：<strong>检查输入数据的类型以及数值范围，同时看看在进行dataloader的时候有没有指定to_tensor的变换等等</strong></p>
</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/75dff8e7ed18">参考资料链接</a></p>
<p><strong>进行数据转换的几种方式</strong></p>
<ol>
<li><p>使用函数<code>tensor1.type_as(tensor2)</code>将1的数据类型转换成2的数据类型。</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor_1 = torch.FloatTensor(<span class="hljs-number">5</span>)<br>tensor_2 = torch.IntTensor([<span class="hljs-number">10</span>, <span class="hljs-number">20</span>])<br>tensor_1 = tensor_1.type_as(tensor_2)<br><br></code></pre></td></tr></table></figure>
</li>
<li><p><code>tensor.type(torch.IntTensor)</code></p>
</li>
<li><code>tensor.long()</code>,<code>tensor.char()</code>,<code>tensor.int()</code>,<code>tensor.byte()</code>,<code>tensor.double()</code></li>
<li><code>tenosr.to(torch.long)</code></li>
</ol>
<h2 id="11-数据维度不对应问题汇总"><a href="#11-数据维度不对应问题汇总" class="headerlink" title="11.数据维度不对应问题汇总"></a>11.数据维度不对应问题汇总</h2><ol>
<li><p><strong>multi-target not supported at</strong>问题实际上可以翻译成：维度上和交叉熵损失函数的需求不对应。在使用交叉熵损失函数的时候，target的形状应该是和label的形状一致或者是只有batchsize这一个维度的。如果target是这样的【batchszie，1】就会出现上述的错误</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">使用squeeze（）函数降低维度<br></code></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="12-取出具体数值时候的问题"><a href="#12-取出具体数值时候的问题" class="headerlink" title="12.取出具体数值时候的问题"></a>12.取出具体数值时候的问题</h2><ol>
<li><strong>RuntimeError: Can’t call numpy() on Variable that requires grad. Use var.detach().numpy()</strong>对于输出的结果要转换成具体的数值的时候，如果我们后续还需要这个数值的梯度，就不能转换到<code>cpu</code>后再转换到<code>numpy</code>,就好比说，我们要取出Loss的时候，我们可以直接使用item()取出具体的数值，而不需要转到CPU<a href="#">上</a></li>
</ol>
<h2 id="13-CPU占用99"><a href="#13-CPU占用99" class="headerlink" title="13.CPU占用99%"></a>13.CPU占用99%</h2><p>问题描述：使用torch自带的dataset中的cifar10的时候，在每个epoch结束的时候，CPU占用率高达99%，并不随着num_workder而改变，问题可能由于pytorch开辟了太多的线程</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/stay_zezo/article/details/107809409">windows10下pytorch的GPU利用率低，占用率低_stay_zezo的博客-CSDN博客</a></p>
<p>可能是由于GPU运算太快了，启用了多线程进行加载数据，这种时候启用<code>pin_memory=true</code> 能起到一定的作用把，加快一点数据读取。</p>
<p>最终解决方案 ：<code>pin-memory=false</code> 反正原因很神奇，但是最终就是因为这个解决的，可能是因为memory超了，所以每次都需要重新empty_cache 重新装进页，所以反而加重了CPU的负担</p>
<h2 id="14-预测值全为0，模型收敛到奇怪的地方，损失保持一致（全均等）"><a href="#14-预测值全为0，模型收敛到奇怪的地方，损失保持一致（全均等）" class="headerlink" title="14. 预测值全为0，模型收敛到奇怪的地方，损失保持一致（全均等）"></a>14. 预测值全为0，模型收敛到奇怪的地方，损失保持一致（全均等）</h2><p>这种情况通常是由于模型设计中存在一点问题：</p>
<p>比如这次是由于模型中fc后面添加了relu，这样导致输出的负值全被抑制了，导致学习出现了严重的错误后果。</p>
<h2 id="15-模型部分：-训练中模型准确率不上升"><a href="#15-模型部分：-训练中模型准确率不上升" class="headerlink" title="15.模型部分： 训练中模型准确率不上升"></a>15.模型部分： 训练中模型准确率不上升</h2><p>由于框架已经验证过是可以进行正常训练的，在这种情况下出现模型的准确率不上升可能是由于模型本身设计（内部代码编写）上的问题。</p>
<h2 id="16-On-entry-to-SGEMM-parameter-number-8-had-an-illegal-value"><a href="#16-On-entry-to-SGEMM-parameter-number-8-had-an-illegal-value" class="headerlink" title="16. On entry to SGEMM parameter number 8 had an illegal value"></a>16. On entry to SGEMM parameter number 8 had an illegal value</h2><p>Tracing failed sanity checks!<br>Graphs differed across invocations!</p>
<p>fc的问题，输入fc和对应的网络输入层不一致，检查阶段数目和feature输出的特征维度</p>
<h2 id="17-CUDA-error-device-side-assert-triggered-CUDA-kernel-errors-might-be-asynchronously-reported-at-some-other-API-call"><a href="#17-CUDA-error-device-side-assert-triggered-CUDA-kernel-errors-might-be-asynchronously-reported-at-some-other-API-call" class="headerlink" title="17. CUDA error: device-side assert triggered CUDA kernel errors might be asynchronously reported at some other API call"></a>17. CUDA error: device-side assert triggered CUDA kernel errors might be asynchronously reported at some other API call</h2><p>这个问题的出现的根本原因在于：</p>
<p>维度不匹配：标签的dimension 超出了全连接层最后输出的dimension，这一部分错误的触发，和Loss的计算，Acc的计算，有着强烈的相关关系。</p>
<p>为了解决这个问题，我们在训练相关的验证和训练环节，需要保持训练数据集和验证数据集在类别数目上的一致性，而在我们需要对数据集外的数据进行测试的时候，我们避免进行Loss的计算，在对Acc进行计算的时候，也尽量避免Torch中的自有库，避免产生该类的问题/</p>
<h2 id="RuntimeError-the-derivative-for-target-is-not-implemented"><a href="#RuntimeError-the-derivative-for-target-is-not-implemented" class="headerlink" title="RuntimeError the derivative for target is not implemented"></a>RuntimeError the derivative for target is not implemented</h2><p>问题通常出现在损失计算的过程中，这个错误是由于我们在损失中的第二项 <code>targets</code>不应该有梯度，但是在这个地方却存在梯度导致的.</p>
<p>在这里我们可以通过仅仅取出 <code>tensor</code>的<code>data</code>或者使用<code>detach</code>and<code>copy</code>来进行数值的传递</p>
<h2 id="Only-Tensors-created-explicitly-by-the-user-graph-leaves-support-the-deepcopy-protocol-at-the-moment"><a href="#Only-Tensors-created-explicitly-by-the-user-graph-leaves-support-the-deepcopy-protocol-at-the-moment" class="headerlink" title="Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment"></a>Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment</h2><p>该错误是由deepcopy和require_grad, require_fn同时构成, 如果我们对一个需要计算梯度的非叶子节点进行deepcopy就会触发这个错误。</p>
<p>如果我们需要对这个数据进行存储的话，我们可以执行<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">save = copy.deepcopy(feature.data.cpu().numpy())<br></code></pre></td></tr></table></figure></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>PyTorch Handbook 00 （Archive）</p><p><a href="http://aikenh.cn/cn/PyTorch/">http://aikenh.cn/cn/PyTorch/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>AikenH</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2021-12-15</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-10-30</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/Python/">Python, </a><a class="link-muted" rel="tag" href="/tags/Pytorch/">Pytorch </a></div></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/img/alipay.jpg" alt="Alipay"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/img/wechat.jpg" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/cn/NerualNetworkTraining/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Training Strategy</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/cn/Loss-WhyZero/"><span class="level-item">Loss-WhyZero</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://aikenh.cn/cn/PyTorch/';
            this.page.identifier = 'cn/PyTorch/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'my-tech-blog-3' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/title.jpg" alt="AikenH"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">AikenH</p><p class="is-size-6 is-block">Future Full-Stack Developer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>ShenZhen</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">137</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">42</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">101</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/AikenH" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="ZhiHu" href="https://www.zhihu.com/people/Aiken-h"><i class="fab fa-zhihu"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="https://weibo.com/u/1788200627"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Steam" href="https://steamcommunity.com/id/AikenH/"><i class="fab fa-steam"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/AikenH"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Basic-Part基础设定部分"><span class="level-left"><span class="level-item">1</span><span class="level-item">Basic Part基础设定部分</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Tensor张量计算"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">Tensor张量计算</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#两个tensor的数乘"><span class="level-left"><span class="level-item">1.1.1</span><span class="level-item">两个tensor的数乘</span></span></a></li><li><a class="level is-mobile" href="#张量命名"><span class="level-left"><span class="level-item">1.1.2</span><span class="level-item">张量命名</span></span></a></li><li><a class="level is-mobile" href="#类型转换"><span class="level-left"><span class="level-item">1.1.3</span><span class="level-item">类型转换</span></span></a></li><li><a class="level is-mobile" href="#维度堆叠"><span class="level-left"><span class="level-item">1.1.4</span><span class="level-item">维度堆叠</span></span></a></li></ul></li><li><a class="level is-mobile" href="#基本的张量函数"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">基本的张量函数</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#选取划窗"><span class="level-left"><span class="level-item">1.2.1</span><span class="level-item">选取划窗</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Torch环境设置"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">Torch环境设置</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#pytorch中的随机种子初始化"><span class="level-left"><span class="level-item">1.3.1</span><span class="level-item">pytorch中的随机种子初始化</span></span></a></li><li><a class="level is-mobile" href="#nn-parameter"><span class="level-left"><span class="level-item">1.3.2</span><span class="level-item">nn.parameter()</span></span></a></li><li><a class="level is-mobile" href="#nn-Softmax中的dim"><span class="level-left"><span class="level-item">1.3.3</span><span class="level-item">nn.Softmax中的dim</span></span></a></li></ul></li><li><a class="level is-mobile" href="#测试、验证模块"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">测试、验证模块</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#基本编写"><span class="level-left"><span class="level-item">1.4.1</span><span class="level-item">基本编写</span></span></a></li><li><a class="level is-mobile" href="#model-eval-和model-train-的区别"><span class="level-left"><span class="level-item">1.4.2</span><span class="level-item">model.eval()和model.train()的区别</span></span></a></li><li><a class="level is-mobile" href="#with-torch-no-grad"><span class="level-left"><span class="level-item">1.4.3</span><span class="level-item">with torch.no_grad()</span></span></a></li><li><a class="level is-mobile" href="#模型的保存和读取专题"><span class="level-left"><span class="level-item">1.4.4</span><span class="level-item">模型的保存和读取专题</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#GPU相关的设置"><span class="level-left"><span class="level-item">2</span><span class="level-item">GPU相关的设置</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#查看GPU状态"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">查看GPU状态</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#设置默认GPU设备"><span class="level-left"><span class="level-item">2.1.1</span><span class="level-item">设置默认GPU设备</span></span></a></li><li><a class="level is-mobile" href="#设备基本信息"><span class="level-left"><span class="level-item">2.1.2</span><span class="level-item">设备基本信息</span></span></a></li></ul></li><li><a class="level is-mobile" href="#GPU使用率优化（注意事项）"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">GPU使用率优化（注意事项）</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#缓存爆炸问题"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">缓存爆炸问题</span></span></a></li><li><a class="level is-mobile" href="#运行效率优化"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">运行效率优化</span></span></a></li><li><a class="level is-mobile" href="#设置使用GPU的方式"><span class="level-left"><span class="level-item">2.2.3</span><span class="level-item">设置使用GPU的方式</span></span></a></li><li><a class="level is-mobile" href="#设置相应的随机种子"><span class="level-left"><span class="level-item">2.2.4</span><span class="level-item">设置相应的随机种子</span></span></a></li><li><a class="level is-mobile" href="#CUDA转换"><span class="level-left"><span class="level-item">2.2.5</span><span class="level-item">CUDA转换</span></span></a></li><li><a class="level is-mobile" href="#多GPU并行"><span class="level-left"><span class="level-item">2.2.6</span><span class="level-item">多GPU并行</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#CPU"><span class="level-left"><span class="level-item">3</span><span class="level-item">CPU</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#核心和线程数设置"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">核心和线程数设置</span></span></a></li></ul></li><li><a class="level is-mobile" href="#网络定义模块"><span class="level-left"><span class="level-item">4</span><span class="level-item">网络定义模块</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#数据定义模块"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">数据定义模块</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#利用TorchVision读取本地数据"><span class="level-left"><span class="level-item">4.1.1</span><span class="level-item">利用TorchVision读取本地数据</span></span></a></li><li><a class="level is-mobile" href="#torch-自定义Dataset后的使用"><span class="level-left"><span class="level-item">4.1.2</span><span class="level-item">torch 自定义Dataset后的使用</span></span></a></li><li><a class="level is-mobile" href="#Dataloader中的transformer（）："><span class="level-left"><span class="level-item">4.1.3</span><span class="level-item">Dataloader中的transformer（）：</span></span></a></li><li><a class="level is-mobile" href="#Dataloader中的参数"><span class="level-left"><span class="level-item">4.1.4</span><span class="level-item">Dataloader中的参数</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#collate-fn"><span class="level-left"><span class="level-item">4.1.4.1</span><span class="level-item">collate_fn()</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#编写模型"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">编写模型</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#模型基本单元"><span class="level-left"><span class="level-item">4.2.1</span><span class="level-item">模型基本单元</span></span></a></li><li><a class="level is-mobile" href="#模型参数共享："><span class="level-left"><span class="level-item">4.2.2</span><span class="level-item">模型参数共享：</span></span></a></li><li><a class="level is-mobile" href="#网络定义的方式对比分析"><span class="level-left"><span class="level-item">4.2.3</span><span class="level-item">网络定义的方式对比分析</span></span></a></li><li><a class="level is-mobile" href="#Detach-amp-detach"><span class="level-left"><span class="level-item">4.2.4</span><span class="level-item">Detach &amp; detach_</span></span></a></li><li><a class="level is-mobile" href="#模型调用的Tips"><span class="level-left"><span class="level-item">4.2.5</span><span class="level-item">模型调用的Tips</span></span></a></li><li><a class="level is-mobile" href="#Warm-up-factor"><span class="level-left"><span class="level-item">4.2.6</span><span class="level-item">Warm-up factor</span></span></a></li><li><a class="level is-mobile" href="#Weight-decay（L2）"><span class="level-left"><span class="level-item">4.2.7</span><span class="level-item">Weight decay（L2）</span></span></a></li><li><a class="level is-mobile" href="#Learning-Rate-Decay"><span class="level-left"><span class="level-item">4.2.8</span><span class="level-item">Learning Rate Decay</span></span></a></li></ul></li><li><a class="level is-mobile" href="#损失函数"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">损失函数</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#CrossEntropy交叉熵"><span class="level-left"><span class="level-item">4.3.1</span><span class="level-item">CrossEntropy交叉熵</span></span></a></li></ul></li><li><a class="level is-mobile" href="#优化器设计"><span class="level-left"><span class="level-item">4.4</span><span class="level-item">优化器设计</span></span></a></li><li><a class="level is-mobile" href="#模型参数初始化和架构查看方法"><span class="level-left"><span class="level-item">4.5</span><span class="level-item">模型参数初始化和架构查看方法</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#children、modules、parameters："><span class="level-left"><span class="level-item">4.5.1</span><span class="level-item">children、modules、parameters：</span></span></a></li><li><a class="level is-mobile" href="#初始化原则：（继续调研）"><span class="level-left"><span class="level-item">4.5.2</span><span class="level-item">初始化原则：（继续调研）</span></span></a></li><li><a class="level is-mobile" href="#典型的参数初始化方法"><span class="level-left"><span class="level-item">4.5.3</span><span class="level-item">典型的参数初始化方法</span></span></a></li></ul></li><li><a class="level is-mobile" href="#数据类型和维度"><span class="level-left"><span class="level-item">4.6</span><span class="level-item">数据类型和维度</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#输入数据的通道"><span class="level-left"><span class="level-item">4.6.1</span><span class="level-item">输入数据的通道</span></span></a></li><li><a class="level is-mobile" href="#标签的形式转换one-hot"><span class="level-left"><span class="level-item">4.6.2</span><span class="level-item">标签的形式转换one-hot</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#Visualize-可视化部分"><span class="level-left"><span class="level-item">5</span><span class="level-item">Visualize 可视化部分</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Tensorboard-in-Pytorch"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">Tensorboard in Pytorch</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Histogram-直方图参数统计"><span class="level-left"><span class="level-item">5.1.1</span><span class="level-item">Histogram 直方图参数统计</span></span></a></li><li><a class="level is-mobile" href="#Embedding-Projection"><span class="level-left"><span class="level-item">5.1.2</span><span class="level-item">Embedding Projection</span></span></a></li><li><a class="level is-mobile" href="#PR-CURVE"><span class="level-left"><span class="level-item">5.1.3</span><span class="level-item">PR_CURVE</span></span></a></li><li><a class="level is-mobile" href="#Add-TEXT"><span class="level-left"><span class="level-item">5.1.4</span><span class="level-item">Add_TEXT</span></span></a></li><li><a class="level is-mobile" href="#ADD-Figure"><span class="level-left"><span class="level-item">5.1.5</span><span class="level-item">ADD_Figure</span></span></a></li></ul></li><li><a class="level is-mobile" href="#可视化神经网络热力图（CAM）"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">可视化神经网络热力图（CAM）</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#算法原理"><span class="level-left"><span class="level-item">5.2.1</span><span class="level-item">算法原理</span></span></a></li><li><a class="level is-mobile" href="#代码实现："><span class="level-left"><span class="level-item">5.2.2</span><span class="level-item">代码实现：</span></span></a></li></ul></li><li><a class="level is-mobile" href="#BUGs"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">BUGs</span></span></a></li></ul></li><li><a class="level is-mobile" href="#DEBUG"><span class="level-left"><span class="level-item">6</span><span class="level-item">DEBUG</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-ImportError-cannot-import-name-‘PILLOW-VERSION’"><span class="level-left"><span class="level-item">6.1</span><span class="level-item">1.ImportError: cannot import name ‘PILLOW_VERSION’</span></span></a></li><li><a class="level is-mobile" href="#2-模型参数-amp-计算量统计-and-Debug输出"><span class="level-left"><span class="level-item">6.2</span><span class="level-item">2.模型参数&amp;计算量统计 and Debug输出</span></span></a></li><li><a class="level is-mobile" href="#3-PyTorch加载预训练模型"><span class="level-left"><span class="level-item">6.3</span><span class="level-item">3.PyTorch加载预训练模型</span></span></a></li><li><a class="level is-mobile" href="#4-some-of-the-strides-of-a-given-numpy-array-are-negative"><span class="level-left"><span class="level-item">6.4</span><span class="level-item">4.some of the strides of a given numpy array are negative.</span></span></a></li><li><a class="level is-mobile" href="#5-读取loader的时候图像的大小不一"><span class="level-left"><span class="level-item">6.5</span><span class="level-item">5.读取loader的时候图像的大小不一</span></span></a></li><li><a class="level is-mobile" href="#6-bus-error-dataloader-num-worker"><span class="level-left"><span class="level-item">6.6</span><span class="level-item">6.bus error dataloader num_worker</span></span></a></li><li><a class="level is-mobile" href="#7-bus-error：insufficient-shared-memory（shm）"><span class="level-left"><span class="level-item">6.7</span><span class="level-item">7.bus error：insufficient shared memory（shm）</span></span></a></li><li><a class="level is-mobile" href="#8-训练过程中Cache和Memory的占用逐渐升高"><span class="level-left"><span class="level-item">6.8</span><span class="level-item">8.训练过程中Cache和Memory的占用逐渐升高</span></span></a></li><li><a class="level is-mobile" href="#9-梯度爆炸问题，算法没有学习效果"><span class="level-left"><span class="level-item">6.9</span><span class="level-item">9.梯度爆炸问题，算法没有学习效果</span></span></a></li><li><a class="level is-mobile" href="#10-类型转换问题汇总"><span class="level-left"><span class="level-item">6.10</span><span class="level-item">10.类型转换问题汇总</span></span></a></li><li><a class="level is-mobile" href="#11-数据维度不对应问题汇总"><span class="level-left"><span class="level-item">6.11</span><span class="level-item">11.数据维度不对应问题汇总</span></span></a></li><li><a class="level is-mobile" href="#12-取出具体数值时候的问题"><span class="level-left"><span class="level-item">6.12</span><span class="level-item">12.取出具体数值时候的问题</span></span></a></li><li><a class="level is-mobile" href="#13-CPU占用99"><span class="level-left"><span class="level-item">6.13</span><span class="level-item">13.CPU占用99%</span></span></a></li><li><a class="level is-mobile" href="#14-预测值全为0，模型收敛到奇怪的地方，损失保持一致（全均等）"><span class="level-left"><span class="level-item">6.14</span><span class="level-item">14. 预测值全为0，模型收敛到奇怪的地方，损失保持一致（全均等）</span></span></a></li><li><a class="level is-mobile" href="#15-模型部分：-训练中模型准确率不上升"><span class="level-left"><span class="level-item">6.15</span><span class="level-item">15.模型部分： 训练中模型准确率不上升</span></span></a></li><li><a class="level is-mobile" href="#16-On-entry-to-SGEMM-parameter-number-8-had-an-illegal-value"><span class="level-left"><span class="level-item">6.16</span><span class="level-item">16. On entry to SGEMM parameter number 8 had an illegal value</span></span></a></li><li><a class="level is-mobile" href="#17-CUDA-error-device-side-assert-triggered-CUDA-kernel-errors-might-be-asynchronously-reported-at-some-other-API-call"><span class="level-left"><span class="level-item">6.17</span><span class="level-item">17. CUDA error: device-side assert triggered CUDA kernel errors might be asynchronously reported at some other API call</span></span></a></li><li><a class="level is-mobile" href="#RuntimeError-the-derivative-for-target-is-not-implemented"><span class="level-left"><span class="level-item">6.18</span><span class="level-item">RuntimeError the derivative for target is not implemented</span></span></a></li><li><a class="level is-mobile" href="#Only-Tensors-created-explicitly-by-the-user-graph-leaves-support-the-deepcopy-protocol-at-the-moment"><span class="level-left"><span class="level-item">6.19</span><span class="level-item">Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><figure class="media-left"><a class="image" href="/cn/pwsh_BgTask/"><img src="/img/header_img/lml_bg18.jpg" alt="Windows Powershell 01 后台任务"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-01-28T07:44:25.000Z">2024-01-28</time></p><p class="title"><a href="/cn/pwsh_BgTask/">Windows Powershell 01 后台任务</a></p><p class="categories"><a href="/categories/Windows/">Windows</a> / <a href="/categories/Windows/Powershell/">Powershell</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/cn/StableDiffusionWebUI%E9%89%B4%E6%9D%83%E8%AE%BE%E8%AE%A1/"><img src="/img/header_img/lml_bg16.jpg" alt="StableDiffusionWebUI鉴权设计"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-01-27T07:42:36.000Z">2024-01-27</time></p><p class="title"><a href="/cn/StableDiffusionWebUI%E9%89%B4%E6%9D%83%E8%AE%BE%E8%AE%A1/">StableDiffusionWebUI鉴权设计</a></p><p class="categories"><a href="/categories/Machine-Learning/">Machine Learning</a> / <a href="/categories/Machine-Learning/AIGC/">AIGC</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/cn/BackupToolsForHomeServer/"><img src="/img/header_img/lml_bg14.jpg" alt="家庭服务器的备份工具选择"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-12-29T00:04:06.000Z">2023-12-29</time></p><p class="title"><a href="/cn/BackupToolsForHomeServer/">家庭服务器的备份工具选择</a></p><p class="categories"><a href="/categories/NAS/">NAS</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/cn/%E5%AE%B6%E5%BA%AD%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%95%B4%E4%BD%93%E6%96%B9%E6%A1%88/"><img src="/img/header_img/lml_bg15.jpg" alt="家庭服务器整体方案"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-12-20T00:24:36.000Z">2023-12-20</time></p><p class="title"><a href="/cn/%E5%AE%B6%E5%BA%AD%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%95%B4%E4%BD%93%E6%96%B9%E6%A1%88/">家庭服务器整体方案</a></p><p class="categories"><a href="/categories/NAS/">NAS</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/cn/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%AE%B6%E5%BA%AD%E5%BD%B1%E9%9F%B3%E4%B8%AD%E5%BF%832/"><img src="/img/header_img/lml_bg1.jpg" alt="树莓派家庭影音中心2"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-11-11T04:08:37.000Z">2023-11-11</time></p><p class="title"><a href="/cn/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%AE%B6%E5%BA%AD%E5%BD%B1%E9%9F%B3%E4%B8%AD%E5%BF%832/">树莓派家庭影音中心2</a></p><p class="categories"><a href="/categories/Dev/">Dev</a> / <a href="/categories/Dev/Raspberry-pie/">Raspberry-pie</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Aiken&#039;s Blog</a><p class="is-size-7"><span>&copy; 2024 AikenH</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_pv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span> and <span id="busuanzi_container2_site_uv"><span id="busuanzi_value_site_pv">0</span>&nbsp;visits</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/night.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>