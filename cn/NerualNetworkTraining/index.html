<!doctype html>
<html lang="cn"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Training Strategy - AikenH Blogs</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Aiken Hong"><meta name="msapplication-TileImage" content="/img/pokemon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Aiken Hong"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="@Aiken 2020， 主要针对神经网络的训练过程中的一些基础策略的调整，比如当训练的曲线出现一定的问题的时候，我们应该怎么去调整我们训练过程中的策略。 参数调整过程中最重要的就是优化器（优化或者说是下降算法）和学习率（优化算法的核心参数），此外像是数据增强策略还是Normalization策略，都能极大的影响一个模型的好坏。 优化器Some Material实际上虽然有很多的优化算法，但是到最"><meta property="og:type" content="blog"><meta property="og:title" content="Training Strategy"><meta property="og:url" content="http://aikenh.cn/cn/NerualNetworkTraining/"><meta property="og:site_name" content="AikenH Blogs"><meta property="og:description" content="@Aiken 2020， 主要针对神经网络的训练过程中的一些基础策略的调整，比如当训练的曲线出现一定的问题的时候，我们应该怎么去调整我们训练过程中的策略。 参数调整过程中最重要的就是优化器（优化或者说是下降算法）和学习率（优化算法的核心参数），此外像是数据增强策略还是Normalization策略，都能极大的影响一个模型的好坏。 优化器Some Material实际上虽然有很多的优化算法，但是到最"><meta property="og:locale" content="cn"><meta property="og:image" content="http://aikenh.cn/img/header_img/lml_bg37.jpg"><meta property="article:published_time" content="2021-12-16T00:34:44.000Z"><meta property="article:modified_time" content="2023-10-30T09:27:38.600Z"><meta property="article:author" content="AikenH"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="Acceleration"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://aikenh.cn/img/header_img/lml_bg37.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://aikenh.cn/cn/NerualNetworkTraining/"},"headline":"Training Strategy","image":["http://aikenh.cn/img/header_img/lml_bg37.jpg"],"datePublished":"2021-12-16T00:34:44.000Z","dateModified":"2023-10-30T09:27:38.600Z","author":{"@type":"Person","name":"AikenH"},"publisher":{"@type":"Organization","name":"AikenH Blogs","logo":{"@type":"ImageObject","url":{"text":"Aiken's Blog"}}},"description":"@Aiken 2020， 主要针对神经网络的训练过程中的一些基础策略的调整，比如当训练的曲线出现一定的问题的时候，我们应该怎么去调整我们训练过程中的策略。 参数调整过程中最重要的就是优化器（优化或者说是下降算法）和学习率（优化算法的核心参数），此外像是数据增强策略还是Normalization策略，都能极大的影响一个模型的好坏。 优化器Some Material实际上虽然有很多的优化算法，但是到最"}</script><link rel="canonical" href="http://aikenh.cn/cn/NerualNetworkTraining/"><link rel="icon" href="/img/pokemon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-dark.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css" title="default"><link rel="alternate stylesheet" href="/css/cyberpunk.css" title="cyberpunk"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="/atom.xml" title="AikenH Blogs" type="application/atom+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Aiken&#039;s Blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/AikenH"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-lightbulb" id="night-icon"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/img/header_img/lml_bg37.jpg" alt="Training Strategy"></span></div><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>Training Strategy</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2021-12-16T00:34:44.000Z" title="2021-12-16T00:34:44.000Z">2021-12-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-10-30T09:27:38.600Z" title="2023/10/30 17:27:38">2023-10-30</time></span><span class="level-item"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></span><span class="level-item">27 minutes read (About 4090 words)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><div class="content"><p>@Aiken 2020，</p>
<p>主要针对神经网络的训练过程中的一些基础策略的调整，比如当训练的曲线出现一定的问题的时候，我们应该怎么去调整我们训练过程中的策略。</p>
<p>参数调整过程中最重要的就是优化器（优化或者说是下降算法）和学习率（优化算法的核心参数），此外像是数据增强策略还是Normalization策略，都能极大的影响一个模型的好坏。</p>
<h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p><a target="_blank" rel="noopener" href="https://wizardforcel.gitbooks.io/learn-dl-with-pytorch-liaoxingyu/content/">Some Material</a><br>实际上虽然有很多的优化算法，但是到最后最常用的还是 SGD+Mon 和 Adam两种：</p>
<p>Adam主要的有事在于自适应学习率，他对我们设计的学习率实际上没有那么敏感，但是在具体实验中往往不会有调的好的SGD那么好，只是在SGD的参数调整中会比较费劲。</p>
<p>但是有了根据patient调整lr的scheduler后，我们基本上可以使用SGD做一个较为简单的调整，只要设计好初始的lr的实验以及用来调整学习率的参数值。</p>
<h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2><p>$\omega^{n} \leftarrow \omega^{n}-\eta \frac{\partial L}{\partial \omega^{n}}$ 其中的权重就是学习率lr，</p>
<p>==Basic==</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>学习率大</th>
<th>学习率小</th>
</tr>
</thead>
<tbody>
<tr>
<td>学习速度</td>
<td>快</td>
<td>慢</td>
</tr>
<tr>
<td>使用情景</td>
<td>刚开始训练时</td>
<td>一定的次数过后</td>
</tr>
<tr>
<td>副作用</td>
<td>1. Loss爆炸 2.振荡</td>
<td>1.过拟合 2.收敛速度慢</td>
</tr>
</tbody>
</table>
</div>
<h3 id="学习率的基本设置"><a href="#学习率的基本设置" class="headerlink" title="学习率的基本设置"></a>学习率的基本设置</h3><span id="more"></span>
<p>在训练过程中，一般根据训练轮数设置动态变化的学习率。</p>
<ul>
<li>刚开始训练时：学习率以 0.01 ~ 0.001 为宜。</li>
<li>一定轮数过后：逐渐减缓。</li>
<li>接近训练结束：学习速率的衰减应该在100倍以上。</li>
</ul>
<p><strong>Note：</strong><br>如果是 <strong>迁移学习</strong> ，由于模型已在原始数据上收敛，此时应设置较小学习率 (≤10−4) 在新数据上进行 <strong>微调</strong> 。</p>
<h3 id="学习率变化方法"><a href="#学习率变化方法" class="headerlink" title="学习率变化方法"></a>学习率变化方法</h3><p>==warm up==</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/338066667/answer/771252708">warm up为什么有用</a></p>
<p>warm up衰减策略与上述的策略有些不同，它是先从一个极低的学习率开始增加，增加到某一个值后再逐渐减少, 这点上倒是和Cosine Anneal LR有一定的相似之处，将这两种结合起来是一种常见的训练策略：</p>
<p>这样训练模型更加稳定，因为在刚开始时模型的参数都是随机初始化的，此时如果学习率应该取小一点，这样就不会使模型一下子跑偏。</p>
<p>而这样的跑偏对于<strong>大模型</strong>而言，可能是导致很严重的影响，后面收敛了也可能不会达到最佳的效果，一开始的跑偏，可能会造成准确率在后面的严重结果。<br><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20211216001833.png" alt="warmup"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">   <span class="hljs-comment"># MultiStepLR without warm up</span><br>   scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \<br>							milestones=args.milestones, gamma=<span class="hljs-number">0.1</span>)<br><br>   <span class="hljs-comment"># warm_up_with_multistep_lr</span><br>   warm_up_with_multistep_lr = <span class="hljs-keyword">lambda</span> epoch: epoch / args.warm_up_epochs <span class="hljs-keyword">if</span> \<br>epoch &lt;= args.warm_up_epochs <span class="hljs-keyword">else</span> <span class="hljs-number">0.1</span>**<span class="hljs-built_in">len</span>([m <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> args.milestones <span class="hljs-keyword">if</span> m &lt;= epoch])<br>   scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer,<br>											  lr_lambda=warm_up_with_multistep_lr)<br><br>   <span class="hljs-comment"># warm_up_with_cosine_lr</span><br>   warm_up_with_cosine_lr = <span class="hljs-keyword">lambda</span> epoch: epoch / args.warm_up_epochs <span class="hljs-keyword">if</span> \<br>epoch &lt;= args.warm_up_epochs <span class="hljs-keyword">else</span> <span class="hljs-number">0.5</span> *\<br>( math.cos((epoch - args.warm_up_epochs) /(args.epochs - args.warm_up_epochs) * math.pi) + <span class="hljs-number">1</span>)<br>   scheduler = torch.optim.lr_scheduler.LambdaLR( optimizer, lr_lambda=warm_up_with_cosine_lr)<br><br></code></pre></td></tr></table></figure>
<p>==Scheduler Setting：==</p>
<p>分组的学习率也能通过scheduler进行学习率的更新，可以放心使用。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>轮数减缓</th>
<th>指数减缓</th>
<th>分数减缓</th>
</tr>
</thead>
<tbody>
<tr>
<td>step decay</td>
<td>exponential decay</td>
<td>1/t1/t decay</td>
</tr>
<tr>
<td>每N轮学习率减半</td>
<td>学习率按训练轮数增长指数插值递减</td>
<td>lrt=lr0/(1+kt)，k 控制减缓幅度，t 为训练轮数</td>
</tr>
</tbody>
</table>
</div>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42662358/article/details/93732852">Pytorch的Scheduler</a><br>pytorch中提供了很多scheduler的方法，其中用的最多的可能还是<code>multistep</code>，考虑到后续可能会用到基于指标调整的学习率，这里特别提一个<code>cosine</code>的学习率调整策略，它的学习率呈现的是一种周期变化的样子。</p>
<p>==Custom Scheduler==</p>
<p>Pytorch为可能的自定义提供了一个方便的Scheduler接口，<code>ReduceLROnPlateau</code>，通过<code>step</code> 调用指标的变化，进行学习率的调整，极其方便。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="hljs-string">&#x27;max&#x27;</span>,  <br>			factor=<span class="hljs-number">0.1</span>, patience=<span class="hljs-number">10</span>, verbose=<span class="hljs-literal">False</span>, threshold=<span class="hljs-number">1e-4</span>, <br>			threshold_model=<span class="hljs-string">&#x27;rel&#x27;</span>, cooldown=<span class="hljs-number">0</span>, min_lr=<span class="hljs-number">1e-8</span>)<br><br>scheduler.step(acc)<br></code></pre></td></tr></table></figure>
<p>基本的参数包括： </p>
<ul>
<li>mode 很好理解，max（acc），min（loss）值</li>
<li>factor 学习率下降的参数</li>
<li>patience 多少次没有变化就调整</li>
<li>cooldown 调整后多久的冷却期</li>
<li>threshold，threshold_model 调整我们的动态上下限</li>
</ul>
<p><strong>threshold (float)</strong> – Threshold for measuring the new optimum, to only focus on significant changes. Default: 1e-4.</p>
<p><strong>threshold_mode (str)</strong> – One of rel, abs. In rel mode, <code>dynamic_threshold = best * ( 1 + threshold )</code> in ‘max’ mode or <code>best * ( 1 - threshold )</code> in min mode. In abs mode, dynamic_threshold = best + threshold in max mode or best - threshold in min mode. Default: ‘rel’.</p>
<h3 id="分析学习率的大小"><a href="#分析学习率的大小" class="headerlink" title="分析学习率的大小"></a>分析学习率的大小</h3><p>在训练过程中可视化Loss下降曲线是相当重要的，那么针对Loss出现异常的情况我们应该怎么样去调整使得Loss逐步趋于正常呢？</p>
<p><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911210315.png" alt="image-20201120105459815"></p>
<p>曲线 初始时 上扬 [红线]：（直接起飞梯度爆炸）<br>初始 <strong>学习率过大</strong> 导致 振荡，应减小学习率，并从头开始训练 。</p>
<p>曲线 初始时 强势下降 没多久 归于水平 [紫线]：<br>Solution：后期<strong>学习率过大</strong>导致无法拟合，应减小学习率，并重新训练后几轮 。</p>
<p>曲线 全程缓慢 [黄线]：<br>Solution：初始 <strong>学习率过小</strong> 导致收敛慢，应增大学习率，并从头开始训练 。</p>
<h2 id="过拟合欠拟合现象"><a href="#过拟合欠拟合现象" class="headerlink" title="过拟合欠拟合现象"></a>过拟合欠拟合现象</h2><p>过拟合-&gt;各种泛化能力差的现象在这里我个人对这个现象的定义为以下的几种：</p>
<ul>
<li>训练阶段的准确率和验证/测试阶段的准确率相差大</li>
<li>训练过程和验证过程中的损失下降不一致，验证集中的准确率没有随着训练提升</li>
<li>典型的过拟合导致这样的现象</li>
</ul>
<p>下面整理一下<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ah411t7Pp?spm_id_from=333.999.0.0">李沐对该部分的讲解</a></p>
<p><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20211114181130.png" alt="image-20211114181128291"></p>
<p>bug部分可能是由于增强做的过高或者问题太难, 但是在正常的表现下也不应该出现这种问题, 误差应该是差不多的.</p>
<p><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20211114181412.png" alt="image-20211114181411611"></p>
<p>上面的这张图片也说明了, 我们模型和问题的难度是需要相互匹配的, 如果不匹配就会出现各种各样的问题, 模型的复杂度, 通常可以从可学习参数的数来进行简单的判断的. </p>
<h3 id="过拟合问题定义和分析"><a href="#过拟合问题定义和分析" class="headerlink" title="过拟合问题定义和分析"></a>过拟合问题定义和分析</h3><p>定义：模型对于训练集的假设过度严格，导致对训练集的数据拟合的“很好”，但是在测试验证集中效果不理想。可能会出现的典型现象如下：</p>
<ol>
<li>验证损失先下降后上升</li>
<li>训练集和测试集稳定后的准确率相差很大</li>
</ol>
<p>下面这张图, 显示的是模型的复杂度和相应的泛化和训练误差之间的关系, 在训练的时候复杂度还是需要自我调整. </p>
<p><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211026161951.png" alt="image-20211026161949994"></p>
<p><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20211114182734.png" alt="image-20211114182733584"></p>
<h3 id="收敛过快泛化能力差"><a href="#收敛过快泛化能力差" class="headerlink" title="收敛过快泛化能力差"></a>收敛过快泛化能力差</h3><p>过拟合的一种衍生问题，当模型在训练集中快速收敛，在这种情况下可能会陷入极小值，由于损失太小，模型参数难以跳出极小值点，这种情况下，如果不加以约束会影响泛化能力，可以考虑使用，</p>
<ul>
<li><code>flood</code> 方法来设计我们的loss（效果未知，作为一种策略把，保证模型能够有一定量的损失，同时希望验证集上的损失能够下降到一个平缓的地方，来保证泛化能力）</li>
</ul>
<h3 id="产生的原因分析"><a href="#产生的原因分析" class="headerlink" title="产生的原因分析"></a>产生的原因分析</h3><ol>
<li>训练数据样本单一，数据量不足</li>
<li>噪声干扰过大：失去了真实的输入输出之间的关系</li>
<li>模型的复杂度太高，足够死记硬背所有训练集的数据，导致不知道变通</li>
</ol>
<h3 id="数据的复杂度分析"><a href="#数据的复杂度分析" class="headerlink" title="数据的复杂度分析:"></a>数据的复杂度分析:</h3><p>大部分情况下进行数据的对比还是一个比较直观的情况, 其实可以从这几个方面进行比较</p>
<ul>
<li>数据集的样本数, 类别</li>
<li>数据集的分辨率</li>
<li>数据的时空结构和多样性</li>
</ul>
<h3 id="常见的解决方式"><a href="#常见的解决方式" class="headerlink" title="常见的解决方式"></a>常见的解决方式</h3><ol>
<li><p>:zap:添加正则化L1，L2（weight decay），</p>
<p>weight decay等权重下降的方法，需要熟练掌握在pytorch上的设置</p>
</li>
<li><p>:zap:降低模型的复杂度，对应模型的设计和问题的规模需要更好的分析。</p>
</li>
<li><p>:zap:数据增强，使得数据的多样化指标进一步上升</p>
</li>
<li><p>:zap:Dropout，Early Stop</p>
</li>
<li><p>BatchNormalization</p>
</li>
<li><p>集成学习方法，通过对多个模型进行集成来降低单一模型的过拟合风险</p>
</li>
</ol>
<h3 id="图像增强"><a href="#图像增强" class="headerlink" title="图像增强"></a>图像增强</h3><p>这里我们为图像增强另外开一个文档，图像增强的内容实际上可以考虑《数字图像处理》的这样一门课。</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/tV5eSx73fzMovq0d7Jvu9Q">自监督学习和对比学习 (qq.com)</a></p>
<p>文中提到对准确率提升最多的一些增强方式是如下的三种：</p>
<ul>
<li>Crop，Resize ，Flip</li>
<li>Colour Distortion</li>
<li>Gaussian Blur</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<br><span class="hljs-comment"># Size used in SimCLR</span><br>size = <span class="hljs-number">224</span><br>crop_resize_flip = transforms.Compose([transforms.RandomResizedCrop(size, scale=(<span class="hljs-number">0.08</span>, <span class="hljs-number">1.0</span>), ratio=(<span class="hljs-number">3</span>/<span class="hljs-number">4</span>, <span class="hljs-number">4</span>/<span class="hljs-number">3</span>)),<br>                                       transforms.RandomHorizontalFlip(p=<span class="hljs-number">0.5</span>)])<br><br><span class="hljs-comment"># Higher means stronger </span><br>s = <span class="hljs-number">1.0</span><br><span class="hljs-comment"># 0.8*s and 0.2*s are from the paper</span><br>colour_jitter = transforms.ColorJitter(brightness=<span class="hljs-number">0.8</span>*s, contrast=<span class="hljs-number">0.8</span>*s, saturation=<span class="hljs-number">0.8</span>*s, hue=<span class="hljs-number">0.2</span>*s)<br>colour_jitter = transforms.RandomApply([colour_jitter], p=<span class="hljs-number">0.8</span>)<br>colour_distortion = transforms.Compose([colour_jitter,<br>                                        transforms.RandomGrayscale(p=<span class="hljs-number">0.2</span>)])<br><br>kernel_size = <span class="hljs-built_in">int</span>(<span class="hljs-number">0.1</span>*size)<br><span class="hljs-comment"># The size of the kernel must be odd</span><br>kernel_size = kernel_size <span class="hljs-keyword">if</span> kernel_size%<span class="hljs-number">2</span> == <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> kernel_size+<span class="hljs-number">1</span><br>gaussian_blur = transforms.GaussianBlur(kernel_size, sigma=(<span class="hljs-number">0.1</span>, <span class="hljs-number">2.0</span>))<br>gaussian_blur = transforms.RandomApply([gaussian_blur], p=<span class="hljs-number">0.5</span>)<br>                                       <br>augment = transforms.Compose([crop_resize_flip,<br>                              colour_distortion,<br>                              gaussian_blur])<br><br></code></pre></td></tr></table></figure>
<p><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20211118153623.png" alt="image-20211118153622239"></p>
<h2 id="早停法"><a href="#早停法" class="headerlink" title="早停法"></a>早停法</h2><p><a target="_blank" rel="noopener" href="https://microsoft.github.io/ai-edu/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/A2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/en-us/Step7%20-%20DNN/16.4-%E6%97%A9%E5%81%9C%E6%B3%95.html">MicroSoft Ai 教程 ES</a></p>
<p>因为准确率都不再提高了，损失值反而上升了，再继续训练也是无益的，只会浪费训练的时间。那么该做法的一个重点便是怎样才认为验证集不再提高了呢？并不是说准确率一降下来便认为不再提高了，因为可能在这个Epoch上，准确率降低了，但是随后的Epoch准确率又升高了，所以不能根据一两次的连续降低就判断不再提高。</p>
<p>对模型进行训练的过程即是对模型的参数进行学习更新的过程，这个参数学习的过程往往会用到一些迭代方法，如梯度下降（Gradient descent）学习算法。Early stopping便是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。</p>
<p>更好的一个方式应该是使用一个类来进行计数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TrainingTrace</span>():<br>	<span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, need_earlystop=<span class="hljs-literal">False</span>, patience=<span class="hljs-number">10</span>, mode=<span class="hljs-string">&#x27;max&#x27;</span></span>):<br>		self.early_stop = need_earlystop<br>		self.patience = patience<br>		self.patience_count = <span class="hljs-number">0</span><br>		self.last_vid_metrric = <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>) <span class="hljs-keyword">if</span> model ==<span class="hljs-string">&#x27;min&#x27;</span> <span class="hljs-keyword">else</span> <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;-inf&#x27;</span>)<br>		self.compare = new_min <span class="hljs-keyword">if</span> model == <span class="hljs-string">&#x27;min&#x27;</span> <span class="hljs-keyword">else</span> new_max<br>	<br>	<span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self, value</span>):<br>		<br></code></pre></td></tr></table></figure>
<p>在得到早停的迭代次数和权重矩阵参数后，后续有几种方法可以选择。</p>
<p><strong>彻底停止</strong><br>就是啥也不做了，最多再重复几次早停的试验，看看是不是稳定，然后就使用做为训练结果。</p>
<p><strong>再次训练</strong><br>由于第一次早停是通过验证集计算loss值来实现的，所以这次不再分训练集和验证集，记住了早停时的迭代次数，可以重新初始化权重矩阵参数，使用所有数据再次训练，然后到达第一次的时停止。</p>
<p>但是由于样本多了，更新批次也会变多，所以可以比较两种策略：</p>
<p>1) 总迭代次数epoch保持不变 2) 总更新梯度的次数保持不变</p>
<p>优点：使用更多的样本可以达到更好的泛化能力。</p>
<p>缺点：需要重新花时间训练。</p>
<p><strong>继续训练</strong><br>得到后，用全部训练数据（不再分训练集和验证集），在此基础上继续训练若干轮，并且继续用以前的验证集来监控损失函数值，如果能得到比以前更低的损失值，将会是比较理想的情况。</p>
<p>优点：可以避免重新训练的成本。</p>
<p>缺点：有可能不能达到目的，损失值降不到理想位置，从而不能终止训练。</p>
<h2 id="效率优化"><a href="#效率优化" class="headerlink" title="效率优化"></a>效率优化</h2><p>and there are some tips in this <a target="_blank" rel="noopener" href="https://sagivtech.com/2017/09/19/optimizing-pytorch-training-code/">article</a>, we should read and learn about it</p>
<p>这一部分希望通过trick或者对应的一些代码技巧，优化训练过程中带来的资源占用和损耗，进一步提升训练时效性和资源上的有效利用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># making relu inplace will save memory </span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">inplace_relu</span>(<span class="hljs-params">m</span>):<br>    classname = m.__class__.__name__<br>    <span class="hljs-keyword">if</span> classname.find(<span class="hljs-string">&#x27;ReLU&#x27;</span>) != -<span class="hljs-number">1</span>:<br>        m.inplace=<span class="hljs-literal">True</span><br><span class="hljs-comment"># we need to learn this function</span><br>model.apply(inplace_relu)<br></code></pre></td></tr></table></figure>
<p>relu(inplace = True)</p>
<h3 id="rapidAI"><a href="#rapidAI" class="headerlink" title="rapidAI"></a>rapidAI</h3><p>Thanks to Nvidia, we could using np, spicy, pandas, sklearn on CUDA, which is much more faster. Achieve this by those repo: cuml for sklearn, cupy for numpy and spicy, cudf for dataframe and so on.</p>
<p>借助这几个仓库的文档, 我们可以学习如何调用这些库去加速和实现我们的代码. </p>
<p>在这里要注意的是, 使用这几个仓库的同时会<strong>引入更多的数据类型</strong>, 以及<strong>设备存储</strong>情况, 我们要在必要的时候对数据的存储位置进行<strong>分析和迁移</strong>.</p>
<p>过于频繁的数据移动可能反而会减慢运行速度, 但是如果是后续不需要的数据我们可以进行迁移. </p>
<p><strong>Install</strong> </p>
<ol>
<li>如果版本和torch的匹配(old version) 10.2 可以通过以下的命令安装cuml, 但是要注意panda版本 == 1.3.0, 首先对panda版本进行修改, 这种时候可能使用pip结合conda是一个更好的选择 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">	<br></code></pre></td></tr></table></figure></li>
<li>如果版本不匹配, 我们可以首先配置rapidai的环境, 在安装pytorch即可, 或者使用nvidia发布的相同cuda版本的pytorch.</li>
</ol>
<h3 id="torch-Cuda-AMP"><a href="#torch-Cuda-AMP" class="headerlink" title="torch.Cuda.AMP"></a>torch.Cuda.AMP</h3><p>使用Torch自带的AMP取代APEX的AMP进行优化，在&gt;=1.6的情况下，Torch已经自动支持了AMP混合, 而且事实证明在大多数情况下, Torch对amp的支持相比APEX来说要更加稳定和性能友好。</p>
<p>使用方法：<br>较为简单，只需要在训练的主流程中进行如下的嵌入<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.cuda.amp <span class="hljs-keyword">import</span> autocast, GradScaler<br><br><span class="hljs-comment"># 在训练最开始的阶段实例化一个GradScaler对象</span><br>scaler = GradScaler()<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> epochs:<br>	<span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> iterators:<br>		...<br>		<br>		<span class="hljs-comment"># model and loss</span><br>		<span class="hljs-keyword">with</span> autocast():<br>			out = model(<span class="hljs-built_in">input</span>)<br>			loss = loss_fn(output, target)<br>		<br>		<span class="hljs-comment"># and change the update and backward phas</span><br>		<span class="hljs-comment"># 放大loss</span><br>		scaler.scale(loss).backward()<br>		<span class="hljs-comment"># 对inf和nan进行判断，没有问题的话就进行step</span><br>		scaler.step(optimizer)<br>		<span class="hljs-comment"># 是否对scaler进行更新</span><br>		scaler.update()<br>		<br>		<br></code></pre></td></tr></table></figure></p>
<h3 id="APEX-显存优化"><a href="#APEX-显存优化" class="headerlink" title="APEX_显存优化"></a>APEX_显存优化</h3><p>this session is write for the nvidia module <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/apex">APEX</a> which can save a lot of memory and accelerate the training speed. we should learn how to use it .</p>
<p>通过APEX好像能优化接近50%的显存，而且在修改原框架代码中的要求很小，所以在这里有必要通过APEX去优化我们的框架</p>
<p>理论参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/79887894">基于Apex的混合精度加速</a>；</p>
<p>其中<code>opt_level</code>分别表示：O0纯FP32，O1混合精度训练，O2几乎FP16除了BN，O3纯FP16很不稳定，但是速度最快</p>
<p><strong>安装</strong>：</p>
<ul>
<li><p>验证cuda版本，验证torch的cuda版本</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">nvcc -V<br><span class="hljs-comment"># nvcc 很可能会找不到命令，去如下路径搜索是否cuda正确安装</span><br><span class="hljs-built_in">cd</span> /usr/local/cuda*/bin<br><span class="hljs-comment"># 其中若有nvcc命令的话可以直接执行</span><br>nvcc -V<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <br><span class="hljs-built_in">print</span>(torch.version.cuda)<br></code></pre></td></tr></table></figure>
</li>
<li><p>安装apex</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/NVIDIA/apex<br><span class="hljs-built_in">cd</span> apex<br>pip install -v --no-cache-dir --global-option=<span class="hljs-string">&quot;--cpp_ext&quot;</span> --global-option=<span class="hljs-string">&quot;--cuda_ext&quot;</span> ./<br><br></code></pre></td></tr></table></figure>
</li>
<li><p>import验证安装成功</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> apex<br></code></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>使用</strong>：</p>
<p>参考官方示例，我们可以知道APEX的使用场景主要集中在几个部分：</p>
<p>model,optimizer,loss upgrade and parallel</p>
<p>故而我们对原始代码修改或添加如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> apex <span class="hljs-keyword">import</span> amp<br><span class="hljs-keyword">from</span> apex.parallel <span class="hljs-keyword">import</span> DistributedDataParallel<br><br>model = resnet()<br>optimizer = torch.optim.SGD(model.parameters(),lr=<span class="hljs-number">1e-3</span>)<br><span class="hljs-comment"># MODEL PART: after model and optimizer design</span><br>model, optimizer = amp.initialize(model, optimizer, opt_level = <span class="hljs-string">&quot;O1&quot;</span>)<br><br><span class="hljs-comment"># DISTRIBUTION PART:</span><br><span class="hljs-comment"># replace nn.parallel.DistributedDataParallel()</span><br>model = DistributedDataParallel(model)<br><br><span class="hljs-comment"># LOSS PART:</span><br><span class="hljs-comment"># replace the loss BP process</span><br><br><span class="hljs-comment"># loss.backward()</span><br><span class="hljs-keyword">with</span> amp.scale_loss(loss, optimizer) <span class="hljs-keyword">as</span> scaled_loss:<br>    scaled_loss.backward()<br>optimizer.step()<br><br></code></pre></td></tr></table></figure>
<p>此外，如果我们希望使用APEX在训练过程中执行resume的话，我们还需要对代码做如下的添加</p>
<p>Note that we recommend restoring the model using the same <code>opt_level</code>. Also note that we recommend calling the <code>load_state_dict</code> methods after <code>amp.initialize</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Save checkpoint</span><br>checkpoint = &#123;<br>    <span class="hljs-string">&#x27;model&#x27;</span>: model.state_dict(),<br>    <span class="hljs-string">&#x27;optimizer&#x27;</span>: optimizer.state_dict(),<br>    <span class="hljs-string">&#x27;amp&#x27;</span>: amp.state_dict()<br>&#125;<br>torch.save(checkpoint, <span class="hljs-string">&#x27;amp_checkpoint.pt&#x27;</span>)<br>...<br><br><span class="hljs-comment"># Restore</span><br>model = ...<br>optimizer = ...<br>checkpoint = torch.load(<span class="hljs-string">&#x27;amp_checkpoint.pt&#x27;</span>)<br><br>model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)<br>model.load_state_dict(checkpoint[<span class="hljs-string">&#x27;model&#x27;</span>])<br>optimizer.load_state_dict(checkpoint[<span class="hljs-string">&#x27;optimizer&#x27;</span>])<br>amp.load_state_dict(checkpoint[<span class="hljs-string">&#x27;amp&#x27;</span>])<br><br><span class="hljs-comment"># Continue training</span><br>...<br></code></pre></td></tr></table></figure>
<p>安装过程中遇到了很多的问题：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/apex/issues/1043">Build error “fatal error: ATen/cuda/CUDAGraphsUtils.cuh: No such file or directory” · Issue #1043 · NVIDIA/apex (github.com)</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># rollback apex to the previous commit</span><br>git reset --hard 3fe10b5597ba14a748ebb271a6ab97c09c5701ac<br></code></pre></td></tr></table></figure>
<p> cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install -U cpython<br><span class="hljs-comment"># this method is not useful</span><br></code></pre></td></tr></table></figure>
<p>command ‘gcc’ failed with exit status 1</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git checkout f3a960f80244cf9e80558ab30f7f7e8cbf03c0a0<br></code></pre></td></tr></table></figure>
<h2 id="限制网络的输出范围"><a href="#限制网络的输出范围" class="headerlink" title="限制网络的输出范围"></a>限制网络的输出范围</h2><p>实际上，这一部分的应用就属于激活函数的数学理念问题了，我们倘若需要将网络的<strong>输出限制在一定的范围</strong>内，除了<strong>自己编写相关的数据处理</strong>手段之外，<strong>激活函数</strong>实际上有一部分原因就是为了这点设置的。</p>
<ol>
<li>神经网络基于对非线性运算的需要，引入了激活函数，强化了网络的学习能力；</li>
<li>同时神经网络<strong>对于输出</strong>有所要求（很多时候是以一种概率表达的方式输出的）所以就会需要softmax（0，1同时<code>sum==1</code>）之类的函数，<strong>可以将分类器的原始输出映射为概率。</strong> Sigmoid tanh之类的将输出限制在（0，1），但是并没有对加和有要求，这里可以做一个区分<a target="_blank" rel="noopener" href="https://www.cnblogs.com/jins-note/p/12528412.html区分sigmoid（多分类）和Softmax（单分类）">https://www.cnblogs.com/jins-note/p/12528412.html区分sigmoid（多分类）和Softmax（单分类）</a></li>
<li>Softmax和tanh可能会出现梯度消失的问题，ReLU将输出限制在（0，1）<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/73214810">一部分激活函数的特点</a></li>
</ol>
<p>所以很显然，我们可以通过对于相应的激活函数的应用，来限制我们的网络输出范围。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Training Strategy</p><p><a href="http://aikenh.cn/cn/NerualNetworkTraining/">http://aikenh.cn/cn/NerualNetworkTraining/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>AikenH</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2021-12-16</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-10-30</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/Machine-Learning/">Machine Learning, </a><a class="link-muted" rel="tag" href="/tags/Pytorch/">Pytorch, </a><a class="link-muted" rel="tag" href="/tags/Acceleration/">Acceleration </a></div></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/img/alipay.jpg" alt="Alipay"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/img/wechat.jpg" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/cn/Loss-SmoothSharpen/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Loss-Smooth(Sharpen)</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/cn/PyTorch/"><span class="level-item">PyTorch Handbook 00 （Archive）</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://aikenh.cn/cn/NerualNetworkTraining/';
            this.page.identifier = 'cn/NerualNetworkTraining/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'my-tech-blog-3' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/title.jpg" alt="AikenH"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">AikenH</p><p class="is-size-6 is-block">Future Full-Stack Developer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>ShenZhen</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">154</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">46</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">103</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/AikenH" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="ZhiHu" href="https://www.zhihu.com/people/Aiken-h"><i class="fab fa-zhihu"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="https://weibo.com/u/1788200627"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Steam" href="https://steamcommunity.com/id/AikenH/"><i class="fab fa-steam"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/AikenH"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#优化器"><span class="level-left"><span class="level-item">1</span><span class="level-item">优化器</span></span></a></li><li><a class="level is-mobile" href="#学习率"><span class="level-left"><span class="level-item">2</span><span class="level-item">学习率</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#学习率的基本设置"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">学习率的基本设置</span></span></a></li><li><a class="level is-mobile" href="#学习率变化方法"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">学习率变化方法</span></span></a></li><li><a class="level is-mobile" href="#分析学习率的大小"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">分析学习率的大小</span></span></a></li></ul></li><li><a class="level is-mobile" href="#过拟合欠拟合现象"><span class="level-left"><span class="level-item">3</span><span class="level-item">过拟合欠拟合现象</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#过拟合问题定义和分析"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">过拟合问题定义和分析</span></span></a></li><li><a class="level is-mobile" href="#收敛过快泛化能力差"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">收敛过快泛化能力差</span></span></a></li><li><a class="level is-mobile" href="#产生的原因分析"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">产生的原因分析</span></span></a></li><li><a class="level is-mobile" href="#数据的复杂度分析"><span class="level-left"><span class="level-item">3.4</span><span class="level-item">数据的复杂度分析:</span></span></a></li><li><a class="level is-mobile" href="#常见的解决方式"><span class="level-left"><span class="level-item">3.5</span><span class="level-item">常见的解决方式</span></span></a></li><li><a class="level is-mobile" href="#图像增强"><span class="level-left"><span class="level-item">3.6</span><span class="level-item">图像增强</span></span></a></li></ul></li><li><a class="level is-mobile" href="#早停法"><span class="level-left"><span class="level-item">4</span><span class="level-item">早停法</span></span></a></li><li><a class="level is-mobile" href="#效率优化"><span class="level-left"><span class="level-item">5</span><span class="level-item">效率优化</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#rapidAI"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">rapidAI</span></span></a></li><li><a class="level is-mobile" href="#torch-Cuda-AMP"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">torch.Cuda.AMP</span></span></a></li><li><a class="level is-mobile" href="#APEX-显存优化"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">APEX_显存优化</span></span></a></li></ul></li><li><a class="level is-mobile" href="#限制网络的输出范围"><span class="level-left"><span class="level-item">6</span><span class="level-item">限制网络的输出范围</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><figure class="media-left"><a class="image" href="/cn/LearnWeb16-Web%E5%AE%9E%E6%88%9801-%E9%A6%96%E9%A1%B5%E8%AE%BE%E8%AE%A1/"><img src="/img/header_img/lml_bg17.jpg" alt="LearnWeb15-Web实战01-首页设计"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-02-17T00:28:28.000Z">2024-02-17</time></p><p class="title"><a href="/cn/LearnWeb16-Web%E5%AE%9E%E6%88%9801-%E9%A6%96%E9%A1%B5%E8%AE%BE%E8%AE%A1/">LearnWeb15-Web实战01-首页设计</a></p><p class="categories"><a href="/categories/Web/">Web</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/cn/LearnWeb15-CSS09-%E5%AA%92%E4%BD%93%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%93%8D%E5%BA%94%E5%BC%8F%E8%AE%BE%E8%AE%A1/"><img src="/img/header_img/lml_bg16.jpg" alt="LearnWeb15-CSS09-媒体查询与响应式设计"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-02-08T09:53:55.000Z">2024-02-08</time></p><p class="title"><a href="/cn/LearnWeb15-CSS09-%E5%AA%92%E4%BD%93%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%93%8D%E5%BA%94%E5%BC%8F%E8%AE%BE%E8%AE%A1/">LearnWeb15-CSS09-媒体查询与响应式设计</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/CSS/">CSS</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/cn/LearnWeb14-CSS08-CSS%E5%B8%83%E5%B1%80/"><img src="/img/header_img/lml_bg15.jpg" alt="LearnWeb14-CSS08-CSS布局"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-02-07T09:09:44.000Z">2024-02-07</time></p><p class="title"><a href="/cn/LearnWeb14-CSS08-CSS%E5%B8%83%E5%B1%80/">LearnWeb14-CSS08-CSS布局</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/CSS/">CSS</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/cn/LearnWeb13-CSS07-CSS%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B/"><img src="/img/header_img/lml_bg14.jpg" alt="LearnWeb13-CSS07-CSS开发流程"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-02-07T07:54:33.000Z">2024-02-07</time></p><p class="title"><a href="/cn/LearnWeb13-CSS07-CSS%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B/">LearnWeb13-CSS07-CSS开发流程</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/CSS/">CSS</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/cn/LearnWeb12-CSS06-%E8%A1%A8%E6%A0%BC%E6%A0%B7%E5%BC%8F%E5%A4%84%E7%90%86/"><img src="/img/header_img/lml_bg13.jpg" alt="LearnWeb12-CSS06-表格样式处理"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-02-07T07:06:05.000Z">2024-02-07</time></p><p class="title"><a href="/cn/LearnWeb12-CSS06-%E8%A1%A8%E6%A0%BC%E6%A0%B7%E5%BC%8F%E5%A4%84%E7%90%86/">LearnWeb12-CSS06-表格样式处理</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/CSS/">CSS</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Aiken&#039;s Blog</a><p class="is-size-7"><span>&copy; 2024 AikenH</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_pv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span> and <span id="busuanzi_container2_site_uv"><span id="busuanzi_value_site_pv">0</span>&nbsp;visits</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/night.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>