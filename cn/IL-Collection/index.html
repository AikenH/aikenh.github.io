<!doctype html>
<html lang="cn"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>IL Collection - AikenH Blogs</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Aiken Hong"><meta name="msapplication-TileImage" content="/img/pokemon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Aiken Hong"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="@AikenHong 2022 [[Draft&amp;#x2F;IL 总结]]: Thx 2 wyz to provide some clus for learnning Incremental Learning. In this Doc, we may add some related knowledge distill works which is used to design our Incremental"><meta property="og:type" content="blog"><meta property="og:title" content="IL Collection"><meta property="og:url" content="http://aikenh.cn/cn/IL-Collection/"><meta property="og:site_name" content="AikenH Blogs"><meta property="og:description" content="@AikenHong 2022 [[Draft&amp;#x2F;IL 总结]]: Thx 2 wyz to provide some clus for learnning Incremental Learning. In this Doc, we may add some related knowledge distill works which is used to design our Incremental"><meta property="og:locale" content="cn"><meta property="og:image" content="http://aikenh.cn/img/header_img/lml_bg9.jpg"><meta property="article:published_time" content="2022-01-03T17:38:04.000Z"><meta property="article:modified_time" content="2023-10-31T00:08:31.637Z"><meta property="article:author" content="AikenH"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Survey"><meta property="article:tag" content="Incremental Learning"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://aikenh.cn/img/header_img/lml_bg9.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://aikenh.cn/cn/IL-Collection/"},"headline":"IL Collection","image":["http://aikenh.cn/img/header_img/lml_bg9.jpg"],"datePublished":"2022-01-03T17:38:04.000Z","dateModified":"2023-10-31T00:08:31.637Z","author":{"@type":"Person","name":"AikenH"},"publisher":{"@type":"Organization","name":"AikenH Blogs","logo":{"@type":"ImageObject","url":{"text":"Aiken's Blog"}}},"description":"@AikenHong 2022 [[Draft&#x2F;IL 总结]]: Thx 2 wyz to provide some clus for learnning Incremental Learning. In this Doc, we may add some related knowledge distill works which is used to design our Incremental"}</script><link rel="canonical" href="http://aikenh.cn/cn/IL-Collection/"><link rel="icon" href="/img/pokemon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-dark.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css" title="default"><link rel="alternate stylesheet" href="/css/cyberpunk.css" title="cyberpunk"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="/atom.xml" title="AikenH Blogs" type="application/atom+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Aiken&#039;s Blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/AikenH"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-lightbulb" id="night-icon"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/img/header_img/lml_bg9.jpg" alt="IL Collection"></span></div><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>IL Collection</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2022-01-03T17:38:04.000Z" title="2022-01-03T17:38:04.000Z">2022-01-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-10-31T00:08:31.637Z" title="2023/10/31 08:08:31">2023-10-31</time></span><span class="level-item"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></span><span class="level-item">29 minutes read (About 4422 words)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><div class="content"><p>@AikenHong 2022</p>
<p>[[Draft/IL 总结]]: Thx 2 wyz to provide some clus for learnning Incremental Learning.</p>
<p>In this Doc, we may add some related knowledge distill works which is used to design our Incremental Structure.<br>在这个文档中，我们可能还会添加一些知识蒸馏的相关工作的文献，这些实际上对于我的增量学习架构有一个比较大的启发</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_36474809/article/details/116176371">DER</a></li>
<li>SPPR 没有 get 到方法到底是怎么做的</li>
</ul>
<h2 id="Introduction-👿"><a href="#Introduction-👿" class="headerlink" title="Introduction 👿"></a>Introduction 👿</h2><p>在很多视觉应用中，需要在保留旧知识的基础上学习新知识，==举个例子==，理想的情况是，我们可以保留之前学习的参数，而不发生==灾难性遗忘==，或者我们基于之前的数据进行协同训练，灾难性遗忘是 IL 中最核心的问题。</p>
<p>Incremental 的基本过程可以表示如下<sub>[4]</sub>：<br><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/20220106101003.png" alt="dsa"></p>
<p>我们将模型可以划分为以下的两个部分<sub>[1]</sub>：backbone 和 classifier<br><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20220105213925.png" alt="split"></p>
<p>从 LWF 中我们可以知道经典的 Paradigm，主要有下面的三种来对$\theta _S$ 和$\theta_o$来进行更新：</p>
<ul>
<li>仅重新训练分类器：仅更新$\theta_o$</li>
<li>微调特征提取器，重新训练分类器</li>
<li>联合训练</li>
</ul>
<p><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20220106111235.png" alt=""></p>
<h2 id="基于蒸馏架构的方法"><a href="#基于蒸馏架构的方法" class="headerlink" title="基于蒸馏架构的方法"></a>基于蒸馏架构的方法</h2><span id="more"></span>
<p>这一系列的方法实际上是 IL 最经典的发展路线，实际上从初始的蒸馏架构开始，最后逐渐的发展到结合回放的策略中，我认为结合<strong>rehearsal</strong>才是该类方法最后的归宿，所以我将基于蒸馏正则化的 Pod 和 LWF 也放到了这一部分。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>《Learning without Forgetting》LWF 主要带来的就是将 KD 损失引入 Joint Training 的范式，也就是我们印象中最原始的增量学习的途径，利用 <code>expand_dim</code> 训练最后输出的新的节点，但是这个范式是不需要旧数据。</p>
<script type="math/tex; mode=display">
Loss = L_{CE} + L_{KD}</script><p>这里的蒸馏使用的是最终的 pred 输出，后续对于蒸馏损失的有 KDC 的变体，根据新旧样本的比例来赋予权重，考虑模型优化的权重。</p>
<script type="math/tex; mode=display">
Loss = \lambda L_{CE} + (1-\lambda) L_{KD}</script><p>其中 $\lambda^2 = \frac{|C<em>{old}|}{|C</em>{old}|+ |C_{new}|}$</p>
<p>这就是最经典的 Incremental Learning 的范式，我们首先继承一部分分类器的参数，然后通过这个损失对整个框架进行协同训练。</p>
<h3 id="引入旧样例"><a href="#引入旧样例" class="headerlink" title="引入旧样例"></a>引入旧样例</h3><p>《ICaRL: Incremental Classifier and Representation Learning》在 LWF 基础上引入部分旧数据来避免灾难遗忘的问题</p>
<ul>
<li>基于特征提取器对新旧数据的训练集提取==平均特征向量==（Kmeans + KNN）</li>
<li>基于最近邻均值分类算法 NME 计算出新旧数据的预测值 计算 LWF 的经典损失，优化模型</li>
</ul>
<blockquote>
<p>本文的亮点主要在于引入了旧的数据进行复习（有一个比较好的数据选取策略），以及最后使用的不是全连接层而是最近邻分类器来作为预测。（Will This Get Better？）</p>
</blockquote>
<p>后续在 ==《End-to-End Incremental Learning》== 中，将最近邻分类器替换成分类层，其动机就是对 ICaRL 进行优化。</p>
<p><strong>memory 保存旧样本 -&gt; CE+KD -&gt; reBalance + Fine-tune</strong></p>
<p><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20220106112111.png" alt=""></p>
<p>由于数据量上的有偏，导致分类器会严重有偏与 New-Classes，但是我认为这个可能是对于场景设定的不同，加入新的需求和发现少量的数据两者是一个比较大的不同。</p>
<p>这里的 <strong>样本选择策略</strong> 可能在后续会比较有用。</p>
<ul>
<li>在 ICaRL 中选择的是最确信的样本来 rehearsal，也就是使用特征中心的 KNN 方法来选取样本。</li>
<li>而在==《Rainbow Memory》==<sub>[8]</sub>中则是选择最难的样本，其 motivation 是选择最接近判别界样本</li>
</ul>
<p>RM 的最终实现的思路是通过 Data Augmentation 对样本进行变化，将不同 Augmentaion 后的预测的偏差（不确定程度）来衡量一个样本是 Hard or Simple Task，基于这种方式来选择 Hard-Task（Uncertainty）</p>
<p>具体而言，标签为 c 的样本，经过 perturbed 后，被网络预测为 c 类的次数越多，则不确定性越弱。</p>
<h3 id="优化分类器"><a href="#优化分类器" class="headerlink" title="优化分类器"></a>优化分类器</h3><p>由于新类的大量数据带来的偏差，==《Large Sacale Incremental Learning》== 试图解决这个问题</p>
<ul>
<li>将训练集划分一个 rebalance 的 dataset 作为验证集，并用该数据集训练一个 Bias Correction Layer 得到修正的参数，</li>
</ul>
<p>该层的输出如下，实际上就是一个线性回归层，只有两个参数</p>
<script type="math/tex; mode=display">
q_{k}=\left\{\begin{array}{lr}
o_{k} & 1 \leq k \leq n \\
\alpha o_{k}+\beta & n+1 \leq k \leq n+m
\end{array}\right.</script><p>训练该层的时候固定 CLF 和 BB，使用 CE 损失即可，但是模型在大数据集上的表现更佳，在 cifar100 的小数据集上表现一般。</p>
<p>另外还有借助 Long-Tailed 中的策略，从$||W||$的角度矫正偏差的文章 ==《Learning a Unified Classifier Incrementally via Rebalancing》==</p>
<p>动机是由于：1）imbalance：new classes 的权重的大小远远高于 old classes 的权重。2）特征与 old classes 的权重关系没有保留。3）一些 new classes 的权重与 old classes 的权重相近（容易混淆的类别），导致歧义性。</p>
<p>引入了 Cosine Normalization 分类器，实际上就是进分类器之前进行正则化，加入 Margin 损失（可以参考人脸比对的 Cosine Face 之类的）最终损失为：</p>
<script type="math/tex; mode=display">
L=\frac{1}{|\mathcal{N}|} \sum_{x \in \mathcal{N}}\left(L_{\mathrm{ce}}(x)+\lambda L_{\mathrm{dis}}^{\mathrm{G}}(x)\right)+\frac{1}{\left|\mathcal{N}_{\mathrm{o}}\right|} \sum_{x \in \mathcal{N}_{\mathrm{o}}} L_{\mathrm{mr}}(x)</script><p>更简单的有==《Maintaining discrimination and fairness in class incremental learning》==，通过对于分类器中的新旧模型的<strong>weight </strong>做 Rescale 使其再 W 上达成一致来维持一个较好的效果</p>
<script type="math/tex; mode=display">
\begin{gathered}
W = (W_{old},W_{new}) ; Norm_{old} = (||W_1||, ···, ||W_{c^b_{old}}) \\
\gamma = \frac{Mean(Norm_{old})}{Mean(Norm_{new}} \\
\hat{W}_{new} = \gamma · W_{new} \\
\end{gathered}</script><h3 id="优化特征提取器"><a href="#优化特征提取器" class="headerlink" title="优化特征提取器"></a>优化特征提取器</h3><p>其实Incremental现阶段的任务也倾向于使用两阶段的架构，基于这样的架构，我们首先提名最重要的就是基于SCL的这篇文章<sub>[9]</sub>,这篇文章主要的思路是：</p>
<center>SCL+projector（Train）+NCM（Test）</center>

<p>训练的Batch就是普通的Memory+New，但是值得一提的是，这篇文章对Memory的数据选取做了消融实验，得到了这样的结果：</p>
<p><strong>随机选取</strong>Memory的效果&gt;GSS（NIPS2019）和ASER（AAAI2021），是一个令人惊讶的结果.</p>
<p>而类似的，也有使用图像旋转的SSL（缓解ce带来的特征bias）+CE结合Prototype（rehearsal避免遗忘）+ KDLoss的研究<sub>[14]</sub>，证明了结合类似的自监督任务能够有效缓解特征之间的重叠。</p>
<p>使用的SSL任务是常见的Rotate Loss, KD是和上一轮的模型做约束。</p>
<script type="math/tex; mode=display">
L_{t,total} = L_{t,ce} + \lambda L_{t,protoAug} + \gamma L_{t,kd}</script><p><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20220107221329.png" alt="Feature Compare"></p>
<p>同样的C<sup>2</sup>OL<sub>[15]</sub> 这篇方法，就用最基本的对比学习的损失来研究该方法对于IL的实用性，也发现了基于CL学出来的特征确实更适合用在蒸馏的任务之上，验证了我们的猜想。</p>
<h3 id="优化损失设计"><a href="#优化损失设计" class="headerlink" title="优化损失设计"></a>优化损失设计</h3><p>《PODNet Pooled Outputs Distillation for Small-Tasks Incremental Learning》基于样本回放的方法，改进 KD，定义了 Pooled Output Distillation。</p>
<ul>
<li>spatial-based distillation-loss，基于空间的蒸馏损失，改进蒸馏方法</li>
<li>representation comprising multiplt proxy vectors，代理向量改进了分类器</li>
</ul>
<p>==part1 Update KD Loss==</p>
<center> Pooling 简略图</center>

<p><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20220106095511.png" alt="pooling"></p>
<p>假设：$\hat{y} = g(f(x))$ 为分类过程，其中$f(x)$ 代表特征提取过程。<br>POD 算法则为，不仅将蒸馏应用到特征提取的最终输出，还将其用于$f(x)$的中间过程的输出</p>
<script type="math/tex; mode=display">
f^t(x) = f^t_L .. ·f^t_l .. ·f^t_1(x)</script><p>中的每一层（如下式）都作为中间的结果，用来做 KD，上标 t 表示 task，下标则表示模型第几层。</p>
<script type="math/tex; mode=display">
h^t_{l,c,w,h} = f^t_l(·)</script><p>对该输出的各层执行各种级别的 POD 蒸馏，作为我们的监督来实现对灾难性遗忘的避免：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\text {POD-pixel }}\left(\mathbf{h}_{\ell}^{t-1}, \mathbf{h}_{\ell}^{t}\right)=\sum_{c=1}^{C} \sum_{w=1}^{W} \sum_{h=1}^{H}\left\|\mathbf{h}_{\ell, c, w, h}^{t-1}-\mathbf{h}_{\ell, c, w, h}^{t}\right\|^{2}</script><p>显然 pixel 级别对于模型的约束是最强的</p>
<script type="math/tex; mode=display">
\begin{gathered}
\mathcal{L}_{\text {POD-channel }}\left(\mathbf{h}_{\ell}^{t-1}, \mathbf{h}_{\ell}^{t}\right)=\sum_{w=1}^{W} \sum_{h=1}^{H}\left\|\sum_{c=1}^{C} \mathbf{h}_{\ell, c, w, h}^{t-1}-\sum_{c=1}^{C} \mathbf{h}_{\ell, c, w, h}^{t}\right\|^{2} \\
\mathcal{L}_{\text {POD-gap }}\left(\mathbf{h}_{\ell}^{t-1}, \mathbf{h}_{\ell}^{t}\right)=\sum_{c=1}^{C}\left\|\sum_{w=1}^{W} \sum_{h=1}^{H} \mathbf{h}_{\ell, c, w, h}^{t-1}-\sum_{w=1}^{W} \sum_{h=1}^{H} \mathbf{h}_{\ell, c, w, h}^{t}\right\|^{2} \\
\mathcal{L}_{\text {POD-width }}\left(\mathbf{h}_{\ell}^{t-1}, \mathbf{h}_{\ell}^{t}\right)=\sum_{c=1}^{C} \sum_{h=1}^{H}\left\|\sum_{w=1}^{W} \mathbf{h}_{\ell, c, w, h}^{t-1}-\sum_{w=1}^{W} \mathbf{h}_{\ell, c, w, h}^{t}\right\|^{2}
\end{gathered}</script><p>pixel 级别的蒸馏对于模型限制比较严格，其他级别的对于模型限制相对较松，需要一个权衡，作者最终选用的是 Spatial 级别的蒸馏，相当于 width 和 height 层面蒸馏 loss 之和</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\text {POD-spatial }}\left(\mathbf{h}_{\ell}^{t-1}, \mathbf{h}_{\ell}^{t}\right)=\mathcal{L}_{\text {POD-width }}\left(\mathbf{h}_{\ell}^{t-1}, \mathbf{h}_{\ell}^{t}\right)+\mathcal{L}_{\text {POD-height }}\left(\mathbf{h}_{\ell}^{t-1}, \mathbf{h}_{\ell}^{t}\right)</script><p>特征提取模型最终的特征则使用 pixel 级别的蒸馏：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{POD-flat}(h^{t-1},h^t) = ||h^{t-1} - h^t||^2</script><p>将这些蒸馏损失整合起来取代原本的 KD-Loss，再加上我们的 CE 即可：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\mathcal{L}_{POD-final} = \frac{\lambda_c}{L-1}\sum_{l=1}^{L-1} \mathcal{L}_{POD-spatial}(f_l^{t-1}(x),f_l^t(x)) +  \\
\lambda_f \mathcal{L}_{POD-flat}(f_l^{t-1}(x),f_l^t(x))
\end{gathered}</script><p>==Part2 Local Similarity Classifier==</p>
<p>第一个改进点就是将 Loss 修正为 Cosine 的形式<sub>UCiR</sub>，实际上就是使用的归一化后的 FC 层，但是如果只使用一个 Cos 相似度，好像多样化的需求无法满足，需要类似一个多头的机制</p>
<blockquote>
<p>和 LT 的地方一样，IL 近年来的主要架构也是两部分进行分离的，所以我们可以考虑从我们的角度来实现类似 POD-Loss 的架构维持，也就是一定程度上为我们的 SSL-SCL 架构的可行性提供了一定的信心。</p>
</blockquote>
<p>该方法迄今为止还是很多增量任务的榜单前几，该方法的蒸馏性能也被验证为有效，但是实际上将该方法用于模型中需要增加大量的特征输出模块，整体架构上修改起来可能会较为复杂。</p>
<h2 id="基于模型结构的方法"><a href="#基于模型结构的方法" class="headerlink" title="基于模型结构的方法"></a>基于模型结构的方法</h2><p>这一部分不是我研究的重点，可以看到有一部分设计的拓张模型或者，堆叠模型，用额外的结构来承载对应的新类的研究，可能考虑到一部分参数公用然后实行协同判断的策略把。</p>
<p>或者是其他的图模型，拓扑结构（神经气体网络）等等的方法，拓扑结构等方法可能户籍是未来的一个方向。</p>
<h3 id="特征网络堆叠"><a href="#特征网络堆叠" class="headerlink" title="特征网络堆叠"></a>特征网络堆叠</h3><ul>
<li>DER 特征网络堆叠的方法</li>
</ul>
<h2 id="其他方法"><a href="#其他方法" class="headerlink" title="其他方法"></a>其他方法</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.pnas.org/content/pnas/114/13/3521.full.pdf">EWC</a> ：这类方法一般是对网络中每个参数的重要性进行评估，根据每个参数的重要性，调整梯度信息更新参数。<br>-</li>
</ul>
<h2 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h2><p>这里会收集一部分 IL 中存在的一些现象或者问题</p>
<h3 id="新类优于旧类"><a href="#新类优于旧类" class="headerlink" title="新类优于旧类"></a>新类优于旧类</h3><p>模型倾向于时间上接近的模型有更高的敏感度，这可能是训练的过程决定的，也可能是由于再新类的训练上新类的权重要明显高于旧类，导致的某种数据不均衡的现象。</p>
<p>此外在传统的设定中，新类的数据量会大大的大于旧类</p>
<h3 id="Few-Shot-Incremental"><a href="#Few-Shot-Incremental" class="headerlink" title="Few-Shot Incremental"></a>Few-Shot Incremental</h3><p>Few-Shot 的增量情景更贴切于我的场景假设，在这种假设的背景之下，增量学习也会面临一些新的困难，这个篇章中我们可能会简要的总结一些方法对抗小样本和灾难性遗忘的思路和策略。</p>
<ul>
<li>小样本的类别原型不稳定</li>
<li>容易和旧类别混淆</li>
</ul>
<p>在进行总结的同时，我们的调研方向也会有所侧重，比如基于拓扑的神经气体网络方法，我们可能暂时不那么关心（精力有限）</p>
<p><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20220107182311.png" alt=""></p>
<h4 id="拓扑结构方法"><a href="#拓扑结构方法" class="headerlink" title="拓扑结构方法"></a>拓扑结构方法</h4><ul>
<li>《Few-Shot Class-Incremental Learning》<sub>[10]</sub></li>
</ul>
<h4 id="SPPR"><a href="#SPPR" class="headerlink" title="SPPR"></a>SPPR</h4><p>《Self-Promoted Prototype Refinement for Few-Shot Class-Incremental Learning》这篇文章的主要贡献有以下的两点：</p>
<ul>
<li>提出了 RESS（随机 episode 选择策略）通过强制特征自适应于各种随机模拟的增量过程来增强特征表示的可扩展性。</li>
<li>引入了一种自提升的原型细化机制(SPPR)，利用新类样本和旧类 prototype 表示之间的关系矩阵来更新现有的 prototypical</li>
</ul>
<blockquote>
<p>RESS 实际上应该是类比 Meta Learning 提出的一种训练策略<br>SPPR 是本文的核心，为了保持旧类之间的依赖和新类置假你都区分度，要对新类的原型进行提炼<br>理论上讲 SPPR 更新的应该是模型的参数，但是在代码中我暂时没有找到对应的实现的地方</p>
</blockquote>
<p>so we drop this method which is not match our structure</p>
<h4 id="Evolved-classifier"><a href="#Evolved-classifier" class="headerlink" title="Evolved classifier"></a>Evolved classifier</h4><p>由于数据量少的这个特点，我们解耦 BB 和 CLF，每次增量任务只更新分类器。</p>
<p>该文章[12]在多个数据集上实现了 SOTA，提出了 CEC（Continually Evolved Classifier），将图模型用在分类器上，它的分类器是一个无参数的 class mean classifier（听起来像 NCM）；</p>
<p><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20220107175617.png" alt=""></p>
<p>实际上就是在一个较优的特征空间的基础上调整我们的决策边界的一个策略，该方法引入了图注意力模型（GAT），该方法有一个特性是：</p>
<ul>
<li>增加节点而不改变其他的节点，</li>
<li>利用拓扑关系，链接关系的不变性，利于保留旧知识</li>
</ul>
<script type="math/tex; mode=display">
w_{new} = w_{old} + (\sum^{w_n}_k=1a_{jk}U_{w_k})</script><p>使用 GAT 获得线性变换矩阵和注意力系数来更新模型的权重。</p>
<p>此外提供了一种旋转增强的新型策略，效果特别好==pseudo incremental learning==，可能和 GAT 的一些特性有关，结合 GAT 效果提升巨大，要警惕这种方法是通用的还是特异性的。最好是看看有没有原理分析。</p>
<h4 id="CEBN"><a href="#CEBN" class="headerlink" title="CEBN"></a>CEBN</h4><p>采用三阶段的方式来实现小样本的增量学习，根据上述的任务划分图来确定不同的实验阶段：</p>
<ol>
<li>用大量数据训练基准的分类模型，使用的就是 base class</li>
<li>学习 novel class 防止灾难性遗忘，只使用新类数据（修正 CE 考虑小样本问题）（使用参数来正则防止 BB 灾难遗忘）</li>
<li>混合数据进行训练，这个时候使用一个 balance 的数据集，比如说做多次增量的话，在最后一次使用 balance replay 即可。</li>
</ol>
<p>第二阶段的损失是这里的关键，基于 CEBN 修改 CE，为啥我看不出区别，我感觉实际上就是 CE，只是虽然只用新数据训练，但是分类器是完整的罢了，其实就是 CE：</p>
<script type="math/tex; mode=display">
CE_{BN}(x) = \sum_{C_N}y_iln(\frac{exp(o_i)}{\sum_{C_N} exp(o_j) + \sum_{C_B}(o_k)})</script><p>正则项则通过对前后的 Backbone 进行约束得到：</p>
<script type="math/tex; mode=display">
L_2^{WC} = \sum||\theta_1 - \theta_2||^2</script><p>最终整合起来的损失如下：</p>
<script type="math/tex; mode=display">
Loss = L_2^{WC} + \lambda CE_{BN}</script><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p> <a target="_blank" rel="noopener" href="https://github.com/xialeiliu/Awesome-Incremental-Learning">📚Awesom Incremental Learning Collections</a> | <a target="_blank" rel="noopener" href="https://paperswithcode.com/task/incremental-learning">🌤️Paper w Code Incremental Learning</a></p>
<ol>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.09282">Learning without Forgetting</a> | <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/51587674">ZHIHU</a> | ECCV2016</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.07725">iCaRL Incremental Classifier and Representation Learning</a> | <a target="_blank" rel="noopener" href="https://www.cnblogs.com/marsggbo/p/10321834.html">CnBlog</a> ， <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/51639634">ZhiHu</a> | CVPR2017</li>
<li><a href="">Ene-to-End Incremental Learning</a> | ECCV2018</li>
<li>⭐ <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.13513">PODNet Pooled Outputs Distillation for Small-Tasks Incremental Learning</a> | <a target="_blank" rel="noopener" href="https://github.com/arthurdouillard/incremental_learning.pytorch">ECCV2020</a> | <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_36474809/article/details/116140481">CSDN</a></li>
<li><a href="">Large Sacale Incremental Learning</a> | CVPR2019 | <a target="_blank" rel="noopener" href="https://blog.csdn.net/dhaiuda/article/details/102852694">CSDN</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/dhaiuda/article/details/102850853">Learning a Unified Classifier Incrementally via Rebalancing</a> | CVPR2019 | </li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.07053.pdf">Maintaining discrimination and fairness in class incremental learning</a> | CVPR2020</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.17230">Rainbow Memory: Continual Learning with a Memory of Diverse Samples</a> | <a target="_blank" rel="noopener" href="https://github.com/clovaai/rainbow-memory">CVPR2021</a> | <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_36474809/article/details/116140087">CSDN</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.13885">Supervised Contrastive Replay: Revisiting the Nearest Class Mean Classifier in Online Class-Incremental Continual Learning</a> | CVPR2021 | <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_36474809/article/details/116310575">CSDN</a></li>
<li><a href="">Few-Shot Class-Incremental Learning</a> | CVPR2020 | <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_36474809/article/details/116176530">CSDN</a></li>
<li><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhu_Self-Promoted_Prototype_Refinement_for_Few-Shot_Class-Incremental_Learning_CVPR_2021_paper.pdf">Self-Promoted Prototype Refinement for Few-Shot Class-Incremental Learning</a> | <a target="_blank" rel="noopener" href="https://github.com/zhukaii/SPPR">CVPR2021</a> | <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_40825479/article/details/122199901">CSDN</a></li>
<li><a href="">Few Shot Incremental Learning with Continually Evolved Classifiers</a> | <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_36474809/article/details/116612960">CVPR2021</a> |</li>
<li><a href="">Generalized and Incremental Few-Shot Learning by  Explicit Learning and Calibration without Forgetting</a> | ICCV2021 | <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_58666589/article/details/120682594">CSDN</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//openaccess.thecvf.com/content/CVPR2021/html/Zhu_Prototype_Augmentation_and_Self-Supervision_for_Incremental_Learning_CVPR_2021_paper.html">Prototype Augmentation and Self-Supervision for Incremental Learning</a> | <a href="https://link.zhihu.com/?target=https%3A//github.com/Impression2805/CVPR21_PASS">CVPR2021</a> | <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/416717749?utm_medium=social&amp;utm_oi=74269941825536">ZHIHU</a></li>
<li><a href="">Contrastive Continual Learning</a> | ICCV2022 | CSDN</li>
</ol>
<p><strong>一些总结笔记</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/337287727">Classic Incremental Papers</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/abcdefg90876/article/details/114109237">Background and Dilemma</a></li>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0925231221014995">Online Continual Learning An Empirical Survey</a> | 2021 | <a target="_blank" rel="noopener" href="https://ripe-heliotrope-6f4.notion.site/Online-Continual-Learning-in-Image-Classification-An-Empirical-Survey-25bbcd8d3c2b492aa983a4320d1150de#a57ead60cdaf4ac5b42b8dce849266b2">Notion</a> 这篇综述给人的感觉比较一般把，或者可能是总结文档里没有写出比较关键的一些看法和证据。感觉不是特别推荐阅读。</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_36474809/article/details/116720597">Incremental Learning in 20-21</a> | 下面的图也来自这篇文章</li>
</ul>
<p><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20220106201641.png" alt="Fig1"><br><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20220106201645.png" alt="Fig2"><br><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20220106201649.png" alt="Fig3"></p>
<h2 id="to-be-placed-in-the-right-place"><a href="#to-be-placed-in-the-right-place" class="headerlink" title="to be placed in the right place"></a>to be placed in the right place</h2><p>将一些新的研究先放在这里，到时候看看要组织到笔记的那一部分。</p>
<h3 id="class-Incremental-learning-via-Dual-Augmentation"><a href="#class-Incremental-learning-via-Dual-Augmentation" class="headerlink" title="class-Incremental learning via Dual Augmentation"></a>class-Incremental learning via Dual Augmentation</h3><p>该文章认为，类增量学习中灾难性遗忘可以被总结为两个方面带来的：特征表示上的偏差和分类器上的偏差。</p>
<ol>
<li>增量过程中如果不对特征提取器进行适应，则对新特征的提取能力不够；如果进行适应则会产生灾难性的遗忘</li>
<li>分类器如果不进行更新，会和新的特征表示不适应，而由于没有旧类的数据，就没有更新旧类的方向</li>
</ol>
<p>解决的思路是：</p>
<ul>
<li>训练的阶段做mixup来做混合类的学习，通过这种预先训练，来帮助模型得到一个较为稳定的表征。</li>
<li>分类上将历史数据的均值和方差记录下来，对模型更新的时候，通过分布信息生成语义特征维持决策边界，防止对旧类分成新类。</li>
</ul>
<p>第二部分具体细节的实现上还不是很清晰，后续可以看代码，但是目前来看不是我们需要的。</p>
<p><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20220315151105.jpg" alt="preview"></p>
<h3 id="Looking-back-on-learned-experiences-for-class-task-incremental-learning"><a href="#Looking-back-on-learned-experiences-for-class-task-incremental-learning" class="headerlink" title="Looking back on learned experiences for class/task incremental learning"></a>Looking back on learned experiences for class/task incremental learning</h3><p>主要贡献：无数据的增量学习，支持经验重放，不需要平行网络输出蒸馏监督。</p>
<p>kd使用的是最小欧拉距离：L2范数的平方作为损失。</p>
<h3 id="Overcoming-Catastrophic-Forgetting-in-Incremental-Few-Shot-Learning-by-Finding-Flat-Minima"><a href="#Overcoming-Catastrophic-Forgetting-in-Incremental-Few-Shot-Learning-by-Finding-Flat-Minima" class="headerlink" title="Overcoming Catastrophic Forgetting in Incremental Few-Shot Learning by Finding Flat Minima"></a>Overcoming Catastrophic Forgetting in Incremental Few-Shot Learning by Finding Flat Minima</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_40825479/article/details/122352675">CSDN</a></p>
<p>在基础模型训练阶段，企图找到一个损失的下降平坦点而不是简单的一个极小值，平坦极小值的模型的鲁棒性能会比普通的模型优异一些，具体对于平坦点的定义可以参见下面的这张图：</p>
<p><img src="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20220315161428" alt="img"></p>
<p>这种平坦点的研究，实际上和NotZeroLoss的设定具有相当的相似性，帮助模型学习到一个更加稳定的解，而该解在后续进行增量学习的过程中，会减少对应的灾难性遗忘的现象。</p>
<h3 id="Distilling-causal-effect-of-data-in-class-incremental-learning"><a href="#Distilling-causal-effect-of-data-in-class-incremental-learning" class="headerlink" title="Distilling causal effect of data in class-incremental learning"></a>Distilling causal effect of data in class-incremental learning</h3><p>也是通过因果分析来筹建分类结果，通过TDE的方式消除类别偏差，这一部分实际上和我们的Causal模块和统计均值模块应该是起到了相同的作用，这里暂时不深入进行解读。</p>
<p>==the two below== is important for our research:</p>
<h3 id="Do-not-Forget-to-Attend-to-Uncertainty-while-Mitigating-Catastrophic-Forgetting"><a href="#Do-not-Forget-to-Attend-to-Uncertainty-while-Mitigating-Catastrophic-Forgetting" class="headerlink" title="Do not Forget to Attend to Uncertainty while Mitigating Catastrophic Forgetting"></a>Do not Forget to Attend to Uncertainty while Mitigating Catastrophic Forgetting</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.01906.pdf">Papers</a> using attention and the bayes formula to calculate the Uncertainty or something else.</p>
<h3 id="Continual-Learning-in-the-Teacher-Student-Setup-Impact-of-Task-Similarity"><a href="#Continual-Learning-in-the-Teacher-Student-Setup-Impact-of-Task-Similarity" class="headerlink" title="Continual Learning in the Teacher-Student Setup: Impact of Task Similarity"></a>Continual Learning in the Teacher-Student Setup: Impact of Task Similarity</h3><p><a target="_blank" rel="noopener" href="https://www.ijcai.org/proceedings/2021/0137.pdf">Papers</a> do a lot for the loss, which we should pay attention for it.</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>IL Collection</p><p><a href="http://aikenh.cn/cn/IL-Collection/">http://aikenh.cn/cn/IL-Collection/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>AikenH</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2022-01-04</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-10-31</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/Machine-Learning/">Machine Learning, </a><a class="link-muted" rel="tag" href="/tags/Survey/">Survey, </a><a class="link-muted" rel="tag" href="/tags/Incremental-Learning/">Incremental Learning </a></div></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/img/alipay.jpg" alt="Alipay"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/img/wechat.jpg" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/cn/Transfer-Sync-Files/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Linux 文件传输和同步</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/cn/IL-WYZ/"><span class="level-item">WYZ-IL-Collection</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://aikenh.cn/cn/IL-Collection/';
            this.page.identifier = 'cn/IL-Collection/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'my-tech-blog-3' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/title.jpg" alt="AikenH"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">AikenH</p><p class="is-size-6 is-block">Future Full-Stack Developer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>ShenZhen</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">132</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">42</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">98</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/AikenH" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="ZhiHu" href="https://www.zhihu.com/people/Aiken-h"><i class="fab fa-zhihu"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="https://weibo.com/u/1788200627"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Steam" href="https://steamcommunity.com/id/AikenH/"><i class="fab fa-steam"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/AikenH"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Introduction-👿"><span class="level-left"><span class="level-item">1</span><span class="level-item">Introduction 👿</span></span></a></li><li><a class="level is-mobile" href="#基于蒸馏架构的方法"><span class="level-left"><span class="level-item">2</span><span class="level-item">基于蒸馏架构的方法</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Motivation"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">Motivation</span></span></a></li><li><a class="level is-mobile" href="#引入旧样例"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">引入旧样例</span></span></a></li><li><a class="level is-mobile" href="#优化分类器"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">优化分类器</span></span></a></li><li><a class="level is-mobile" href="#优化特征提取器"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">优化特征提取器</span></span></a></li><li><a class="level is-mobile" href="#优化损失设计"><span class="level-left"><span class="level-item">2.5</span><span class="level-item">优化损失设计</span></span></a></li></ul></li><li><a class="level is-mobile" href="#基于模型结构的方法"><span class="level-left"><span class="level-item">3</span><span class="level-item">基于模型结构的方法</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#特征网络堆叠"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">特征网络堆叠</span></span></a></li></ul></li><li><a class="level is-mobile" href="#其他方法"><span class="level-left"><span class="level-item">4</span><span class="level-item">其他方法</span></span></a></li><li><a class="level is-mobile" href="#其他问题"><span class="level-left"><span class="level-item">5</span><span class="level-item">其他问题</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#新类优于旧类"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">新类优于旧类</span></span></a></li><li><a class="level is-mobile" href="#Few-Shot-Incremental"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">Few-Shot Incremental</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#拓扑结构方法"><span class="level-left"><span class="level-item">5.2.1</span><span class="level-item">拓扑结构方法</span></span></a></li><li><a class="level is-mobile" href="#SPPR"><span class="level-left"><span class="level-item">5.2.2</span><span class="level-item">SPPR</span></span></a></li><li><a class="level is-mobile" href="#Evolved-classifier"><span class="level-left"><span class="level-item">5.2.3</span><span class="level-item">Evolved classifier</span></span></a></li><li><a class="level is-mobile" href="#CEBN"><span class="level-left"><span class="level-item">5.2.4</span><span class="level-item">CEBN</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#References"><span class="level-left"><span class="level-item">6</span><span class="level-item">References</span></span></a></li><li><a class="level is-mobile" href="#to-be-placed-in-the-right-place"><span class="level-left"><span class="level-item">7</span><span class="level-item">to be placed in the right place</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#class-Incremental-learning-via-Dual-Augmentation"><span class="level-left"><span class="level-item">7.1</span><span class="level-item">class-Incremental learning via Dual Augmentation</span></span></a></li><li><a class="level is-mobile" href="#Looking-back-on-learned-experiences-for-class-task-incremental-learning"><span class="level-left"><span class="level-item">7.2</span><span class="level-item">Looking back on learned experiences for class/task incremental learning</span></span></a></li><li><a class="level is-mobile" href="#Overcoming-Catastrophic-Forgetting-in-Incremental-Few-Shot-Learning-by-Finding-Flat-Minima"><span class="level-left"><span class="level-item">7.3</span><span class="level-item">Overcoming Catastrophic Forgetting in Incremental Few-Shot Learning by Finding Flat Minima</span></span></a></li><li><a class="level is-mobile" href="#Distilling-causal-effect-of-data-in-class-incremental-learning"><span class="level-left"><span class="level-item">7.4</span><span class="level-item">Distilling causal effect of data in class-incremental learning</span></span></a></li><li><a class="level is-mobile" href="#Do-not-Forget-to-Attend-to-Uncertainty-while-Mitigating-Catastrophic-Forgetting"><span class="level-left"><span class="level-item">7.5</span><span class="level-item">Do not Forget to Attend to Uncertainty while Mitigating Catastrophic Forgetting</span></span></a></li><li><a class="level is-mobile" href="#Continual-Learning-in-the-Teacher-Student-Setup-Impact-of-Task-Similarity"><span class="level-left"><span class="level-item">7.6</span><span class="level-item">Continual Learning in the Teacher-Student Setup: Impact of Task Similarity</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><figure class="media-left"><a class="image" href="/cn/deploy_server_byipv6_03_ddnsgo/"><img src="/img/header_img/lml_bg12.jpg" alt="使用Ipv6部署服务03 DDNS-go 动态域名解析"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-11-07T05:40:13.000Z">2023-11-07</time></p><p class="title"><a href="/cn/deploy_server_byipv6_03_ddnsgo/">使用Ipv6部署服务03 DDNS-go 动态域名解析</a></p><p class="categories"><a href="/categories/NAS/">NAS</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/cn/deploy_server_byipv6_02_nginx/"><img src="/img/header_img/lml_bg11.jpg" alt="使用Ipv6部署服务02 Nginx和Https"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-11-06T12:49:16.000Z">2023-11-06</time></p><p class="title"><a href="/cn/deploy_server_byipv6_02_nginx/">使用Ipv6部署服务02 Nginx和Https</a></p><p class="categories"><a href="/categories/NAS/">NAS</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/cn/Windows%E7%AB%AF%E5%8F%A3%E5%BC%82%E5%B8%B8%E5%8D%A0%E7%94%A8/"><img src="/img/header_img/lml_bg9.jpg" alt="Windows端口异常占用"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-11-05T15:46:54.000Z">2023-11-05</time></p><p class="title"><a href="/cn/Windows%E7%AB%AF%E5%8F%A3%E5%BC%82%E5%B8%B8%E5%8D%A0%E7%94%A8/">Windows端口异常占用</a></p><p class="categories"><a href="/categories/Windows/">Windows</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/cn/deploy_server_byipv6/"><img src="/img/header_img/lml_bg10.jpg" alt="使用Ipv6部署服务01 IPV6开启和设置"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-11-05T11:23:17.000Z">2023-11-05</time></p><p class="title"><a href="/cn/deploy_server_byipv6/">使用Ipv6部署服务01 IPV6开启和设置</a></p><p class="categories"><a href="/categories/NAS/">NAS</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/cn/Flashcard_template/"><img src="/img/header_img/lml_bg35.jpg" alt="Obsidian使用 Spaced Repetition 制作闪念卡片"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-10-27T16:39:59.000Z">2023-10-28</time></p><p class="title"><a href="/cn/Flashcard_template/">Obsidian使用 Spaced Repetition 制作闪念卡片</a></p><p class="categories"><a href="/categories/Editor/">Editor</a> / <a href="/categories/Editor/Obsidian/">Obsidian</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Aiken&#039;s Blog</a><p class="is-size-7"><span>&copy; 2023 AikenH</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_pv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span> and <span id="busuanzi_container2_site_uv"><span id="busuanzi_value_site_pv">0</span>&nbsp;visits</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/night.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>