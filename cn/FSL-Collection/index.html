<!doctype html>
<html lang="cn"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Survey for Few-Shot Learning - AikenH Blogs</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Aiken Hong"><meta name="msapplication-TileImage" content="/img/pokemon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Aiken Hong"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="@aikenhong 2020@h.aiken.970@gmail.com 另一个综述文章：https:&amp;#x2F;&amp;#x2F;zhuanlan.zhihu.com&amp;#x2F;p&amp;#x2F;61215293对该文中一些内容有一些补充，可以看看 FSL简介：https:&amp;#x2F;&amp;#x2F;blog.csdn.net&amp;#x2F;xhw205&amp;#x2F;article&amp;#x2F;details&amp;#x2F;79491649 GCN用于FSL：https:&amp;#x2F;&amp;#x2F;blog.csdn.net&amp;#x2F;qq_3602"><meta property="og:type" content="blog"><meta property="og:title" content="Survey for Few-Shot Learning"><meta property="og:url" content="http://aikenh.cn/cn/FSL-Collection/"><meta property="og:site_name" content="AikenH Blogs"><meta property="og:description" content="@aikenhong 2020@h.aiken.970@gmail.com 另一个综述文章：https:&amp;#x2F;&amp;#x2F;zhuanlan.zhihu.com&amp;#x2F;p&amp;#x2F;61215293对该文中一些内容有一些补充，可以看看 FSL简介：https:&amp;#x2F;&amp;#x2F;blog.csdn.net&amp;#x2F;xhw205&amp;#x2F;article&amp;#x2F;details&amp;#x2F;79491649 GCN用于FSL：https:&amp;#x2F;&amp;#x2F;blog.csdn.net&amp;#x2F;qq_3602"><meta property="og:locale" content="cn"><meta property="og:image" content="http://aikenh.cn/img/header_img/lml_bg28.jpg"><meta property="article:published_time" content="2021-11-29T05:12:05.000Z"><meta property="article:modified_time" content="2023-10-31T00:06:15.654Z"><meta property="article:author" content="AikenH"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Survey"><meta property="article:tag" content="FSL"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://aikenh.cn/img/header_img/lml_bg28.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://aikenh.cn/cn/FSL-Collection/"},"headline":"Survey for Few-Shot Learning","image":["http://aikenh.cn/img/header_img/lml_bg28.jpg"],"datePublished":"2021-11-29T05:12:05.000Z","dateModified":"2023-10-31T00:06:15.654Z","author":{"@type":"Person","name":"AikenH"},"publisher":{"@type":"Organization","name":"AikenH Blogs","logo":{"@type":"ImageObject","url":{"text":"Aiken's Blog"}}},"description":"@aikenhong 2020@h.aiken.970@gmail.com 另一个综述文章：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;61215293对该文中一些内容有一些补充，可以看看 FSL简介：https:&#x2F;&#x2F;blog.csdn.net&#x2F;xhw205&#x2F;article&#x2F;details&#x2F;79491649 GCN用于FSL：https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_3602"}</script><link rel="canonical" href="http://aikenh.cn/cn/FSL-Collection/"><link rel="icon" href="/img/pokemon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-dark.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css" title="default"><link rel="alternate stylesheet" href="/css/cyberpunk.css" title="cyberpunk"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="/atom.xml" title="AikenH Blogs" type="application/atom+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Aiken&#039;s Blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/AikenH"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-lightbulb" id="night-icon"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/img/header_img/lml_bg28.jpg" alt="Survey for Few-Shot Learning"></span></div><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>Survey for Few-Shot Learning</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2021-11-29T05:12:05.000Z" title="2021-11-29T05:12:05.000Z">2021-11-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-10-31T00:06:15.654Z" title="2023/10/31 08:06:15">2023-10-31</time></span><span class="level-item"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></span><span class="level-item">43 minutes read (About 6448 words)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><div class="content"><p>@aikenhong 2020<br>@h.aiken.970@gmail.com</p>
<p>另一个综述文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/61215293">https://zhuanlan.zhihu.com/p/61215293</a><br>对该文中一些内容有一些补充，可以看看</p>
<p>FSL简介：<a target="_blank" rel="noopener" href="https://blog.csdn.net/xhw205/article/details/79491649">https://blog.csdn.net/xhw205/article/details/79491649</a></p>
<p>GCN用于FSL：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36022260/article/details/93753532">https://blog.csdn.net/qq_36022260/article/details/93753532</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>FSL的根本目的就是弥合人工智能和人类之间的鸿沟，从少量带有监督信息的示例中学习。像人类一样有很高的泛化能力。这也能解决在实际应用场景中，数据难以收集或者大型数据难以建立的情景。</p>
<p>FSL的<strong>核心问题</strong>是：经验风险最小化器不可靠；那么如何<strong>使用先验知识</strong>去解决这个问题？</p>
<p>三个主要的角度：</p>
<ol>
<li>数据：使用先验知识增强数据的监督经验</li>
<li>模型：使用先验知识来降低假设空间</li>
<li>算法：使用先验知识来改变搜索最佳假设（来进行搜索？)</li>
</ol>
<p>现阶段针对FSL提出的一些相关的机器学习方法：<br><code>meta-learning;</code> <code>embedding learning;</code>  <code>generative modeling etc.</code></p>
<p><strong>本文的主要工作：</strong><br><span id="more"></span></p>
<ol>
<li>基于FSL的原有设定，在现阶段的FSL发展上给出正式定义，同时阐明具体目标以及解决方式</li>
<li>通过具体示例列举和FSL的相关学习问题，比较了相关性和差异性，更好的区分问题</li>
<li><p>指出核心问题：经验风险最小化器不可靠，这提供了更系统有组织的改进FSL的方向。<br>经验风险最小化器👉：基于ML中的错误分解来分析的</p>
</li>
<li><p>整理，更好的理解</p>
</li>
<li>未来方向</li>
</ol>
<h2 id="Notation-and-Terminology"><a href="#Notation-and-Terminology" class="headerlink" title="Notation and Terminology"></a>Notation and Terminology</h2><p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200519154522883.png" alt=" "><br>一般基于参数方法（因为非参数方法需要大量数据），在假设空间中搜索最优假设，并基于基于标签的Loss Function 来衡量效果。</p>
<h2 id="Main-Body"><a href="#Main-Body" class="headerlink" title="Main Body"></a>Main Body</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>2.1：具体定义&amp;示例 2.2：相关问题和FSL的相关性和差异 2.3：核心问题 2.4:现有的方法如何处理这个问题</p>
<h4 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h4><p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200519160715974.png" alt="image-20200519160715974"></p>
<p><strong>基本定义</strong>：FSL是一类机器学习（由E，T，P定义），其中E只包含有限数量的带有目标T监管信息的示例。</p>
<p><strong>研究方法：</strong>通常使用N-way K-shot的分类研究方法：从少量类别中的少量样本回归出ML模型，</p>
<p>​                                Training Set Contains：KN examples.</p>
<p><strong>Typical scenarios of FSL:</strong></p>
<p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200519165647756.png" alt="image-20200519165647756"></p>
<ul>
<li>Reducing data gathering effort and computational cost:<br>“raw images of other classes or pre-trained models ”<br>似乎有点迁移学习的味道了，改善从已有的类似数据集过来的model？</li>
<li>Learning for rare cases</li>
<li>Acting as a test bed for learning like human.</li>
</ul>
<p>和普通的ML的应用最明显的<strong>区别</strong>就是E中prior knowledge的应用，将T和先验知识结合起来。（such as Bayesian Learning [35,76]）</p>
<p><strong>Attention：</strong>Zero-shot：要求E中需要包含其他模态的信息（比如属性，wordnet，word embedding之类的）</p>
<h4 id="Relevant-Learning-Problems"><a href="#Relevant-Learning-Problems" class="headerlink" title="Relevant Learning Problems"></a>Relevant Learning Problems</h4><ul>
<li><p><strong>WSL:Weakly supervised learning:</strong><br>  重点在于learns from E containing only Weak supervised（such as： 不完全，不精确，不准确，或者充满噪声的监督信息），WS 中信息不完全只有少样本这一类情况就是FSL了，在此基础上基于Oracle还是人工干预的方法，可以进一步细分为：</p>
</li>
<li><p><strong>Semi-supervised learning：</strong><br>  从E中的少量标记样本和大量未标记样本中学习。示例：文本和网页分类；其中包含Positive-unlabeled learning这种特殊问题，只包含positive label的问题：具体而言就是，只知道用户现在用户中标记的好友，而与其他未标记人之间的关系是未知的。</p>
</li>
<li><p><strong>Active Learning：</strong><br>  文章：选择信息量最大的未标记数据区query an ordacle？<br>  个人理解：选择信息量最大（通常用不确定性大的数据表示）来让人标注，从而构建数据集，让算法能够通过较少的数据标注操作实现更好的效果。</p>
<p>WSL with incomplete supervision 仅仅包括分类和回归的内容，而FSL还包含RL问题；WSL使用unlabel data 对E进行扩充，而FSL更多的使用各种类型的prior knowledge来扩充E，包括pre-train model ，其他领域的监督数据，未标记数据等等。</p>
</li>
</ul>
<hr>
<ul>
<li><p><strong>Imbalance learning：</strong></p>
<p>数据集的分布不均衡，比如一些y值很少用到的情况。IL从所有可能的y中Train&amp;test，FSL基于少量案例train&amp;test y，同时也可能基于一些先验知识来。</p>
</li>
<li><p><strong>Transfer learning：</strong></p>
<p>transfers knowledge from the source domain/task 👉 target domain/task, where training data is scarce.其中<strong>Domin adaptation</strong>，是一种TL：source/target <strong>tasks</strong> are the same but the source/target <strong>domains</strong> are different.举例说明就是：情感识别，一个基于电影评论，一个基于日用品评论。<br>Transfer Learning广泛的应用于FSL，[7,82,85]将先验知识从源任务转移到Few-shot task，从一些训练数据丰富的源域转移</p>
</li>
</ul>
<ul>
<li><p><strong>Meta-learning：</strong>感觉正文中讲的是个狗屎，后续通过附录中的看看</p>
<p>Meta-learning methods can be used to deal with the FSL problem. the meta-learner is taken as prior knowledge to guide each specific FSL task.</p>
</li>
</ul>
<h4 id="Core-Issue"><a href="#Core-Issue" class="headerlink" title="Core Issue"></a>Core Issue</h4><ul>
<li>经验风险最小化<br>Machine Learning其实是一个经验风险最小化的模型<script type="math/tex; mode=display">
R(h)=\int \ell(h(x), y) d p(x, y)=\mathbb{E}[\ell(h(x), y)] \\</script></li>
</ul>
<script type="math/tex; mode=display">
R_{I}(h)=\frac{1}{I} \sum_{i=1}^{I} \ell\left(h\left(x_{i}\right), y_{i}\right)\\</script><script type="math/tex; mode=display">
\mathbb{E}\left[R\left(h_{I}\right)-R(\hat{h})\right]=\underbrace{\mathbb{E}\left[R\left(h^{*}\right)-R(\hat{h})\right]}_{\mathcal{E}_{\mathrm{app}}(\mathcal{H})}+\underbrace{\mathbb{E}\left[R\left(h_{I}\right)-R\left(h^{*}\right)\right]}_{\mathcal{S}_{\mathrm{est}}(\mathcal{H}, I)}</script><p>  <img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200520155345136.png" alt="image-20200520155345136"></p>
<p> 上面这1 3的区别一个是在全空间上，另一个是在是我们的假设空间中，能取到的最优解。</p>
<p>  总体误差可以基于最小预期风险和最小经验风险来表示，如等式3。期望实和训练集的随机选择有关的，the approximation error 衡量了假设空间中的函数能够接近最优假设的程度，the estimation error 衡量了，最小经验误差代替最小期望误差在假设空间内的影响。</p>
<p>  <img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200520154733872.png" alt="image-20200520154733872"></p>
<ul>
<li>data （which provides Dtrain）数据角度</li>
<li>model which determines H（embedding function，转换到假设空间）</li>
<li>algorithm（searches for the optimal h）学习算法，下降方向</li>
</ul>
<ul>
<li><p>不可靠的经验风险最小化</p>
<p>  如果数据足够大的话，通过少量样本计算出来的假设空间就可以逼近实际上的最优假设空间，也就能得到一个很好的近似，但是在FSL中，可用的样本数很少，所以可能没办法产生很好的逼近，在这种情况下，产生的<strong>经验风险最小化指标hl过拟合</strong>，这就是FSL中的核心问题。</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200520155719041.png" alt="image-20200520155719041"></p>
<h4 id="Taxonomy"><a href="#Taxonomy" class="headerlink" title="Taxonomy"></a>Taxonomy</h4><p>为了解决FSL问题中经验风险最小化工具中hl的问题，prior knowledge是至关重要的，利用先验知识来扩充信息量的不足，基于先验知识的类别和使用方式就能对FSL works进行分类。</p>
<p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200520161249706.png" alt="image-20200520161249706"></p>
<ul>
<li>Data：通过数据增强等方式，增加数据量，从而使得经验风险最小化因子能够更加的准确。</li>
<li>Model：用先验知识来约束假设空间，使得需要搜索的范围变小，那么基于较少的数据也能够得到一个较好的估计，（相比原来）</li>
<li>Algorithm：使用先验知识，来搜索最优假设的参数，基于这些先验知识提供一个较好的initialization，或者guiding the searching steps</li>
</ul>
<p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200520164955304.png" alt="image-20200520164955304"></p>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p><strong>通过手工指定规则</strong>来进行数据增强的方式例如：:arrow_double_down: 很大程度上取决于领域的知识也需要人工成本，此外，这样的方式在数据集间的泛化能力很差，一般都是针对性设计，而且这样的不变性，不可能由人工穷举出来，所以这样的方式不能完全解决FSL问题。</p>
<blockquote>
<p>translation, flipping, shearing, scaling, reflection, cropping, rotation.</p>
</blockquote>
<p><strong>Advance data augmentation:</strong></p>
<p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200520193608911.png" alt="image-20200520193608911"></p>
<h4 id="Transforming-Samples-from-Dtrain"><a href="#Transforming-Samples-from-Dtrain" class="headerlink" title="Transforming Samples from Dtrain"></a>Transforming Samples from Dtrain</h4><ol>
<li><p>对训练集的数据进行几何<strong>变化处理</strong>，生成其他的样本，构建一个更大的数据集。</p>
</li>
<li><p>从相似类中学习一组编码器（每个编码器代表一个<strong>类内可变性</strong>），将这些习得的变化量添加到样本中形成新的样本。</p>
</li>
<li><p>基于差异从其他类别中转移过来</p>
</li>
<li><p>从一个样本变成多个；连续属性子空间来添加属性变化</p>
</li>
</ol>
<p>基本思路是一致的，通过变换，在原本数据的基础上，构建新的数据，只是有着不同的构建方式。详细的各种类型的构建可以看参考文献。</p>
<h4 id="Transforming-Samples-from-a-Weakly-Labeled-or-Unlabeled-Data-Set"><a href="#Transforming-Samples-from-a-Weakly-Labeled-or-Unlabeled-Data-Set" class="headerlink" title="Transforming Samples from a Weakly Labeled or Unlabeled Data Set"></a>Transforming Samples from a Weakly Labeled or Unlabeled Data Set</h4><p>基于弱标签或者无标签的数据来进行数据增强的情况，<strong>类似视频</strong>中有些事件之间变化比较大的情况，可以将这样的数据添加到训练集中来更清楚的预测。</p>
<p>:dagger:但是如何筛选哪些有需要的弱监督数据？</p>
<p>⭐基于训练数据训练一个svm进行筛选，然后将具有目标的示例添加进数据集</p>
<p>⭐Label Propagation,直接使用未标记的数据集</p>
<p>⭐也有文章采取逐步从信息量最大的数据中筛选的做法</p>
<h4 id="Transforming-Samples-from-Similar-Data-Sets"><a href="#Transforming-Samples-from-Similar-Data-Sets" class="headerlink" title="Transforming Samples from Similar Data Sets"></a>Transforming Samples from Similar Data Sets</h4><p>:tada: 汇总和改造相似的数据集，来扩充Few shot情况，基于样本之间的相似性度量来确立权重，典型的方法就是：使用GAN，生成器将Few-shot的训练集映射到大规模数据集，另一个生成器将大规模数据集的样本映射过来，从而训练出可以辅助样本迁移的模型。</p>
<h4 id="Summary1"><a href="#Summary1" class="headerlink" title="Summary1"></a>Summary1</h4><p>这些方法的使用取决于具体任务；</p>
<p>:x: 缺点：通常是针对数据集量身定做的</p>
<p>:+1: 针对这个问题有人提出了AutoAugment</p>
<p>:heavy_multiplication_x: 缺点：文本和音频的情况下就很难做这样的生成了</p>
<h3 id="MODEL"><a href="#MODEL" class="headerlink" title="MODEL"></a>MODEL</h3><p>:fire: 如果仅仅基于简单的假设去考虑的话，那么可能在我们的假设空间中的最优和实际的最优(不足以模拟现实社会中的复杂问题)会有比较大的距离，但是如果考虑复杂多样的假设空间，那么标准的机器学习模型也是不可行的（数据量不足以优化到最优解），考虑使用先验知识，将复杂多样的假设空间H 约束到较小的情况下进行学习，这样的话经验风险最小化器将会更加的可靠，同时也降低了过拟合的可能性。</p>
<p>根据先验知识的类型，可以划分成如下几种FSL：</p>
<p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200520212452504.png" alt="image-20200520212452504"></p>
<h4 id="Multitask-Learning"><a href="#Multitask-Learning" class="headerlink" title="Multitask Learning"></a>Multitask Learning</h4><p>:fire: 多个相关任务协同训练，基于特定任务的信息和通用任务的信息来一起学习，其中利用某些/其他任务的大量数据（源任务），在训练过程中，通过学习到的参数来对只有Few-shot（target task）进行约束。基于训练中参数对target的约束方式可以分为</p>
<ul>
<li><p><strong>parameter sharing参数共享</strong> 160 61 95 12<br>基本的网络架构如下图</p>
<p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200520232155796.png" alt="image-20200520232155796"><br>:zero: 有多种不同的架构，整体都是由共享层（参数是一致的）和特定于任务的层一起构建的，简单的描述一下如下：</p>
<ol>
<li>初始共享然后分配到特定任务；2. 源任务（pre训练）训练共享层，目标任务训练目标层；3.分别单独学习再有共享的编码器嵌入成一体。</li>
</ol>
</li>
<li><p><strong>parameter tying参数绑定</strong> 45 151 85<br><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200520232612155.png" alt="image-20200520232612155"><br>⭐基本思路：鼓励不同任务之间的参数存在相似性。对参数进行正则化是一种流行的方法。</p>
<p>:one: 有的方法对<strong>成对参数之间的差异</strong>及逆行了惩罚，从而确保参数分布的相似性</p>
<p>:two: 有的方法通过针对源任务和目标任务设置不同的CNN，之间使用特殊的正则化术语对齐。</p>
</li>
</ul>
<h4 id="Embedding-Learning"><a href="#Embedding-Learning" class="headerlink" title="Embedding Learning"></a>Embedding Learning</h4><p>基于先验知识（同时可以额外使用Dtrain中的任务特定信息）构建样本的一个低维嵌入，这样便能得到一个较小的假设空间，同时相似的样本会紧密接近，而异类的样本更容易区分。</p>
<p><strong>Key Components：</strong></p>
<ol>
<li>将测试，训练样本用embedding函数（f，g）嵌入。f，g可以统一，但是分离的时候可以受获更好的准确度</li>
<li>相似性度量在嵌入空间（一般都是维度更低的空间）进行，</li>
</ol>
<p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200521000708513.png" alt="image-20200521000708513"></p>
<p><strong>可以根据embedding函数的参数是否随任务变化分类</strong></p>
<ol>
<li><p>针对任务的嵌入模型<br>仅仅使用来自该任务的信息来学习针对性的嵌入模型。</p>
</li>
<li><p>通用的嵌入模型（task-invariant）<br>使用有足够样本且具有各种输出的大规模数据集，学习通用的embedding function，然后直接用于Fewshot。Recently, more complicated embeddings are learned [70, 150] by a <strong>convolutional siamese net</strong> [20]<br><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200521122115448.png" alt="image-20200521122115448"></p>
<p>通常而言，task-invariant不会使用Few-shot的数据集来更新embedding function参数，但是，其中很多情景都会模拟few-shot 的情景来训练embedding从而确保对此类任务有更好的概括性能。</p>
<p>⭐<strong>Mathching Nets</strong></p>
<pre><code>    meta-learning / resLSTM / AL /set-to-set
</code></pre><p>⭐<strong>Prototypical Networks</strong></p>
<p>​        embedding(xtest)不与每个g(xi)对比，而是每一类别的训练数据都有一个”原型“（原型公式如下），与原型对比，减少计算量。有两种变体：应用到matching-net 和 semi-supervised-108（软分配未标注的样本用以增强Dtrain）</p>
<script type="math/tex; mode=display">
c_{n}=\frac{1}{K} \sum_{i=1}^{K} g\left(x_{i}\right)c_{n}=\frac{1}{K} \sum_{i=1}^{K} g\left(x_{i}\right)</script><p>:zap:<strong>Other Method</strong></p>
<p>​        ARC：利用attention+LSTM将xtest的不同区域和原型进行比较，然后将比较结果作为中间嵌入，在使用biLSTM（双向LSTM）进行最终嵌入；<br>​        Relation Net 使用CNN将Xtest和Xi拼接在一起，再使用另一个CNN输出相似度得分。<br>​        GNN：利用GNN使用临近节点的信息<br>​        SNAIL简单神经注意力学习器（RL通常看重时间信息）：temporal convolution +Attention，聚合临近步长和通过Attention选择特定时间步长的信息。</p>
</li>
<li><p>混合嵌入模型，可以编码 task-specific 和 task-invariant 的信息<br>虽然task-invariant可以再迁移的时候减少计算成本，但是针对一些特殊的少样本情况，他是无法直接适应的，比如说原本就是小概率事件（异常），这种情况下，基于Dtrain训练的先验知识来adapt通用的embedding模型，从而组成一个混合的结构，如下图所示。</p>
<p>​    <img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200521144018844.png" alt="image-20200521144018844"></p>
<p>Learnet从多个meta-training set中学习meta-learner，并将训练实例映射成网络中的参数（convolutional Siamese net），这样f的参数就会随着输入改变。还有一些针对其的改进</p>
<p>TADAM：将类别原型平均化到嵌入中，并使用meta-learned 映射成圆形网络的参数</p>
<p>DCCN：使用固定的滤波器，并从Dtrain中学习组合系数。</p>
</li>
</ol>
<h4 id="Learning-with-External-Memory"><a href="#Learning-with-External-Memory" class="headerlink" title="Learning with External Memory"></a>Learning with External Memory</h4><p>基于Dtrain 训练一个Embedding function，提取出 key-value的知识，存储在外部存储器中，对于新样本（test），用Embedding&amp;相似度函数查询最相似的slots，用这些slots的组合来表示样本，然后用简单的分类器（like softmax）进行分类预测。由于对M操作成本高，所以M通常尺寸较小。当M未满时，可以讲新样本写如空闲的存储插槽。</p>
<p>   <img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200521150103355.png" alt="image-20200521150103355"></p>
<p>   <img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200521151204794.png" alt="image-20200521151204794"><br>   key-value的表征，也就是memory中的定义在这个方法中至关重要，它决定了键值对对test的表征水平。根据存储器的功能，将这类方法分成两种类型：</p>
<p>:one:Refining Representations:</p>
<ul>
<li><strong>MANN</strong>：meta-learns embedding f，将同类的样本映射到同一个value，同一类的样本一起在内存中优化类表示。可以看成ProtoNet中精致的类原型。</li>
<li>当且仅当M不能很好的表征x的时候更新M。</li>
<li><strong>The Abstract Memory：</strong> 使用两个M，一个基于大量数据训练出的固定键值对，另一个从固定键值对对少量类进行精炼提取。为此有的方法会注意保留M中的FS。</li>
<li>few-shot在M中很容易被其他samples的值表征从而取代，为了解决这个问题，提出的此算法👇</li>
<li><strong>lifelong memory：</strong>通过删除oldest slot来update M，同时给所有slot的期限置为0，当新样本在经过M后输出的表征与实际输出匹配的时候，就合并，而不更新M。（但是还是没有真正的解决这个问题）</li>
</ul>
<p>:two:Refining Parameters:</p>
<ul>
<li><strong>MetaNet、MN-Net：</strong> 对特定任务的数据进行fast 学习，而通用任务slow更新，然后结合memory的机制。（参数化学习</li>
</ul>
<h4 id="Generative-Modeling"><a href="#Generative-Modeling" class="headerlink" title="Generative Modeling"></a>Generative Modeling</h4><p>借助先验知识，从x的分布中估计先验p(x；$\theta$ )的分布，从而估计和p(x|y)和p(y)，基于这样的先验数学模型进行后续的计算。而先验估计过程中通常是从别的数据集获悉的先验分布中，基于某个潜在的参数z迁移过来的如下式，这样就能基于既有的后验分布，约束H假设空间。</p>
<script type="math/tex; mode=display">
x \sim \int p(x | z ; \theta) p(z ; y) d z</script><p>通常在识别，生成，反转，重建中有较常见的应用</p>
<p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200529163340216.png" alt="image-20200529163340216"></p>
<ul>
<li><p><strong>Decomposable Components：</strong><br><strong>基于人类</strong>的认知，将数据分解成组件级别，在进行后续的识别和重组；利用类间的通用性；</p>
</li>
<li><p><strong>Groupwise Shared Prior：</strong><br>新的FS类别，先通过无监督学习分组，共享组内的类别先验，然后基于组内的先验对其进行建模。</p>
</li>
<li><p><strong>Parameters of Inference Networks：</strong>网络参数推理</p>
<script type="math/tex; mode=display">
p(z | x ; \theta, \gamma)=\frac{p(x, z ; \theta, y)}{p(x ; y)}=\frac{p(x | z ; \theta) p(z ; \gamma)}{\int p(x | z ; \theta) p(z ; \gamma) d z}</script><p>为了找到最优的$\theta$ ，必须最大化以上的后验概率：</p>
<p>基于数据对其进行求解，inference network能够高效的迁移到新任务，但是inference network 需要大量的参数，所以通常需要在辅助的大规模数据集训练后才使用。很多经典的推理网络都可以在FSL上应用，比如VAE（可变自动编码器），autoregressive model，GAN，VAE+GAN</p>
</li>
</ul>
<h4 id="Summary2"><a href="#Summary2" class="headerlink" title="Summary2"></a>Summary2</h4><p>详细的优缺点，参考文章</p>
<ol>
<li>存在相似任务或者辅助任务：多任务学习 ​</li>
<li>包含足够的各种类别的大规模数据集：embedding方法</li>
<li>存在可用的内存网络：在内存顶部训练一个简单的模型（分类器），可以简单的用于FSL，主要是要精心设计更新规则。</li>
<li>除了FSL还想要执行生成和重构的任务的时候：generative modeling</li>
</ol>
<h3 id="ALGORITHM"><a href="#ALGORITHM" class="headerlink" title="ALGORITHM"></a>ALGORITHM</h3><p>算法层面的改进指的是在最优空间H搜索H*的策略，最基础的有SGD。:x:在FSL的情况下，数据会使得更新次数不够多，同时也没法基于交叉验证找到合适的补偿之类的。:arrows_counterclockwise:本节中的方法用先验知识来影响$\theta$ ，具体体现为：:one:良好的初值；:two:直接学习优化器以输出搜索步骤；​</p>
<p>:zap:基于先验知识对策略的影响，对算法进行分类</p>
<p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200529224457034.png" alt="image-20200529224457034"></p>
<h4 id="Refining-Existing-Parameters"><a href="#Refining-Existing-Parameters" class="headerlink" title="Refining Existing Parameters"></a>Refining Existing Parameters</h4><p>:jack_o_lantern:基本思想：从相关任务中预训练模型的$\theta$0作为一个良好的初始化，然后基于训练集的几次训练来adapt。</p>
<ul>
<li><p>Fine-Tuning Existing Parameter by Regularization：<br>如何解决overfit的问题是此类预训练算法设计关键：其中一种方式就是依赖正则化操作来adapt参数。<br><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200529230946081.png" alt="image-20200529230946081"><br>正则化的方式主要有以下几种：</p>
<p>| Method                                      | Analysis                                                     |<br>| —————————————————————- | —————————————————————————————— |<br>| Early-stopping                              | 监视训练过程，性能没有提高则停止学习                         |<br>| Selectively updating $\theta$               | 根据具体问题，选择需要的部分来更新参数，不更新所有参数       |<br>| Updating related parts of $\theta$ together | 聚类$\theta$，然后共同更新每个组，BP更新$\theta$             |<br>| Using a model regression network            | 捕获任务无关的transformation，基于function进行embedding的映射？ |<br>|                                             |                                                              |</p>
</li>
<li><p>Aggregating a Set of Parameters：<br>聚合相关模型：不贴切的比如眼口鼻到脸；具体使用上，unlabeled/similar label dataset,的pretrain model参数到FSL参数的适应。</p>
<p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200529235519894.png" alt="image-20200529235519894"></p>
<ul>
<li><p>unlabeled dataset:<br>把相似样本分组聚类，然后adapt</p>
</li>
<li><p>similar dataset:<br>替换相似类别中的特征，重新使用已训练的分类器，然后对新类调整分类阈值。</p>
</li>
</ul>
</li>
</ul>
<ul>
<li>Fine-Tuning Existing Parameter with New Parameters：<br>仅仅对模型迁徙可能没办法对FSL完全编码，所以我们在对参数进行adapt的时候加入一个新的参数，然后再Dtrain中同时adapt现存参数和learn新参数<br><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200530001254775.png" alt="image-20200530001254775"></li>
</ul>
<h4 id="Refining-Meta-Learned-Parameter"><a href="#Refining-Meta-Learned-Parameter" class="headerlink" title="Refining Meta-Learned Parameter"></a>Refining Meta-Learned Parameter</h4><p>  本节中细化meta-learned的参数学习：$\theta$再过程中是持续优化的，不是固定的。<br>  <img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200530001506867.png" alt="image-20200530001506867"></p>
<p>  Model-Agnostic Meta-Learning（MAML）通过梯度下降来元学习 $ \theta$  ,基于该参数，得到任务特定参数$\phi~s$,更新公式类似如下形式$\phi<em>{s}=\theta</em>{0}-\alpha \nabla<em>{\theta</em>{0}} \mathcal{L}<em>{\mathrm{train}}^{s}\left(\theta</em>{0}\right) . $ 其中$L^s train$ 是训练样本的损失和， $\alpha $ 是步长，该参数$\phi~s$,对于样本的顺序不受影响，此外元学习中基本的参数更新公式如下$\theta<em>{0} \leftarrow \theta</em>{0}-\beta \nabla<em>{\theta</em>{0}} \sum<em>{T</em>{s} \sim P(T)} \mathcal{L}<em>{\text {test }}^{s}\left(\theta</em>{0}\right)$ ，其中测试误差是整个过程中损失的和。通过元学习将参数转移。</p>
<p>  最近针对MAML提出了主要再以下三个方面的改进：</p>
<ul>
<li>:zap:合并特定任务的信息：MAML为所有任务提供相同的初始化，但是这样忽视了特异性，所以，从一个好的初始化参数的子集中为新任务选择初值</li>
<li>:zap:使用meta-learned $\theta$的不确定性去建模：结合AL</li>
<li>:zap:改进refining过程：对$T~s$使用正则化？</li>
</ul>
<h4 id="Learning-the-Optimizer"><a href="#Learning-the-Optimizer" class="headerlink" title="Learning the Optimizer"></a>Learning the Optimizer</h4><p>不使用梯度下降，学习一种可以直接输出更新的优化器，无需调整步长$\alpha$ 和搜索方向。</p>
<p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200530010815362.png" alt="image-20200530010815362"></p>
<p>LSTM+Meta-Learner？</p>
<h4 id="Discussion-and-Summary"><a href="#Discussion-and-Summary" class="headerlink" title="Discussion and Summary:"></a>Discussion and Summary:</h4><p>通过对现有参数进行微调，从而减少H需要的搜索量：</p>
<ul>
<li>使用现有$\theta$作为初始化：牺牲一些精度换取速度</li>
<li>另外两种策略都是依赖于元学习，元学习可以让参数和指定任务更为接近，还有一种直接充当优化器。</li>
</ul>
<h3 id="Future-Works"><a href="#Future-Works" class="headerlink" title="Future Works"></a>Future Works</h3><ul>
<li>在未来的FSL中使用多模态的prior knowledge</li>
<li>SOTA网络架构的使用来改进data，algorithm，model；</li>
<li>AutoML在FSL任务中的应用</li>
<li>meta-learning中动态学习中，如何避免catastrophic forgetting</li>
<li>在各领域中的应用：CV，bot，NLP，Acoustic signal process，etc</li>
</ul>
<p>:zap:<strong>Theories：</strong></p>
<ul>
<li><p>FSL使用先验知识来弥补缺少监管信息的情况；</p>
</li>
<li><p>FSL很多时候和domain adaptation 有关系</p>
</li>
<li><p>FSL的收敛性研究还没有完全了解</p>
</li>
</ul>
<h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h2><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h3><p><img src="https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200530163634335.png" alt="image-20200530163634335"></p>
<p>之后整理一些可能需要阅读的reference</p>
<ol>
<li><p>只关注小样本的概念学习和经验学习的Another FSL survey:<br>J. Shu, Z. Xu, and D Meng. 2018. Small sample learning in big data era. arXiv preprint arXiv:1808.04572 (2018).</p>
</li>
<li><p>FS-RL，在仅给出少量状态和动作对组成的轨迹的情况下找到一种策略：</p>
<p>[3,33]</p>
</li>
<li><p>Bayesian Learning :<br>[35,76]</p>
</li>
<li></li>
</ol>
<h3 id="Additional-Vocabulary："><a href="#Additional-Vocabulary：" class="headerlink" title="Additional Vocabulary："></a>Additional Vocabulary：</h3><div class="table-container">
<table>
<thead>
<tr>
<th>序号</th>
<th>希腊字母</th>
<th>Markdoown</th>
<th>序号</th>
<th>希腊字母</th>
<th>Markdoown</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>α</td>
<td>\alpha</td>
<td>19</td>
<td>β</td>
<td>\beta</td>
</tr>
<tr>
<td>2</td>
<td>γ</td>
<td>\gamma</td>
<td>20</td>
<td>δ</td>
<td>\delta</td>
</tr>
<tr>
<td>3</td>
<td>Γ</td>
<td>\Gamma</td>
<td>21</td>
<td>Δ</td>
<td>\Delta</td>
</tr>
<tr>
<td>4</td>
<td>ε</td>
<td>\varepsilon</td>
<td>22</td>
<td>ϵ</td>
<td>\epsilon</td>
</tr>
<tr>
<td>5</td>
<td>ζ</td>
<td>\zeta</td>
<td>23</td>
<td>η</td>
<td>\eta</td>
</tr>
<tr>
<td>6</td>
<td>Θ</td>
<td>\Theta</td>
<td>24</td>
<td>ι</td>
<td>\iota</td>
</tr>
<tr>
<td>7</td>
<td>θ</td>
<td>\theta</td>
<td>25</td>
<td>κ</td>
<td>\kappa</td>
</tr>
<tr>
<td>8</td>
<td>Λ</td>
<td>\Lambda</td>
<td>26</td>
<td>λ</td>
<td>\lambda</td>
</tr>
<tr>
<td>9</td>
<td>μ</td>
<td>\mu</td>
<td>27</td>
<td>ν</td>
<td>\nu</td>
</tr>
<tr>
<td>10</td>
<td>ξ</td>
<td>\xi</td>
<td>28</td>
<td>ο</td>
<td>\omicron</td>
</tr>
<tr>
<td>11</td>
<td>Π</td>
<td>\Pi</td>
<td>29</td>
<td>ρ</td>
<td>\rho</td>
</tr>
<tr>
<td>12</td>
<td>π</td>
<td>\pi</td>
<td>30</td>
<td>τ</td>
<td>\tau</td>
</tr>
<tr>
<td>13</td>
<td>Σ</td>
<td>\Sigma</td>
<td>31</td>
<td>Φ</td>
<td>\Phi</td>
</tr>
<tr>
<td>14</td>
<td>σ</td>
<td>\sigma</td>
<td>32</td>
<td>ϕ</td>
<td>\phi</td>
</tr>
<tr>
<td>15</td>
<td>Υ</td>
<td>\Upsilon</td>
<td>33</td>
<td>Ψ</td>
<td>\Psi</td>
</tr>
<tr>
<td>16</td>
<td>υ</td>
<td>\upsilon</td>
<td>34</td>
<td>ψ</td>
<td>\psi</td>
</tr>
<tr>
<td>17</td>
<td>Ω</td>
<td>\Omega</td>
<td>35</td>
<td>ω</td>
<td>\omega</td>
</tr>
<tr>
<td>18</td>
<td>φ</td>
<td>\varphi</td>
<td>36</td>
<td>Ξ</td>
<td>\Xi</td>
</tr>
</tbody>
</table>
</div>
<p>术语或生词:</p>
<ul>
<li>empirical risk minimizer ：经验风险最小化器</li>
<li>ultimate goal ：最终目的</li>
<li>To name a few： 举几个例子</li>
<li>autonomous driving car：自动驾驶汽车</li>
<li>tackled：解决</li>
<li>paradigm：范式</li>
<li>ethic：道德</li>
<li>taxonomy：分类</li>
<li><strong>the pros and cons of</strong> different approaches：不同方法的利弊</li>
<li>with respect to：关于</li>
<li>the approximation error：逼近误差</li>
<li>the estimation error：估计误差</li>
<li>alleviate：缓和，减轻</li>
<li>aggregation：聚集</li>
<li>simultaneously：同时，兼</li>
<li>penalized：受惩罚的</li>
<li>hybrid:混合的</li>
<li>interleaved：交错的</li>
<li>denominator：分母</li>
</ul>
<p>注意区分：</p>
<ul>
<li>sufficient：足够</li>
<li>terminology：术语</li>
<li>refine：提炼 提纯</li>
<li>leverage：利用</li>
<li>latent：潜在的</li>
</ul>
<h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><ul>
<li>[x] Testing Set需要在N-way上进行吗？应该是要的</li>
<li>[x] AL的query or oracle 是啥意思</li>
<li>[x] According to whether the oracle or human intervention is leveraged, this can be further classified into the following<br>此处 oracle到底是什么意思</li>
<li>[x] Semi-Supervised 又和FSL有什么区别呢</li>
<li>[x] Imbalance Learning确实不是很理解</li>
<li>[x] Generative Modeling的具体要素不是很懂</li>
<li>[x] Core Issue中的三个h的关系还有点疑惑<br>app：在最优的情况下，能搜索到的最优解和实际最优解之间的差距<br>est：实际的假设空间中的最优解和基于少量样本的经验得到的假设空间中的最优解之间的距离。</li>
<li>[x] Parameters of Inference Networks，不知道怎么理解，后续要补充</li>
</ul>
<p>—</p>
<h3 id="需要思考的点"><a href="#需要思考的点" class="headerlink" title="需要思考的点"></a>需要思考的点</h3><ul>
<li>[x] <strong>Few-shot-learning &amp; meta learning的问题设置</strong>，就是多类中都有<strong>足量样本</strong>，然后随机的从多类中选取few-way和few-shot的data模拟多种meta环境（fewshot和fewway），单次训练都是小样本的情况，进行学习，在这种环境下学习到，一个模式，然后从而减少数据量的要求。（这样就哪里减少了数据量啊，我就没懂了，）</li>
<li>[x] 那么假如说没有多类动作（怎么构造多类动作）：不会，我们可以在网上爬取，或者自己拍摄，因为只需要少量有标注的数据即可，也就是positive数据可以比较容易获得。</li>
<li>[x] 那么我们在构造增量的时候，也是考虑边际效益，然后当数据量达到一定规模的时候可以采用直接训练分类器的分类器，来对效果进行分类</li>
<li>[x] Few-shot-learning中，训练集和测试集，标签已知和未知到底怎么弄，从代码中以及定义中分析的话怎么感觉是两个意思。我们需要的应该不能用到那个。</li>
<li>[x] few-shot-learning应该指的是，新类只有很少的样本，但是旧类还是有大量标注样本的情况这个我们要好好分析。<h3 id="但是这样的创新点在于更多的是算法的组合，有没有办法提出一个网络结构将这样的思路融合起来。！！！"><a href="#但是这样的创新点在于更多的是算法的组合，有没有办法提出一个网络结构将这样的思路融合起来。！！！" class="headerlink" title="但是这样的创新点在于更多的是算法的组合，有没有办法提出一个网络结构将这样的思路融合起来。！！！"></a>但是这样的创新点在于更多的是算法的组合，有没有办法提出一个网络结构将这样的思路融合起来。！！！</h3></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>Survey for Few-Shot Learning</p><p><a href="http://aikenh.cn/cn/FSL-Collection/">http://aikenh.cn/cn/FSL-Collection/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>AikenH</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2021-11-29</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-10-31</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/Machine-Learning/">Machine Learning, </a><a class="link-muted" rel="tag" href="/tags/Survey/">Survey, </a><a class="link-muted" rel="tag" href="/tags/FSL/">FSL </a></div></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/img/alipay.jpg" alt="Alipay"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/img/wechat.jpg" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/cn/FSL%E5%89%8D%E6%9C%9F%E8%B0%83%E7%A0%94/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">FSL前期调研</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/cn/IL-MgSvF/"><span class="level-item">IL-MgSvF</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://aikenh.cn/cn/FSL-Collection/';
            this.page.identifier = 'cn/FSL-Collection/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'my-tech-blog-3' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/title.jpg" alt="AikenH"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">AikenH</p><p class="is-size-6 is-block">Future Full-Stack Developer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>ShenZhen</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">132</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">42</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">98</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/AikenH" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="ZhiHu" href="https://www.zhihu.com/people/Aiken-h"><i class="fab fa-zhihu"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="https://weibo.com/u/1788200627"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Steam" href="https://steamcommunity.com/id/AikenH/"><i class="fab fa-steam"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/AikenH"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Abstract"><span class="level-left"><span class="level-item">1</span><span class="level-item">Abstract</span></span></a></li><li><a class="level is-mobile" href="#Notation-and-Terminology"><span class="level-left"><span class="level-item">2</span><span class="level-item">Notation and Terminology</span></span></a></li><li><a class="level is-mobile" href="#Main-Body"><span class="level-left"><span class="level-item">3</span><span class="level-item">Main Body</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Overview"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">Overview</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Definition"><span class="level-left"><span class="level-item">3.1.1</span><span class="level-item">Definition</span></span></a></li><li><a class="level is-mobile" href="#Relevant-Learning-Problems"><span class="level-left"><span class="level-item">3.1.2</span><span class="level-item">Relevant Learning Problems</span></span></a></li><li><a class="level is-mobile" href="#Core-Issue"><span class="level-left"><span class="level-item">3.1.3</span><span class="level-item">Core Issue</span></span></a></li><li><a class="level is-mobile" href="#Taxonomy"><span class="level-left"><span class="level-item">3.1.4</span><span class="level-item">Taxonomy</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Data"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">Data</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Transforming-Samples-from-Dtrain"><span class="level-left"><span class="level-item">3.2.1</span><span class="level-item">Transforming Samples from Dtrain</span></span></a></li><li><a class="level is-mobile" href="#Transforming-Samples-from-a-Weakly-Labeled-or-Unlabeled-Data-Set"><span class="level-left"><span class="level-item">3.2.2</span><span class="level-item">Transforming Samples from a Weakly Labeled or Unlabeled Data Set</span></span></a></li><li><a class="level is-mobile" href="#Transforming-Samples-from-Similar-Data-Sets"><span class="level-left"><span class="level-item">3.2.3</span><span class="level-item">Transforming Samples from Similar Data Sets</span></span></a></li><li><a class="level is-mobile" href="#Summary1"><span class="level-left"><span class="level-item">3.2.4</span><span class="level-item">Summary1</span></span></a></li></ul></li><li><a class="level is-mobile" href="#MODEL"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">MODEL</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Multitask-Learning"><span class="level-left"><span class="level-item">3.3.1</span><span class="level-item">Multitask Learning</span></span></a></li><li><a class="level is-mobile" href="#Embedding-Learning"><span class="level-left"><span class="level-item">3.3.2</span><span class="level-item">Embedding Learning</span></span></a></li><li><a class="level is-mobile" href="#Learning-with-External-Memory"><span class="level-left"><span class="level-item">3.3.3</span><span class="level-item">Learning with External Memory</span></span></a></li><li><a class="level is-mobile" href="#Generative-Modeling"><span class="level-left"><span class="level-item">3.3.4</span><span class="level-item">Generative Modeling</span></span></a></li><li><a class="level is-mobile" href="#Summary2"><span class="level-left"><span class="level-item">3.3.5</span><span class="level-item">Summary2</span></span></a></li></ul></li><li><a class="level is-mobile" href="#ALGORITHM"><span class="level-left"><span class="level-item">3.4</span><span class="level-item">ALGORITHM</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Refining-Existing-Parameters"><span class="level-left"><span class="level-item">3.4.1</span><span class="level-item">Refining Existing Parameters</span></span></a></li><li><a class="level is-mobile" href="#Refining-Meta-Learned-Parameter"><span class="level-left"><span class="level-item">3.4.2</span><span class="level-item">Refining Meta-Learned Parameter</span></span></a></li><li><a class="level is-mobile" href="#Learning-the-Optimizer"><span class="level-left"><span class="level-item">3.4.3</span><span class="level-item">Learning the Optimizer</span></span></a></li><li><a class="level is-mobile" href="#Discussion-and-Summary"><span class="level-left"><span class="level-item">3.4.4</span><span class="level-item">Discussion and Summary:</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Future-Works"><span class="level-left"><span class="level-item">3.5</span><span class="level-item">Future Works</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Appendix"><span class="level-left"><span class="level-item">4</span><span class="level-item">Appendix</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Reference"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">Reference:</span></span></a></li><li><a class="level is-mobile" href="#Additional-Vocabulary："><span class="level-left"><span class="level-item">4.2</span><span class="level-item">Additional Vocabulary：</span></span></a></li></ul></li><li><a class="level is-mobile" href="#FAQ"><span class="level-left"><span class="level-item">5</span><span class="level-item">FAQ</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#需要思考的点"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">需要思考的点</span></span></a></li><li><a class="level is-mobile" href="#但是这样的创新点在于更多的是算法的组合，有没有办法提出一个网络结构将这样的思路融合起来。！！！"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">但是这样的创新点在于更多的是算法的组合，有没有办法提出一个网络结构将这样的思路融合起来。！！！</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><figure class="media-left"><a class="image" href="/cn/deploy_server_byipv6_03_ddnsgo/"><img src="/img/header_img/lml_bg12.jpg" alt="使用Ipv6部署服务03 DDNS-go 动态域名解析"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-11-07T05:40:13.000Z">2023-11-07</time></p><p class="title"><a href="/cn/deploy_server_byipv6_03_ddnsgo/">使用Ipv6部署服务03 DDNS-go 动态域名解析</a></p><p class="categories"><a href="/categories/NAS/">NAS</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/cn/deploy_server_byipv6_02_nginx/"><img src="/img/header_img/lml_bg11.jpg" alt="使用Ipv6部署服务02 Nginx和Https"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-11-06T12:49:16.000Z">2023-11-06</time></p><p class="title"><a href="/cn/deploy_server_byipv6_02_nginx/">使用Ipv6部署服务02 Nginx和Https</a></p><p class="categories"><a href="/categories/NAS/">NAS</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/cn/Windows%E7%AB%AF%E5%8F%A3%E5%BC%82%E5%B8%B8%E5%8D%A0%E7%94%A8/"><img src="/img/header_img/lml_bg9.jpg" alt="Windows端口异常占用"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-11-05T15:46:54.000Z">2023-11-05</time></p><p class="title"><a href="/cn/Windows%E7%AB%AF%E5%8F%A3%E5%BC%82%E5%B8%B8%E5%8D%A0%E7%94%A8/">Windows端口异常占用</a></p><p class="categories"><a href="/categories/Windows/">Windows</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/cn/deploy_server_byipv6/"><img src="/img/header_img/lml_bg10.jpg" alt="使用Ipv6部署服务01 IPV6开启和设置"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-11-05T11:23:17.000Z">2023-11-05</time></p><p class="title"><a href="/cn/deploy_server_byipv6/">使用Ipv6部署服务01 IPV6开启和设置</a></p><p class="categories"><a href="/categories/NAS/">NAS</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/cn/Flashcard_template/"><img src="/img/header_img/lml_bg35.jpg" alt="Obsidian使用 Spaced Repetition 制作闪念卡片"></a></figure><div class="media-content"><p class="date"><time dateTime="2023-10-27T16:39:59.000Z">2023-10-28</time></p><p class="title"><a href="/cn/Flashcard_template/">Obsidian使用 Spaced Repetition 制作闪念卡片</a></p><p class="categories"><a href="/categories/Editor/">Editor</a> / <a href="/categories/Editor/Obsidian/">Obsidian</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Aiken&#039;s Blog</a><p class="is-size-7"><span>&copy; 2023 AikenH</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_pv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span> and <span id="busuanzi_container2_site_uv"><span id="busuanzi_value_site_pv">0</span>&nbsp;visits</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/night.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>